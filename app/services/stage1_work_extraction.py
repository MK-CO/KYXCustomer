"""
ç¬¬ä¸€é˜¶æ®µï¼šå·¥å•æ•°æ®æŠ½å–æœåŠ¡
ä»t_workè¡¨æŠ½å–å·¥å•IDï¼Œè·å–è¯„è®ºæ•°æ®ï¼Œç»´æŠ¤AIå¤„ç†çŠ¶æ€
"""
import logging
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set
from sqlalchemy import text
from sqlalchemy.orm import Session
from sqlalchemy.exc import SQLAlchemyError, IntegrityError

from app.db.database import get_db
from config.settings import settings

logger = logging.getLogger(__name__)


class Stage1WorkExtractionService:
    """ç¬¬ä¸€é˜¶æ®µï¼šå·¥å•æ•°æ®æŠ½å–æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¬¬ä¸€é˜¶æ®µæœåŠ¡"""
        self.work_table_base_name = "t_work"
        self.comment_table_base_name = "t_work_comment"
        self.pending_table_name = "ai_work_pending_analysis"
        self.current_year = datetime.now().year
        self._table_cache = {}
        self._cache_expire_time = None
        self._cache_duration = timedelta(hours=1)
    
    # ==================== è¡¨åç®¡ç†æ–¹æ³• ====================
    
    def get_work_table_name(self, year: int = None) -> str:
        """è·å–å·¥å•è¡¨å"""
        if year is None:
            year = self.current_year
        return f"{self.work_table_base_name}_{year}"
    
    def get_comment_table_name(self, year: int = None) -> str:
        """è·å–è¯„è®ºè¡¨å"""
        if year is None:
            year = self.current_year
        return f"{self.comment_table_base_name}_{year}"
    
    def discover_work_tables(self, db: Session) -> List[str]:
        """å‘ç°æ‰€æœ‰å·¥å•åˆ†è¡¨"""
        logger.info("=== å¼€å§‹å‘ç°å·¥å•åˆ†è¡¨ ===")
        try:
            sql = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = DATABASE() 
            AND table_name LIKE :pattern
            AND table_type = 'BASE TABLE'
            ORDER BY table_name DESC
            """
            
            pattern = f"{self.work_table_base_name}_%"
            logger.info(f"æ‰§è¡ŒSQLæŸ¥è¯¢å‘ç°å·¥å•è¡¨ï¼ŒåŒ¹é…æ¨¡å¼: {pattern}")
            logger.debug(f"SQLæŸ¥è¯¢: {sql}")
            
            result = db.execute(text(sql), {"pattern": pattern})
            all_tables = [row[0] for row in result.fetchall()]
            
            logger.info(f"âœ“ æŸ¥è¯¢åˆ° {len(all_tables)} ä¸ªå€™é€‰å·¥å•è¡¨: {all_tables}")
            
            # éªŒè¯è¡¨åæ ¼å¼ï¼Œåªæ¥å— t_work_YYYY æ ¼å¼
            year_pattern = re.compile(rf'^{re.escape(self.work_table_base_name)}_(\d{{4}})$')
            valid_tables = []
            for table in all_tables:
                match = year_pattern.match(table)
                if match:
                    year = int(match.group(1))
                    if 2020 <= year <= 2030:
                        valid_tables.append(table)
                        logger.debug(f"âœ“ éªŒè¯è¡¨ {table} æœ‰æ•ˆï¼Œå¹´ä»½: {year}")
                    else:
                        logger.warning(f"âš ï¸ è¡¨ {table} å¹´ä»½ {year} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (2020-2030)")
                else:
                    logger.warning(f"âš ï¸ è¡¨ {table} æ ¼å¼ä¸åŒ¹é…å¹´ä»½æ¨¡å¼ï¼Œè·³è¿‡")
            
            logger.info(f"âœ“ å‘ç° {len(valid_tables)} ä¸ªæœ‰æ•ˆå·¥å•è¡¨: {valid_tables}")
            if not valid_tables:
                logger.warning(f"âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆå·¥å•è¡¨ï¼Œå°†ä½¿ç”¨é»˜è®¤è¡¨: {self.work_table_base_name}_{self.current_year}")
                return [f"{self.work_table_base_name}_{self.current_year}"]
            
            return valid_tables
            
        except Exception as e:
            logger.error(f"âŒ å‘ç°å·¥å•åˆ†è¡¨å¤±è´¥: {e}")
            default_table = f"{self.work_table_base_name}_{self.current_year}"
            logger.info(f"ä½¿ç”¨é»˜è®¤å·¥å•è¡¨: {default_table}")
            return [default_table]
    
    def discover_comment_tables(self, db: Session) -> List[str]:
        """å‘ç°æ‰€æœ‰è¯„è®ºåˆ†è¡¨"""
        logger.info("=== å¼€å§‹å‘ç°è¯„è®ºåˆ†è¡¨ ===")
        current_time = datetime.now()
        
        # æ£€æŸ¥ç¼“å­˜
        if (self._cache_expire_time and 
            current_time < self._cache_expire_time and 
            self._table_cache):
            logger.info(f"âœ“ ä½¿ç”¨ç¼“å­˜çš„è¯„è®ºè¡¨æ•°æ®: {list(self._table_cache.keys())}")
            return list(self._table_cache.keys())
        
        try:
            sql = """
            SELECT table_name 
            FROM information_schema.tables 
            WHERE table_schema = DATABASE() 
            AND table_name LIKE :pattern
            ORDER BY table_name DESC
            """
            
            pattern = f"{self.comment_table_base_name}_%"
            logger.info(f"æ‰§è¡ŒSQLæŸ¥è¯¢å‘ç°è¯„è®ºè¡¨ï¼ŒåŒ¹é…æ¨¡å¼: {pattern}")
            logger.debug(f"SQLæŸ¥è¯¢: {sql}")
            
            result = db.execute(text(sql), {"pattern": pattern})
            tables = [row[0] for row in result.fetchall()]
            
            logger.info(f"âœ“ æŸ¥è¯¢åˆ° {len(tables)} ä¸ªè¯„è®ºè¡¨: {tables}")
            
            # éªŒè¯è¡¨åæ ¼å¼å¹¶ç¼“å­˜
            year_pattern = re.compile(rf'^{re.escape(self.comment_table_base_name)}_(\d{{4}})$')
            valid_tables = []
            for table in tables:
                match = year_pattern.match(table)
                if match:
                    year = int(match.group(1))
                    if 2020 <= year <= 2030:
                        self._table_cache[table] = year
                        valid_tables.append(table)
                        logger.debug(f"âœ“ éªŒè¯è¡¨ {table} æœ‰æ•ˆï¼Œå¹´ä»½: {year}")
                    else:
                        logger.warning(f"âš ï¸ è¡¨ {table} å¹´ä»½ {year} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (2020-2030)")
                else:
                    logger.warning(f"âš ï¸ è¡¨ {table} æ ¼å¼ä¸åŒ¹é…å¹´ä»½æ¨¡å¼")
            
            self._cache_expire_time = current_time + self._cache_duration
            logger.info(f"âœ“ ç¼“å­˜ {len(valid_tables)} ä¸ªæœ‰æ•ˆè¯„è®ºè¡¨ï¼Œç¼“å­˜æ—¶é—´: {self._cache_duration}")
            
            if not valid_tables:
                logger.warning(f"âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆè¯„è®ºè¡¨ï¼Œå°†ä½¿ç”¨é»˜è®¤è¡¨: {self.comment_table_base_name}_{self.current_year}")
                return [f"{self.comment_table_base_name}_{self.current_year}"]
            
            return valid_tables
            
        except Exception as e:
            logger.error(f"âŒ å‘ç°è¯„è®ºåˆ†è¡¨å¤±è´¥: {e}")
            default_table = f"{self.comment_table_base_name}_{self.current_year}"
            logger.info(f"ä½¿ç”¨é»˜è®¤è¯„è®ºè¡¨: {default_table}")
            return [default_table]
    
    def check_table_exists(self, db: Session, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        logger.debug(f"ğŸ” æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨: {table_name}")
        try:
            sql = """
            SELECT 1 FROM information_schema.tables 
            WHERE table_schema = DATABASE() 
            AND table_name = :table_name
            """
            logger.debug(f"SQLæŸ¥è¯¢: {sql}")
            logger.debug(f"å‚æ•°: table_name = {table_name}")
            
            result = db.execute(text(sql), {"table_name": table_name})
            exists = result.fetchone() is not None
            
            if exists:
                logger.debug(f"âœ“ è¡¨ {table_name} å­˜åœ¨")
            else:
                logger.warning(f"âš ï¸ è¡¨ {table_name} ä¸å­˜åœ¨")
            
            return exists
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥è¡¨ {table_name} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False
    
    # ==================== å·¥å•æ•°æ®æŠ½å–æ–¹æ³• ====================
    
    def _batch_extract_work_orders_by_time_range(
        self,
        db: Session,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        target_date: Optional[datetime] = None,
        days_back: int = 1
    ) -> List[Dict[str, Any]]:
        """é‡æ„ï¼šå…ˆæŸ¥è¯¢æ€»æ•°é‡ï¼Œç„¶åå›ºå®šæ¬¡æ•°æ‰¹é‡æŠ½å–å·¥å•æ•°æ®"""
        
        # ç¡®å®šæ—¶é—´èŒƒå›´
        if start_time is not None and end_time is not None:
            actual_start_time = start_time
            actual_end_time = end_time
        elif target_date is not None:
            actual_start_time = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            actual_end_time = actual_start_time + timedelta(days=1)
        else:
            target_date = datetime.now() - timedelta(days=days_back)
            actual_start_time = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            actual_end_time = actual_start_time + timedelta(days=1)
        
        batch_size = settings.data_extractor_limit_default
        
        logger.info(f"ğŸ“Š é‡æ„åæ‰¹é‡æŠ½å–é…ç½®:")
        logger.info(f"  â° æ—¶é—´èŒƒå›´: {actual_start_time} ~ {actual_end_time}")
        logger.info(f"  ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}æ¡/æ‰¹")
        
        # 1. å…ˆæŸ¥è¯¢ç¬¦åˆæ¡ä»¶çš„å·¥å•æ€»æ•°é‡
        try:
            target_year = actual_start_time.year
            work_table_name = self.get_work_table_name(target_year)
            
            # éªŒè¯è¡¨æ˜¯å¦å­˜åœ¨
            if not self.check_table_exists(db, work_table_name):
                logger.warning(f"âš ï¸ å·¥å•è¡¨ {work_table_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰å¹´ä»½è¡¨")
                work_table_name = self.get_work_table_name()
                if not self.check_table_exists(db, work_table_name):
                    logger.error(f"âŒ å·¥å•è¡¨ {work_table_name} ä¸å­˜åœ¨")
                    return []
            
            count_sql = f"""
            SELECT COUNT(*) as total_count
            FROM {work_table_name}
            WHERE create_time >= :start_time 
            AND create_time < :end_time
            AND deleted = 0
            AND state = 'FINISH'
            """
            
            logger.info(f"ğŸ” æŸ¥è¯¢ç¬¦åˆæ¡ä»¶çš„å·¥å•æ€»æ•°é‡...")
            count_result = db.execute(text(count_sql), {
                "start_time": actual_start_time,
                "end_time": actual_end_time
            })
            total_count = count_result.fetchone()[0]
            
            logger.info(f"ğŸ“Š æŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„å·¥å•æ€»æ•°: {total_count}æ¡")
            
            if total_count == 0:
                logger.info("âš ï¸ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„å·¥å•éœ€è¦æŠ½å–")
                return []
            
            # 2. è®¡ç®—éœ€è¦çš„å›ºå®šå¾ªç¯æ¬¡æ•°
            total_batches = (total_count + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
            logger.info(f"ğŸ“Š è®¡ç®—æ‰¹æ¬¡æ•°: æ€»è®¡{total_count}æ¡ Ã· {batch_size}æ¡/æ‰¹ = {total_batches}æ‰¹æ¬¡")
            
            # åº”ç”¨é…ç½®é™åˆ¶
            max_total_setting = settings.data_extractor_max_total
            max_batches_setting = settings.data_extractor_max_batches
            
            if max_total_setting > 0 and total_count > max_total_setting:
                total_count = max_total_setting
                total_batches = (total_count + batch_size - 1) // batch_size
                logger.info(f"ğŸ“Š åº”ç”¨é…ç½®é™åˆ¶: æœ€å¤§æ€»é‡{max_total_setting}æ¡ï¼Œè°ƒæ•´ä¸º{total_batches}æ‰¹æ¬¡")
            
            if max_batches_setting > 0 and total_batches > max_batches_setting:
                total_batches = max_batches_setting
                logger.info(f"ğŸ“Š åº”ç”¨é…ç½®é™åˆ¶: æœ€å¤§æ‰¹æ¬¡{max_batches_setting}æ‰¹æ¬¡")
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å·¥å•æ€»æ•°å¤±è´¥: {e}")
            return []
        
        # 3. å›ºå®šæ¬¡æ•°å¾ªç¯æŠ½å–
        all_work_orders = []
        current_offset = 0
        
        for batch_num in range(1, total_batches + 1):
            logger.info(f"ğŸ”„ æ‰§è¡Œç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡æŠ½å– (åç§»: {current_offset})")
            
            batch_orders = self.extract_work_orders_by_time_range(
                db, actual_start_time, actual_end_time, None, 1, 
                limit=batch_size, offset=current_offset
            )
            
            if not batch_orders:
                logger.info(f"âœ… ç¬¬ {batch_num} æ‰¹æ¬¡æ— æ•°æ®ï¼Œæå‰å®Œæˆ")
                break
            
            all_work_orders.extend(batch_orders)
            current_offset += len(batch_orders)
            
            logger.info(f"ğŸ“ˆ ç¬¬ {batch_num}/{total_batches} æ‰¹æ¬¡å®Œæˆ: æœ¬æ‰¹ {len(batch_orders)}æ¡ï¼Œç´¯è®¡ {len(all_work_orders)}æ¡")
        
        logger.info(f"ğŸ“Š å›ºå®šæ¬¡æ•°æ‰¹é‡æŠ½å–å®Œæˆ: è®¡åˆ’ {total_batches} æ‰¹æ¬¡ï¼Œå®é™… {batch_num if 'batch_num' in locals() else 0} æ‰¹æ¬¡ï¼Œæ€»è®¡ {len(all_work_orders)}æ¡å·¥å•")
        return all_work_orders
    
    def extract_work_orders_by_time_range(
        self,
        db: Session,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        target_date: Optional[datetime] = None,
        days_back: int = 1,
        limit: Optional[int] = None,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """æ ¹æ®æ—¶é—´èŒƒå›´æŠ½å–å·¥å•"""
        logger.info("=" * 50)
        logger.info("ğŸš€ å¼€å§‹æ ¹æ®æ—¶é—´èŒƒå›´æŠ½å–å·¥å•")
        
        # å¤„ç†æ—¶é—´èŒƒå›´å‚æ•°çš„ä¼˜å…ˆçº§ï¼šstart_time/end_time > target_date > days_back
        if start_time is not None and end_time is not None:
            # ä½¿ç”¨æŒ‡å®šçš„æ—¶é—´èŒƒå›´
            logger.info(f"ğŸ“… ä½¿ç”¨æŒ‡å®šæ—¶é—´èŒƒå›´: {start_time} - {end_time}")
        elif target_date is not None:
            # ä½¿ç”¨æŒ‡å®šæ—¥æœŸçš„æ•´å¤©
            start_time = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            end_time = start_time + timedelta(days=1)
            logger.info(f"ğŸ“… ä½¿ç”¨æŒ‡å®šæ—¥æœŸçš„æ•´å¤©: {target_date.date()}")
        else:
            # ä½¿ç”¨days_backè®¡ç®—
            target_date = datetime.now() - timedelta(days=days_back)
            start_time = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
            end_time = start_time + timedelta(days=1)
            logger.info(f"ğŸ“… ä½¿ç”¨days_back={days_back}è®¡ç®—æ—¶é—´èŒƒå›´")
        
        # ç¡®ä¿æ—¶é—´èŒƒå›´æœ‰æ•ˆ
        if start_time >= end_time:
            logger.error(f"âŒ æ—¶é—´èŒƒå›´æ— æ•ˆ: start_time={start_time} >= end_time={end_time}")
            return []
        
        logger.info(f"ğŸ“… æœ€ç»ˆæŠ½å–å·¥å•æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} - {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"â° æ—¶é—´è·¨åº¦: {(end_time - start_time).total_seconds() / 3600:.1f} å°æ—¶")
        
        # æ ¹æ®å¼€å§‹æ—¶é—´ç¡®å®šç›®æ ‡å¹´ä»½ï¼ˆå¦‚æœè·¨å¹´ï¼Œä½¿ç”¨å¼€å§‹æ—¶é—´çš„å¹´ä»½ï¼‰
        target_year = start_time.year
        work_table_name = self.get_work_table_name(target_year)
        comment_table_name = self.get_comment_table_name(target_year)
        
        logger.info(f"ğŸ¯ é¢„æœŸä½¿ç”¨è¡¨: å·¥å•è¡¨={work_table_name}, è¯„è®ºè¡¨={comment_table_name}")
        
        # éªŒè¯å·¥å•è¡¨æ˜¯å¦å­˜åœ¨
        logger.info("ğŸ” éªŒè¯å·¥å•è¡¨æ˜¯å¦å­˜åœ¨...")
        available_tables = self.discover_work_tables(db)
        if work_table_name not in available_tables:
            logger.warning(f"âš ï¸ å·¥å•è¡¨ {work_table_name} ä¸å­˜åœ¨ï¼Œä½¿ç”¨å½“å‰å¹´ä»½è¡¨")
            work_table_name = self.get_work_table_name()
            comment_table_name = self.get_comment_table_name()
            logger.info(f"ğŸ”„ è°ƒæ•´åä½¿ç”¨è¡¨: å·¥å•è¡¨={work_table_name}, è¯„è®ºè¡¨={comment_table_name}")
        else:
            logger.info(f"âœ“ å·¥å•è¡¨ {work_table_name} å­˜åœ¨ï¼Œç»§ç»­ä½¿ç”¨")
        
        # å†æ¬¡ç¡®è®¤è¡¨å­˜åœ¨
        logger.info(f"ğŸ” æœ€ç»ˆç¡®è®¤è¡¨ {work_table_name} æ˜¯å¦å­˜åœ¨...")
        if not self.check_table_exists(db, work_table_name):
            logger.error(f"âŒ å·¥å•è¡¨ {work_table_name} ä¸å­˜åœ¨ï¼ŒæŠ½å–å¤±è´¥")
            return []
        
        try:
            # åº”ç”¨é™åˆ¶é…ç½®
            if limit is None:
                limit = settings.data_extractor_limit_default
            
            sql = f"""
            SELECT 
                id as work_id,
                create_time,
                type as work_type,
                state as work_state,
                create_by,
                create_name,
                level,
                order_by,
                order_name
            FROM {work_table_name}
            WHERE create_time >= :start_time 
            AND create_time < :end_time
            AND deleted = 0
            AND state = 'FINISH'
            ORDER BY create_time DESC
            LIMIT :limit OFFSET :offset
            """
            
            logger.info(f"ğŸ“ æ‰§è¡Œå·¥å•æŸ¥è¯¢ (é™åˆ¶:{limit}æ¡, åç§»:{offset})")
            logger.info(f"â° æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
            
            result = db.execute(text(sql), {
                "start_time": start_time,
                "end_time": end_time,
                "limit": limit,
                "offset": offset
            })
            
            logger.info("âš¡ SQLæŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")
            
            work_orders = []
            row_count = 0
            for row in result:
                row_count += 1
                work_order = {
                    "work_id": row.work_id,
                    "work_table_name": work_table_name,
                    "comment_table_name": comment_table_name,
                    "extract_date": start_time.date(),  # ä½¿ç”¨å¼€å§‹æ—¶é—´çš„æ—¥æœŸä½œä¸ºæŠ½å–æ—¥æœŸ
                    "start_time": start_time,  # æ·»åŠ æ—¶é—´èŒƒå›´ä¿¡æ¯
                    "end_time": end_time,
                    "create_time": row.create_time,
                    "work_type": row.work_type,
                    "work_state": row.work_state,
                    "create_by": row.create_by,
                    "create_name": row.create_name,
                    "level": getattr(row, 'level', None),
                    "order_by": getattr(row, 'order_by', None),
                    "order_name": getattr(row, 'order_name', None)
                }
                work_orders.append(work_order)
                
                if row_count <= 5:  # åªæ‰“å°å‰5æ¡è®°å½•çš„è¯¦ç»†ä¿¡æ¯
                    logger.debug(f"ğŸ“‹ å·¥å• #{row_count}: ID={row.work_id}, åˆ›å»ºæ—¶é—´={row.create_time}, ç±»å‹={row.work_type}, çŠ¶æ€={row.work_state}")
                elif row_count == 6:
                    logger.debug("... (åç»­å·¥å•ä¿¡æ¯çœç•¥)")
            
            logger.info(f"âœ… ä»è¡¨ {work_table_name} æˆåŠŸæŠ½å–åˆ° {len(work_orders)} ä¸ªå·¥å•")
            
            if len(work_orders) == 0:
                logger.warning(f"âš ï¸ åœ¨æ—¶é—´èŒƒå›´ {start_time} - {end_time} å†…æœªæ‰¾åˆ°ä»»ä½•å·¥å•")
                logger.info("ğŸ’¡ è¯·æ£€æŸ¥:")
                logger.info("   1. æ—¶é—´èŒƒå›´æ˜¯å¦æ­£ç¡®")
                logger.info("   2. å·¥å•è¡¨ä¸­æ˜¯å¦æœ‰æ•°æ®")
                logger.info("   3. deleted=0 æ¡ä»¶æ˜¯å¦è¿‡æ»¤äº†æ‰€æœ‰è®°å½•")
            
            return work_orders
            
        except Exception as e:
            logger.error(f"âŒ ä»è¡¨ {work_table_name} æŠ½å–å·¥å•å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            return []
    
    def extract_work_orders_by_date(
        self,
        db: Session,
        target_date: Optional[datetime] = None,
        days_back: int = 1
    ) -> List[Dict[str, Any]]:
        """æ ¹æ®æ—¥æœŸæŠ½å–å·¥å•ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        logger.warning("âš ï¸ extract_work_orders_by_date æ–¹æ³•å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ extract_work_orders_by_time_range")
        return self.extract_work_orders_by_time_range(
            db=db,
            target_date=target_date,
            days_back=days_back
        )
    
    # ==================== å¾…å¤„ç†è¡¨ç®¡ç†æ–¹æ³• ====================
    
    def insert_pending_analysis_records(
        self,
        db: Session,
        work_orders: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """æ’å…¥å¾…å¤„ç†åˆ†æè®°å½•"""
        logger.info("=" * 50)
        logger.info("ğŸ’¾ å¼€å§‹æ’å…¥å¾…å¤„ç†åˆ†æè®°å½•")
        
        if not work_orders:
            logger.warning("âš ï¸ æ²¡æœ‰å·¥å•æ•°æ®éœ€è¦æ’å…¥")
            return {
                "success": True,
                "inserted": 0,
                "skipped": 0,
                "errors": 0,
                "total": 0,
                "message": "æ²¡æœ‰å·¥å•æ•°æ®éœ€è¦æ’å…¥"
            }
        
        logger.info(f"ğŸ“Š å‡†å¤‡æ’å…¥ {len(work_orders)} ä¸ªå·¥å•åˆ°å¾…å¤„ç†è¡¨: {self.pending_table_name}")
        
        success_count = 0
        skip_count = 0
        error_count = 0
        
        # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨æ‰¹é‡æäº¤æœºåˆ¶ï¼Œé¿å…é•¿æ—¶é—´äº‹åŠ¡é˜»å¡API
        batch_size = 50  # æ¯50æ¡è®°å½•æäº¤ä¸€æ¬¡
        
        for i, work_order in enumerate(work_orders, 1):
            work_id = work_order["work_id"]
            logger.debug(f"ğŸ”„ å¤„ç†å·¥å• {i}/{len(work_orders)}: ID={work_id}")
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                check_sql = f"""
                SELECT id FROM {self.pending_table_name}
                WHERE work_id = :work_id AND extract_date = :extract_date
                LIMIT 1
                """
                
                logger.debug(f"ğŸ” æ£€æŸ¥å·¥å• {work_id} æ˜¯å¦å·²å­˜åœ¨...")
                logger.debug(f"æ£€æŸ¥SQL: {check_sql}")
                
                existing = db.execute(text(check_sql), {
                    "work_id": work_order["work_id"],
                    "extract_date": work_order["extract_date"]
                }).fetchone()
                
                if existing:
                    skip_count += 1
                    logger.debug(f"â­ï¸ å·¥å• {work_id} å·²å­˜åœ¨ï¼Œè·³è¿‡æ’å…¥")
                    continue
                
                # ğŸ”¥ ä¿®å¤ï¼šåœ¨æ’å…¥å‰å®æ—¶æŸ¥è¯¢è¯„è®ºæ•°é‡
                comment_count = self.get_work_comment_count(db, work_order["work_id"], work_order["comment_table_name"])
                has_comments = 1 if comment_count > 0 else 0
                
                # æ’å…¥æ–°è®°å½•
                insert_sql = f"""
                INSERT INTO {self.pending_table_name} (
                    work_id, work_table_name, comment_table_name, extract_date,
                    create_time, work_type, work_state, create_by, create_name,
                    ai_status, comment_count, has_comments, created_at
                ) VALUES (
                    :work_id, :work_table_name, :comment_table_name, :extract_date,
                    :create_time, :work_type, :work_state, :create_by, :create_name,
                    'PENDING', :comment_count, :has_comments, :created_at
                )
                """
                
                logger.debug(f"ğŸ’¾ æ’å…¥å·¥å• {work_id} åˆ°å¾…å¤„ç†è¡¨ï¼Œè¯„è®ºæ•°: {comment_count}")
                logger.debug(f"æ’å…¥SQL: {insert_sql}")
                
                db.execute(text(insert_sql), {
                    "work_id": work_order["work_id"],
                    "work_table_name": work_order["work_table_name"],
                    "comment_table_name": work_order["comment_table_name"],
                    "extract_date": work_order["extract_date"],
                    "create_time": work_order["create_time"],
                    "work_type": work_order["work_type"],
                    "work_state": work_order["work_state"],
                    "create_by": work_order["create_by"],
                    "create_name": work_order["create_name"],
                    "comment_count": comment_count,  # ğŸ”¥ æ–°å¢ï¼šå®é™…è¯„è®ºæ•°é‡
                    "has_comments": has_comments,   # ğŸ”¥ æ–°å¢ï¼šæ˜¯å¦æœ‰è¯„è®ºæ ‡è¯†
                    "created_at": datetime.now()
                })
                
                success_count += 1
                logger.debug(f"âœ… å·¥å• {work_id} æ’å…¥æˆåŠŸ")
                
                # ğŸ”¥ æ‰¹é‡æäº¤æœºåˆ¶ï¼šæ¯å¤„ç†batch_sizeæ¡è®°å½•æˆ–åˆ°è¾¾æœ€åä¸€æ¡æ—¶æäº¤
                if success_count % batch_size == 0 or i == len(work_orders):
                    try:
                        db.commit()
                        logger.info(f"ğŸ’¾ æ‰¹é‡æäº¤: å·²å¤„ç† {i}/{len(work_orders)} æ¡è®°å½• (æˆåŠŸ:{success_count}, è·³è¿‡:{skip_count}, é”™è¯¯:{error_count})")
                    except Exception as commit_error:
                        db.rollback()
                        logger.error(f"âŒ æ‰¹é‡æäº¤å¤±è´¥: {commit_error}")
                        error_count += 1
                
            except IntegrityError as e:
                skip_count += 1
                logger.debug(f"â­ï¸ å·¥å• {work_id} è¿åå”¯ä¸€çº¦æŸï¼Œè·³è¿‡: {e}")
                db.rollback()
                continue
            except Exception as e:
                error_count += 1
                logger.error(f"âŒ æ’å…¥å·¥å• {work_id} åˆ°å¾…å¤„ç†è¡¨å¤±è´¥: {e}")
                logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
                db.rollback()
                continue
        
        return {
            "success": True,
            "inserted": success_count,
            "skipped": skip_count,
            "errors": error_count,
            "total": len(work_orders),
            "message": f"æ’å…¥å®Œæˆ: æˆåŠŸ{success_count}, è·³è¿‡{skip_count}, é”™è¯¯{error_count}"
        }
    
    def get_pending_work_orders(
        self,
        db: Session,
        ai_status: str = 'PENDING',
        limit: int = 100,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> List[Dict[str, Any]]:
        """è·å–å¾…å¤„ç†çš„å·¥å•
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            ai_status: AIå¤„ç†çŠ¶æ€
            limit: é™åˆ¶æ•°é‡
            start_date: å¼€å§‹æ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
            end_date: ç»“æŸæ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
        """
        try:
            # æ„å»ºWHEREæ¡ä»¶
            where_conditions = ["ai_status = :ai_status"]
            params = {
                "ai_status": ai_status,
                "limit": limit
            }
            
            # ğŸ”¥ æ–°å¢ï¼šæ”¯æŒæŒ‰å·¥å•åˆ›å»ºæ—¶é—´èŒƒå›´è¿‡æ»¤
            if start_date:
                where_conditions.append("create_time >= :start_date")
                params["start_date"] = start_date
            
            if end_date:
                where_conditions.append("create_time <= :end_date")
                params["end_date"] = end_date
            
            sql = f"""
            SELECT 
                id, work_id, work_table_name, comment_table_name,
                extract_date, create_time, work_type, work_state,
                create_by, create_name, ai_status, comment_count,
                has_comments, ai_retry_count, created_at
            FROM {self.pending_table_name}
            WHERE {' AND '.join(where_conditions)}
            ORDER BY created_at ASC
            LIMIT :limit
            """
            
            result = db.execute(text(sql), params)
            
            pending_orders = []
            for row in result:
                pending_orders.append({
                    "id": row.id,
                    "work_id": row.work_id,
                    "work_table_name": row.work_table_name,
                    "comment_table_name": row.comment_table_name,
                    "extract_date": row.extract_date,
                    "create_time": row.create_time,
                    "work_type": row.work_type,
                    "work_state": row.work_state,
                    "create_by": row.create_by,
                    "create_name": row.create_name,
                    "ai_status": row.ai_status,
                    "comment_count": row.comment_count,
                    "has_comments": bool(row.has_comments),
                    "ai_retry_count": row.ai_retry_count,
                    "created_at": row.created_at
                })
            
            # æ„å»ºæ—¥å¿—ä¿¡æ¯
            time_range_info = ""
            if start_date or end_date:
                time_parts = []
                if start_date:
                    time_parts.append(f"ä»{start_date.date()}")
                if end_date:
                    time_parts.append(f"åˆ°{end_date.date()}")
                time_range_info = f" ({' '.join(time_parts)})"
            
            logger.info(f"è·å–åˆ° {len(pending_orders)} ä¸ªçŠ¶æ€ä¸º {ai_status} çš„å·¥å•{time_range_info}")
            return pending_orders
            
        except Exception as e:
            logger.error(f"è·å–å¾…å¤„ç†å·¥å•å¤±è´¥: {e}")
            return []
    
    def update_work_order_ai_status(
        self,
        db: Session,
        work_id: int,
        ai_status: str,
        error_message: Optional[str] = None,
        comment_count: Optional[int] = None,
        has_comments: Optional[bool] = None
    ) -> bool:
        """æ›´æ–°å·¥å•AIå¤„ç†çŠ¶æ€"""
        try:
            update_fields = ["ai_status = :ai_status", "updated_at = :updated_at"]
            params = {
                "work_id": work_id,
                "ai_status": ai_status,
                "updated_at": datetime.now()
            }
            
            if ai_status == 'PROCESSING':
                update_fields.append("ai_process_start_time = :start_time")
                params["start_time"] = datetime.now()
            elif ai_status in ['COMPLETED', 'FAILED']:
                update_fields.append("ai_process_end_time = :end_time")
                params["end_time"] = datetime.now()
            
            if error_message is not None:
                update_fields.append("ai_error_message = :error_message")
                params["error_message"] = error_message
            
            if comment_count is not None:
                update_fields.append("comment_count = :comment_count")
                params["comment_count"] = comment_count
            
            if has_comments is not None:
                update_fields.append("has_comments = :has_comments")
                params["has_comments"] = has_comments
            
            if ai_status == 'FAILED':
                update_fields.append("ai_retry_count = ai_retry_count + 1")
            
            sql = f"""
            UPDATE {self.pending_table_name}
            SET {', '.join(update_fields)}
            WHERE work_id = :work_id
            """
            
            result = db.execute(text(sql), params)
            db.commit()
            
            if result.rowcount > 0:
                logger.info(f"å·¥å• {work_id} AIçŠ¶æ€æ›´æ–°ä¸º: {ai_status}")
                return True
            else:
                logger.warning(f"å·¥å• {work_id} æ›´æ–°å¤±è´¥ï¼Œæœªæ‰¾åˆ°è®°å½•")
                return False
                
        except Exception as e:
            db.rollback()
            logger.error(f"æ›´æ–°å·¥å• {work_id} AIçŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def reset_failed_work_orders_for_retry(
        self,
        db: Session,
        work_ids: List[int] = None,
        limit: int = None
    ) -> int:
        """é‡ç½®FAILEDçŠ¶æ€çš„å·¥å•ä¸ºPENDINGï¼Œä»¥ä¾¿é‡æ–°åˆ†æ"""
        try:
            # æ„å»ºWHEREæ¡ä»¶
            where_conditions = ["ai_status = 'FAILED'"]
            params = {}
            
            if work_ids:
                # é‡ç½®æŒ‡å®šçš„å·¥å•
                placeholders = ','.join([f':work_id_{i}' for i in range(len(work_ids))])
                where_conditions.append(f"work_id IN ({placeholders})")
                for i, work_id in enumerate(work_ids):
                    params[f'work_id_{i}'] = work_id
            
            where_clause = " AND ".join(where_conditions)
            limit_clause = f"LIMIT {limit}" if limit else ""
            
            # é‡ç½®çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯
            sql = f"""
            UPDATE {self.pending_table_name}
            SET 
                ai_status = 'PENDING',
                ai_error_message = NULL,
                updated_at = NOW()
            WHERE {where_clause}
            {limit_clause}
            """
            
            result = db.execute(text(sql), params)
            db.commit()
            
            reset_count = result.rowcount
            if reset_count > 0:
                logger.info(f"âœ… æˆåŠŸé‡ç½® {reset_count} ä¸ªFAILEDçŠ¶æ€å·¥å•ä¸ºPENDING")
            else:
                logger.info("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡ç½®çš„FAILEDçŠ¶æ€å·¥å•")
            
            return reset_count
            
        except Exception as e:
            db.rollback()
            logger.error(f"âŒ é‡ç½®FAILEDçŠ¶æ€å·¥å•å¤±è´¥: {e}")
            return 0
    
    # ==================== è¯„è®ºæ•°æ®å¤„ç†æ–¹æ³• ====================
    
    def get_work_comments(
        self,
        db: Session,
        work_id: int,
        comment_table_name: str
    ) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šå·¥å•çš„æ‰€æœ‰è¯„è®ºè®°å½•ï¼ˆä»…å¤„ç†äººè¯„è®ºï¼Œoper=1ï¼‰"""
        try:
            sql = f"""
            SELECT 
                id,
                work_id,
                user_type,
                user_id,
                name,
                content,
                create_time,
                oper,
                image,
                reissue
            FROM {comment_table_name}
            WHERE work_id = :work_id 
            AND deleted = 0
            AND oper = 1
            ORDER BY create_time ASC
            """
            
            result = db.execute(text(sql), {"work_id": work_id})
            
            comments = []
            for row in result:
                comments.append({
                    "id": row.id,
                    "work_id": row.work_id,
                    "user_type": row.user_type,
                    "user_id": row.user_id,
                    "name": row.name,
                    "content": row.content,
                    "create_time": row.create_time,
                    "oper": bool(row.oper) if row.oper is not None else False,
                    "image": row.image,
                    "reissue": row.reissue,
                    "source_table": comment_table_name
                })
            
            logger.info(f"ä»è¡¨ {comment_table_name} è·å–å·¥å• {work_id} çš„ {len(comments)} æ¡å¤„ç†äººè¯„è®ºï¼ˆoper=1ï¼‰")
            return comments
            
        except Exception as e:
            logger.error(f"ä»è¡¨ {comment_table_name} è·å–å·¥å• {work_id} è¯„è®ºå¤±è´¥: {e}")
            return []
    
    def build_conversation_text(self, comments: List[Dict[str, Any]]) -> str:
        """æ„å»ºå·¥å•å¯¹è¯æ–‡æœ¬"""
        if not comments:
            return ""
        
        conversation_parts = []
        
        for comment in comments:
            user_type = comment.get("user_type", "")
            name = comment.get("name", "")
            content = str(comment.get("content") or "")  # é˜²æ­¢NoneTypeé”™è¯¯
            oper = comment.get("oper", False)
            create_time = comment.get("create_time", "")
            
            # ç¡®å®šè§’è‰²æ˜¾ç¤ºåç§°
            if user_type == "customer":
                role = "å®¢æˆ·"
            elif user_type == "service" or oper:
                role = "å®¢æœ"
            elif user_type == "system":
                role = "ç³»ç»Ÿ"
            else:
                role = user_type or "æœªçŸ¥"
            
            # å¦‚æœæœ‰åç§°ï¼Œæ·»åŠ åˆ°è§’è‰²åé¢
            if name:
                role_display = f"{role}({name})"
            else:
                role_display = role
            
            # æ·»åŠ æ—¶é—´æˆ³
            time_str = ""
            if create_time:
                if isinstance(create_time, datetime):
                    time_str = create_time.strftime("%Y-%m-%d %H:%M:%S")
                else:
                    time_str = str(create_time)
            
            # æ„å»ºå¯¹è¯è¡Œ
            if time_str:
                conversation_parts.append(f"[{time_str}] {role_display}: {content}")
            else:
                conversation_parts.append(f"{role_display}: {content}")
        
        return "\n".join(conversation_parts)
    
    def get_work_comment_count(
        self,
        db: Session,
        work_id: int,
        comment_table_name: str
    ) -> int:
        """è·å–å·¥å•è¯„è®ºæ•°é‡"""
        logger.debug(f"ğŸ” è·å–å·¥å• {work_id} çš„è¯„è®ºæ•°é‡ï¼Œè¡¨: {comment_table_name}")
        try:
            # å…ˆæ£€æŸ¥è¯„è®ºè¡¨æ˜¯å¦å­˜åœ¨
            if not self.check_table_exists(db, comment_table_name):
                logger.warning(f"âš ï¸ è¯„è®ºè¡¨ {comment_table_name} ä¸å­˜åœ¨ï¼Œè¿”å›è¯„è®ºæ•°é‡ä¸º0")
                return 0
            
            sql = f"""
            SELECT COUNT(*) as comment_count
            FROM {comment_table_name}
            WHERE work_id = :work_id AND deleted = 0
            """
            
            logger.debug(f"SQLæŸ¥è¯¢: {sql}")
            logger.debug(f"å‚æ•°: work_id={work_id}")
            
            result = db.execute(text(sql), {"work_id": work_id})
            count = result.fetchone().comment_count
            
            logger.debug(f"âœ… å·¥å• {work_id} è¯„è®ºæ•°é‡: {count}")
            return count
            
        except Exception as e:
            logger.error(f"âŒ è·å–å·¥å• {work_id} è¯„è®ºæ•°é‡å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            return 0
    
    # ==================== ä¸»è¦ä¸šåŠ¡æµç¨‹æ–¹æ³• ====================
    
    def extract_work_data_by_time_range(
        self,
        db: Session,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        target_date: Optional[datetime] = None,
        days_back: int = 1
    ) -> Dict[str, Any]:
        """ç¬¬ä¸€é˜¶æ®µï¼šæ ¹æ®æ—¶é—´èŒƒå›´å¾ªç¯æ‰¹é‡æŠ½å–å·¥å•æ•°æ®å¹¶æ’å…¥å¾…å¤„ç†è¡¨"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®æŠ½å–ï¼ˆå¾ªç¯æ‰¹é‡æ¨¡å¼ï¼‰")
        logger.info(f"ğŸ“‹ å‚æ•°: start_time={start_time}, end_time={end_time}, target_date={target_date}, days_back={days_back}")
        
        try:
            # 1. å¾ªç¯æ‰¹é‡æŠ½å–å·¥å•æ•°æ®
            logger.info("ğŸ“ æ­¥éª¤1: å¾ªç¯æ‰¹é‡æŠ½å–å·¥å•æ•°æ®")
            all_work_orders = self._batch_extract_work_orders_by_time_range(
                db, start_time, end_time, target_date, days_back
            )
            
            # ç¡®å®šå®é™…ä½¿ç”¨çš„æ—¶é—´èŒƒå›´
            if start_time is not None and end_time is not None:
                actual_start_time = start_time
                actual_end_time = end_time
                actual_target_date = start_time.date()
            elif target_date is not None:
                actual_target_date = target_date.date()
                actual_start_time = target_date.replace(hour=0, minute=0, second=0, microsecond=0)
                actual_end_time = actual_start_time + timedelta(days=1)
            else:
                actual_target_date = (datetime.now() - timedelta(days=days_back)).date()
                actual_start_time = datetime.combine(actual_target_date, datetime.min.time())
                actual_end_time = actual_start_time + timedelta(days=1)
            
            if not all_work_orders:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦æŠ½å–çš„å·¥å•")
                return {
                    "success": True,
                    "stage": "ç¬¬ä¸€é˜¶æ®µï¼šå·¥å•æ•°æ®æŠ½å–ï¼ˆæ—¶é—´èŒƒå›´ï¼‰",
                    "target_date": actual_target_date.strftime("%Y-%m-%d"),
                    "time_range": {
                        "start_time": actual_start_time.strftime("%Y-%m-%d %H:%M:%S"),
                        "end_time": actual_end_time.strftime("%Y-%m-%d %H:%M:%S")
                    },
                    "days_back": days_back,
                    "statistics": {"extracted": 0, "inserted": 0, "skipped": 0, "updated": 0},
                    "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦æŠ½å–çš„å·¥å•"
                }
            
            logger.info(f"âœ… æ­¥éª¤1å®Œæˆ: æŠ½å–åˆ° {len(all_work_orders)} ä¸ªå·¥å•")
            
            # 2. æ’å…¥å¾…å¤„ç†è¡¨
            logger.info("ğŸ“ æ­¥éª¤2: æ’å…¥å¾…å¤„ç†è¡¨")
            insert_result = self.insert_pending_analysis_records(db, all_work_orders)
            inserted_count = insert_result.get("inserted", 0)
            skipped_count = insert_result.get("skipped", 0)
            logger.info(f"âœ… æ­¥éª¤2å®Œæˆ: {insert_result.get('message', 'æœªçŸ¥ç»“æœ')}")
            
            # 3. ğŸ”¥ ä¼˜åŒ–ï¼šæŸ¥è¯¢è¯„è®ºç»Ÿè®¡ä¿¡æ¯ï¼ˆæ’å…¥æ—¶å·²æ­£ç¡®è®¾ç½®ï¼Œæ— éœ€é‡å¤æ›´æ–°ï¼‰
            logger.info("ğŸ“ æ­¥éª¤3: ç»Ÿè®¡è¯„è®ºä¿¡æ¯")
            updated_count = inserted_count  # æ’å…¥æ—¶å·²æ­£ç¡®è®¾ç½®è¯„è®ºç»Ÿè®¡
            comment_stats = {"with_comments": 0, "without_comments": 0, "total_comments": 0}
            
            if inserted_count > 0:
                try:
                    # ğŸ”¥ ä¼˜åŒ–ï¼šç›´æ¥ä»æ•°æ®åº“æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…é‡å¤è®¡ç®—
                    stats_sql = f"""
                    SELECT 
                        COUNT(CASE WHEN has_comments = 1 THEN 1 END) as with_comments,
                        COUNT(CASE WHEN has_comments = 0 THEN 1 END) as without_comments,
                        SUM(comment_count) as total_comments
                    FROM {self.pending_table_name}
                    WHERE created_at >= :start_time
                    """
                    
                    # ä½¿ç”¨å½“å‰æ‰¹æ¬¡çš„å¼€å§‹æ—¶é—´ä½œä¸ºæŸ¥è¯¢æ¡ä»¶
                    batch_start = datetime.now() - timedelta(minutes=10)  # å‡è®¾æ‰¹æ¬¡åœ¨10åˆ†é’Ÿå†…å®Œæˆ
                    result = db.execute(text(stats_sql), {"start_time": batch_start})
                    row = result.fetchone()
                    
                    if row:
                        comment_stats["with_comments"] = row.with_comments or 0
                        comment_stats["without_comments"] = row.without_comments or 0
                        comment_stats["total_comments"] = row.total_comments or 0
                        logger.debug(f"ğŸ“Š æŸ¥è¯¢å¾—åˆ°è¯„è®ºç»Ÿè®¡: {comment_stats}")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æŸ¥è¯¢è¯„è®ºç»Ÿè®¡å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                    # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨ä¿å®ˆä¼°è®¡
                    comment_stats["with_comments"] = inserted_count
                    comment_stats["without_comments"] = 0
                    comment_stats["total_comments"] = inserted_count * 3  # ä¼°ç®—å¹³å‡3æ¡è¯„è®º
            
            logger.info(f"âœ… æ­¥éª¤3å®Œæˆ: æ’å…¥æ—¶å·²æ­£ç¡®è®¾ç½®è¯„è®ºç»Ÿè®¡ï¼Œå¤„ç† {updated_count} æ¡è®°å½•")
            
            result = {
                "extracted": len(all_work_orders),
                "inserted": inserted_count,
                "skipped": skipped_count,  # ğŸ”¥ æ–°å¢ï¼šè·³è¿‡çš„è®°å½•æ•°
                "updated": updated_count
            }
            
            # æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
            logger.info("=" * 50)
            logger.info("ğŸ“Š ç¬¬ä¸€é˜¶æ®µæ•°æ®æŠ½å–å®Œæˆç»Ÿè®¡:")
            logger.info(f"  ğŸ“¥ æŠ½å–å·¥å•æ•°: {result['extracted']}")
            logger.info(f"  ğŸ’¾ æ’å…¥è®°å½•æ•°: {result['inserted']}")
            logger.info(f"  â­ï¸ è·³è¿‡è®°å½•æ•°: {result['skipped']}")  # ğŸ”¥ æ–°å¢æ—¥å¿—
            logger.info(f"  ğŸ”„ æ›´æ–°è®°å½•æ•°: {result['updated']}")
            logger.info(f"  ğŸ’¬ æœ‰è¯„è®ºå·¥å•: {comment_stats['with_comments']}")
            logger.info(f"  ğŸ’­ æ— è¯„è®ºå·¥å•: {comment_stats['without_comments']}")
            logger.info(f"  ğŸ“ æ€»è¯„è®ºæ•°é‡: {comment_stats['total_comments']}")
            logger.info("=" * 50)
            
            return {
                "success": True,
                "stage": "ç¬¬ä¸€é˜¶æ®µï¼šå·¥å•æ•°æ®æŠ½å–ï¼ˆæ—¶é—´èŒƒå›´ï¼‰",
                "target_date": actual_target_date.strftime("%Y-%m-%d"),
                "time_range": {
                    "start_time": actual_start_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "end_time": actual_end_time.strftime("%Y-%m-%d %H:%M:%S"),
                    "duration_hours": round((actual_end_time - actual_start_time).total_seconds() / 3600, 1)
                },
                "days_back": days_back,
                "statistics": result,
                "comment_statistics": comment_stats,
                "message": f"æˆåŠŸæŠ½å– {result['extracted']} ä¸ªå·¥å•ï¼Œæ’å…¥ {result['inserted']} æ¡è®°å½•ï¼Œæ›´æ–° {result['updated']} æ¡è¯„è®ºç»Ÿè®¡"
            }
            
        except Exception as e:
            logger.error(f"âŒ ç¬¬ä¸€é˜¶æ®µæ•°æ®æŠ½å–å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "stage": "ç¬¬ä¸€é˜¶æ®µï¼šå·¥å•æ•°æ®æŠ½å–",
                "error": str(e),
                "message": "æ•°æ®æŠ½å–å¤±è´¥"
            }
    
    def extract_daily_work_data(
        self,
        db: Session,
        target_date: Optional[datetime] = None,
        days_back: int = 1
    ) -> Dict[str, Any]:
        """ç¬¬ä¸€é˜¶æ®µï¼šæŠ½å–æŒ‡å®šæ—¥æœŸçš„å·¥å•æ•°æ®å¹¶æ’å…¥å¾…å¤„ç†è¡¨ï¼ˆå…¼å®¹æ–¹æ³•ï¼‰"""
        logger.warning("âš ï¸ extract_daily_work_data æ–¹æ³•å·²è¿‡æ—¶ï¼Œè¯·ä½¿ç”¨ extract_work_data_by_time_range")
        return self.extract_work_data_by_time_range(
            db=db,
            target_date=target_date,
            days_back=days_back
        )
    
    def get_extraction_statistics(self, db: Session) -> Dict[str, Any]:
        """è·å–æ•°æ®æŠ½å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            sql = f"""
            SELECT 
                ai_status,
                COUNT(*) as count,
                COUNT(CASE WHEN has_comments = 1 THEN 1 END) as with_comments,
                COUNT(CASE WHEN has_comments = 0 THEN 1 END) as without_comments,
                AVG(comment_count) as avg_comment_count,
                MAX(created_at) as latest_extract,
                MIN(created_at) as earliest_extract
            FROM {self.pending_table_name}
            GROUP BY ai_status
            """
            
            result = db.execute(text(sql))
            
            statistics = {}
            total_count = 0
            total_with_comments = 0
            total_without_comments = 0
            
            for row in result:
                status = row.ai_status
                count = row.count
                with_comments = row.with_comments or 0
                without_comments = row.without_comments or 0
                
                statistics[status] = {
                    "count": count,
                    "with_comments": with_comments,
                    "without_comments": without_comments,
                    "avg_comment_count": float(row.avg_comment_count or 0),
                    "latest_extract": row.latest_extract,
                    "earliest_extract": row.earliest_extract
                }
                
                total_count += count
                total_with_comments += with_comments
                total_without_comments += without_comments
            
            return {
                "success": True,
                "by_status": statistics,
                "totals": {
                    "total_work_orders": total_count,
                    "total_with_comments": total_with_comments,
                    "total_without_comments": total_without_comments,
                    "comment_coverage_rate": round(total_with_comments / total_count * 100, 2) if total_count > 0 else 0
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–æŠ½å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }


# å…¨å±€ç¬¬ä¸€é˜¶æ®µæœåŠ¡å®ä¾‹
stage1_service = Stage1WorkExtractionService()
