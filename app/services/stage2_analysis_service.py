"""
ç¬¬äºŒé˜¶æ®µï¼šå·¥å•è¯„è®ºåˆ†ææœåŠ¡
è·å–å¾…å¤„ç†å·¥å•ï¼Œåˆ†æè¯„è®ºå†…å®¹ï¼Œæ‰§è¡ŒAIåˆ†æä»»åŠ¡ï¼ŒåŒ…å«æ‰¹é‡åˆ†æå’Œæ£€æµ‹å¼•æ“åŠŸèƒ½
"""
import json
import re
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from sqlalchemy import text
from sqlalchemy.orm import Session

from app.db.database import get_db
from app.services.stage1_work_extraction import stage1_service
from app.services.llm.llm_factory import get_llm_provider
from app.services.content_denoiser import content_denoiser
from app.services.keyword_config_manager import keyword_config_manager
from app.models.denoise import safe_json_dumps
from config.settings import settings

logger = logging.getLogger(__name__)


class Stage2AnalysisService:
    """ç¬¬äºŒé˜¶æ®µï¼šå·¥å•è¯„è®ºåˆ†ææœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–ç¬¬äºŒé˜¶æ®µæœåŠ¡"""
        self.stage1 = stage1_service
        self.pending_table_name = "ai_work_pending_analysis"
        self.results_table_name = "ai_work_comment_analysis_results"
        self.llm_provider = get_llm_provider()
        self.keywords_config = {}  # æ”¹ä¸ºä»æ•°æ®åº“åŠ¨æ€åŠ è½½
        self.few_shot_examples = self._init_few_shot_examples()
    
    # ==================== å¾…å¤„ç†å·¥å•è·å–æ–¹æ³• ====================
    
    def get_pending_work_orders_with_comments(
        self,
        db: Session,
        limit: int = 50,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """è·å–å¾…å¤„ç†å·¥å•åŠå…¶è¯„è®ºæ•°æ®
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            limit: é™åˆ¶æ•°é‡
            start_date: å¼€å§‹æ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
            end_date: ç»“æŸæ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
        """
        time_range_info = ""
        if start_date or end_date:
            time_parts = []
            if start_date:
                time_parts.append(f"ä»{start_date}")
            if end_date:
                time_parts.append(f"åˆ°{end_date}")
            time_range_info = f" ({' '.join(time_parts)})"
        
        logger.info(f"ğŸ“‹ å¼€å§‹æ‹‰å–pendingå·¥å•æ•°æ®ï¼Œé™åˆ¶ {limit} æ¡{time_range_info}")
        
        try:
            # 1. è·å–å¾…å¤„ç†å·¥å•åˆ—è¡¨ï¼ˆğŸ”¥ ä¿®å¤ï¼šåˆ†æé˜¶æ®µä¸ä½¿ç”¨æ—¶é—´è¿‡æ»¤ï¼Œå¤„ç†æ‰€æœ‰PENDINGï¼‰
            logger.info("ğŸ“¥ æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“ä¸­çš„PENDINGçŠ¶æ€å·¥å•...")
            pending_orders = self.stage1.get_pending_work_orders(
                db, ai_status='PENDING', limit=limit,
                start_date=start_date, end_date=end_date
            )
            logger.info(f"ğŸ“Š ä»æ•°æ®åº“æŸ¥è¯¢åˆ° {len(pending_orders) if pending_orders else 0} ä¸ªPENDINGå·¥å•")
            
            if not pending_orders:
                return {
                    "success": True,
                    "stage": "ç¬¬äºŒé˜¶æ®µï¼šè·å–å¾…å¤„ç†å·¥å•",
                    "message": "æ²¡æœ‰å¾…å¤„ç†çš„å·¥å•",
                    "work_orders": [],
                    "statistics": {
                        "total_pending": 0,
                        "with_comments": 0,
                        "without_comments": 0
                    }
                }
            
            # 2. æ‰¹é‡è·å–è¯„è®ºæ•°æ®
            logger.info(f"ğŸ’¬ å¼€å§‹å¤„ç† {len(pending_orders)} ä¸ªå·¥å•çš„è¯„è®ºæ•°æ®...")
            work_orders_with_comments = []
            with_comments_count = 0
            without_comments_count = 0
            denoised_count = 0  # ğŸ”¥ æ–°å¢ï¼šå»å™ªå¤„ç†çš„å·¥å•æ•°
            
            for i, order in enumerate(pending_orders, 1):
                work_id = order["work_id"]
                comment_table_name = order["comment_table_name"]
                
                logger.info(f"ğŸ“‹ å¤„ç†å·¥å• {work_id} ({i}/{len(pending_orders)}) - è¯„è®ºè¡¨: {comment_table_name}")
                
                # è·å–è¯„è®ºæ•°æ®
                comments = self.stage1.get_work_comments(db, work_id, comment_table_name)
                logger.info(f"ğŸ’­ å·¥å• {work_id} è·å–åˆ° {len(comments) if comments else 0} æ¡åŸå§‹è¯„è®º")
                
                # è¿‡æ»¤æœ‰æ•ˆè¯„è®º - é˜²æ­¢NoneTypeé”™è¯¯
                valid_comments = [c for c in comments if c.get("content") and str(c.get("content", "")).strip()]
                
                # åº”ç”¨å»å™ªè¿‡æ»¤å¹¶ä¿å­˜è®°å½•
                if valid_comments:
                    logger.info(f"ğŸ” å·¥å• {work_id} å¼€å§‹å»å™ªå¤„ç† {len(valid_comments)} æ¡æœ‰æ•ˆè¯„è®º...")
                    denoise_result = content_denoiser.filter_comments_with_record(
                        valid_comments, work_id, db, save_record=True
                    )
                    valid_comments = denoise_result["filtered_comments"]
                    logger.info(f"âœ… å·¥å• {work_id} å»å™ªå®Œæˆ: {denoise_result['original_count']} -> {denoise_result['filtered_count']} æ¡è¯„è®º")
                    if denoise_result["removed_count"] > 0:
                        denoised_count += 1  # ğŸ”¥ ç»Ÿè®¡å»å™ªå¤„ç†çš„å·¥å•æ•°
                        logger.info(f"ğŸ—‘ï¸ å·¥å• {work_id} å»å™ªç§»é™¤äº† {denoise_result['removed_count']} æ¡è¯„è®º")
                else:
                    logger.info(f"âš ï¸ å·¥å• {work_id} æ— æœ‰æ•ˆè¯„è®ºï¼Œè·³è¿‡å»å™ªå¤„ç†")
                    denoise_result = None
                
                if valid_comments:
                    with_comments_count += 1
                    comment_data = self._build_conversation_json(valid_comments)
                    logger.info(f"âœ… å·¥å• {work_id} æœ‰ {len(valid_comments)} æ¡æœ‰æ•ˆè¯„è®ºï¼Œæ„å»ºå®Œæˆå¯¹è¯æ•°æ®")
                    
                    # æ›´æ–°å·¥å•è¯„è®ºç»Ÿè®¡
                    self.stage1.update_work_order_ai_status(
                        db, work_id, 'PENDING',
                        comment_count=len(valid_comments),
                        has_comments=True
                    )
                else:
                    without_comments_count += 1
                    comment_data = None
                    
                    # ğŸ”¥ ä¼˜åŒ–ï¼šç©ºè¯„è®ºå·¥å•ç›´æ¥æ ‡è®°ä¸ºå®ŒæˆçŠ¶æ€ï¼Œä¸ä¿å­˜ä½é£é™©åˆ†æç»“æœ
                    logger.info(f"ğŸš« å·¥å• {work_id} æ²¡æœ‰è¯„è®ºï¼Œç›´æ¥æ ‡è®°ä¸ºå®Œæˆï¼ˆä¸ä¿å­˜åˆ†æç»“æœï¼‰")
                    self.stage1.update_work_order_ai_status(
                        db, work_id, 'COMPLETED',
                        comment_count=0,
                        has_comments=False,
                        error_message="è¯„è®ºä¸ºç©ºï¼Œä½é£é™©ä¸ä¿å­˜åˆ†æç»“æœ"
                    )
                    
                    # ğŸ”¥ ä¸å†ä¿å­˜ç©ºè¯„è®ºå·¥å•çš„åˆ†æç»“æœï¼Œå› ä¸ºéƒ½æ˜¯ä½é£é™©
                
                # æ„å»ºå®Œæ•´çš„å·¥å•æ•°æ®
                work_order_data = {
                    "pending_id": order["id"],
                    "work_id": work_id,
                    "work_table_name": order["work_table_name"],
                    "comment_table_name": order["comment_table_name"],
                    "extract_date": order["extract_date"],
                    "create_time": order["create_time"],
                    "work_type": order["work_type"],
                    "work_state": order["work_state"],
                    "create_by": order["create_by"],
                    "create_name": order["create_name"],
                    "ai_status": order["ai_status"],
                    "has_comments": comment_data is not None,
                    "comment_count": len(valid_comments) if valid_comments else 0,
                    "comments_data": comment_data  # åŒ…å«å®Œæ•´çš„è¯„è®ºæ•°æ®
                }
                
                work_orders_with_comments.append(work_order_data)
            
            result = {
                "success": True,
                "stage": "ç¬¬äºŒé˜¶æ®µï¼šè·å–å¾…å¤„ç†å·¥å•è¯„è®º",
                "message": f"è·å–åˆ° {len(pending_orders)} ä¸ªå¾…å¤„ç†å·¥å•ï¼Œå…¶ä¸­ {with_comments_count} ä¸ªæœ‰è¯„è®º",
                "work_orders": work_orders_with_comments,
                "statistics": {
                    "total_pending": len(pending_orders),
                    "with_comments": with_comments_count,
                    "without_comments": without_comments_count,
                    "denoised_count": denoised_count  # ğŸ”¥ æ–°å¢ï¼šå»å™ªå¤„ç†çš„å·¥å•æ•°
                }
            }
            
            logger.info("=" * 60)
            logger.info(f"ğŸ“‹ pendingå·¥å•æ•°æ®æ‹‰å–å®Œæˆæ€»ç»“:")
            logger.info(f"  ğŸ“¥ æŸ¥è¯¢åˆ°å·¥å•æ€»æ•°: {len(pending_orders)}")
            logger.info(f"  ğŸ’¬ æœ‰è¯„è®ºå¯åˆ†æ: {with_comments_count}")
            logger.info(f"  ğŸ’­ æ— è¯„è®ºå·²å®Œæˆ: {without_comments_count}")
            logger.info(f"  ğŸ” æ‰§è¡Œå»å™ªå¤„ç†: {denoised_count}")
            logger.info("=" * 60)
            return result
            
        except Exception as e:
            logger.error(f"ç¬¬äºŒé˜¶æ®µè·å–å¾…å¤„ç†å·¥å•è¯„è®ºå¤±è´¥: {e}")
            return {
                "success": False,
                "stage": "ç¬¬äºŒé˜¶æ®µï¼šè·å–å¾…å¤„ç†å·¥å•è¯„è®º",
                "error": str(e),
                "message": "è·å–è¯„è®ºæ•°æ®å¤±è´¥"
            }
    
    def _build_conversation_json(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ„å»ºå·¥å•å¯¹è¯JSONç»“æ„"""
        if not comments:
            return {
                "work_id": None,
                "total_messages": 0,
                "customer_messages": 0,
                "service_messages": 0,
                "system_messages": 0,
                "conversation_text": "",
                "messages": [],
                "session_info": {
                    "start_time": None,
                    "end_time": None,
                    "duration_minutes": 0
                }
            }
        
        work_id = comments[0]["work_id"]
        
        # ç»Ÿè®¡æ¶ˆæ¯ç±»å‹
        customer_count = 0
        service_count = 0
        system_count = 0
        
        messages = []
        
        for comment in comments:
            user_type = comment.get("user_type", "")
            oper = comment.get("oper", False)
            
            # ç»Ÿè®¡æ¶ˆæ¯æ•°é‡
            if user_type == "customer":
                customer_count += 1
            elif user_type == "service" or oper:
                service_count += 1
            elif user_type == "system":
                system_count += 1
            
            # æ„å»ºæ¶ˆæ¯å¯¹è±¡
            messages.append({
                "id": comment["id"],
                "user_type": user_type,
                "user_id": comment.get("user_id"),
                "name": comment.get("name"),
                "content": str(comment.get("content") or ""),  # é˜²æ­¢NoneTypeé”™è¯¯
                "create_time": comment["create_time"].isoformat() if isinstance(comment["create_time"], datetime) else str(comment["create_time"]),
                "oper": oper,
                "image": comment.get("image"),
                "reissue": comment.get("reissue", 0)
            })
        
        # è®¡ç®—ä¼šè¯æ—¶é•¿
        start_time = comments[0]["create_time"]
        end_time = comments[-1]["create_time"]
        duration_minutes = 0
        
        if isinstance(start_time, datetime) and isinstance(end_time, datetime):
            duration = end_time - start_time
            duration_minutes = duration.total_seconds() / 60
        
        # æ„å»ºå¯¹è¯æ–‡æœ¬
        conversation_text = self.stage1.build_conversation_text(comments)
        
        return {
            "work_id": work_id,
            "total_messages": len(comments),
            "customer_messages": customer_count,
            "service_messages": service_count,
            "system_messages": system_count,
            "conversation_text": conversation_text,
            "messages": messages,
            "session_info": {
                "start_time": start_time.isoformat() if isinstance(start_time, datetime) else str(start_time),
                "end_time": end_time.isoformat() if isinstance(end_time, datetime) else str(end_time),
                "duration_minutes": round(duration_minutes, 2)
            },
            "metadata": {
                "extracted_at": datetime.now().isoformat(),
                "source_table": comments[0].get("source_table", ""),
                "comment_ids": [c["id"] for c in comments]
            }
        }
    
    # ==================== å»å™ªå¤„ç†æ–¹æ³• ====================
    
    def apply_denoise_to_work_orders(
        self,
        work_orders: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """å¯¹å·¥å•åˆ—è¡¨åº”ç”¨å»å™ªå¤„ç†"""
        logger.info(f"ğŸ” å¼€å§‹å¯¹ {len(work_orders)} ä¸ªå·¥å•åº”ç”¨å»å™ªå¤„ç†")
        
        try:
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å»å™ªå™¨è¿›è¡Œæ‰¹é‡å¤„ç†ï¼Œä¼ é€’æ•°æ®åº“ä¼šè¯ä»¥ä¿å­˜è®°å½•
            from app.db.connection_manager import get_db_session
            
            with get_db_session() as db_session:
                denoise_result = content_denoiser.batch_filter_work_orders(
                    work_orders, db=db_session, save_records=True
                )
            
            logger.info("ğŸ‰ æ‰¹é‡å»å™ªå¤„ç†å®Œæˆ:")
            logger.info(f"  ğŸ“‹ å¤„ç†å·¥å•æ•°: {denoise_result['total_work_orders']}")
            logger.info(f"  ğŸ“¥ åŸå§‹è¯„è®ºæ€»æ•°: {denoise_result['statistics']['total_original_comments']}")
            logger.info(f"  ğŸ“¤ è¿‡æ»¤åè¯„è®ºæ€»æ•°: {denoise_result['statistics']['total_filtered_comments']}")
            logger.info(f"  ğŸ—‘ï¸ ç§»é™¤è¯„è®ºæ€»æ•°: {denoise_result['statistics']['total_removed_comments']}")
            logger.info(f"  ğŸ“Š æ•´ä½“è¿‡æ»¤ç‡: {denoise_result['statistics']['overall_filter_rate']:.1f}%")
            
            if denoise_result['statistics']['filter_reasons']:
                logger.info("ğŸ” è¿‡æ»¤åŸå› ç»Ÿè®¡:")
                for reason, count in denoise_result['statistics']['filter_reasons'].items():
                    logger.info(f"  - {reason}: {count} æ¡")
            
            return {
                "success": True,
                "processed_orders": denoise_result["processed_orders"],
                "denoise_statistics": denoise_result["statistics"],
                "message": f"æˆåŠŸå¯¹ {denoise_result['total_work_orders']} ä¸ªå·¥å•åº”ç”¨å»å™ªï¼Œæ€»ä½“è¿‡æ»¤ç‡ {denoise_result['statistics']['overall_filter_rate']:.1f}%"
            }
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å»å™ªå¤„ç†å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "message": "å»å™ªå¤„ç†å¤±è´¥"
            }
    
    # ==================== å•ä¸ªå·¥å•å¤„ç†æ–¹æ³• ====================
    
    def process_single_work_order(
        self,
        db: Session,
        work_id: int
    ) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªå·¥å•çš„å®Œæ•´æµç¨‹"""
        logger.info(f"å¼€å§‹å¤„ç†å•ä¸ªå·¥å•: {work_id}")
        
        try:
            # 1. æŸ¥æ‰¾å·¥å•åœ¨å¾…å¤„ç†è¡¨ä¸­çš„è®°å½•
            pending_orders = self.stage1.get_pending_work_orders(db, limit=1000)
            target_order = None
            
            for order in pending_orders:
                if order["work_id"] == work_id:
                    target_order = order
                    break
            
            if not target_order:
                return {
                    "success": False,
                    "work_id": work_id,
                    "message": f"å·¥å• {work_id} ä¸åœ¨å¾…å¤„ç†åˆ—è¡¨ä¸­"
                }
            
            # 2. æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            self.stage1.update_work_order_ai_status(
                db, work_id, 'PROCESSING'
            )
            
            # 3. è·å–è¯„è®ºæ•°æ®
            comments = self.stage1.get_work_comments(
                db, work_id, target_order["comment_table_name"]
            )
            
            # è¿‡æ»¤æœ‰æ•ˆè¯„è®º - é˜²æ­¢NoneTypeé”™è¯¯
            valid_comments = [c for c in comments if c.get("content") and str(c.get("content", "")).strip()]
            
            # åº”ç”¨å»å™ªè¿‡æ»¤å¹¶ä¿å­˜è®°å½•
            if valid_comments:
                denoise_result = content_denoiser.filter_comments_with_record(
                    valid_comments, work_id, db, save_record=True
                )
                valid_comments = denoise_result["filtered_comments"]
                logger.info(f"ğŸ” å·¥å• {work_id} å»å™ªç»“æœ: {denoise_result['original_count']} -> {denoise_result['filtered_count']} æ¡è¯„è®º")
                if denoise_result["removed_count"] > 0:
                    logger.debug(f"å»å™ªç§»é™¤: {denoise_result['filter_statistics']['filter_reasons']}")
                # è®°å½•å»å™ªä¿å­˜çŠ¶æ€
                if denoise_result.get("denoise_record", {}).get("saved"):
                    logger.debug(f"ğŸ’¾ å·¥å• {work_id} å»å™ªè®°å½•å·²ä¿å­˜ï¼Œæ‰¹æ¬¡: {denoise_result['denoise_record']['batch_id']}")
            
            comment_data = self._build_conversation_json(valid_comments) if valid_comments else None
            
            # 4. æ„å»ºç»“æœ
            result = {
                "success": True,
                "work_id": work_id,
                "pending_id": target_order["id"],
                "work_table_name": target_order["work_table_name"],
                "comment_table_name": target_order["comment_table_name"],
                "work_info": {
                    "extract_date": target_order["extract_date"],
                    "create_time": target_order["create_time"],
                    "work_type": target_order["work_type"],
                    "work_state": target_order["work_state"],
                    "create_by": target_order["create_by"],
                    "create_name": target_order["create_name"]
                },
                "has_comments": comment_data is not None,
                "comment_count": len(valid_comments) if valid_comments else 0,
                "comments_data": comment_data,
                "ai_status": "PROCESSING"
            }
            
            # 5. æ›´æ–°è¯„è®ºç»Ÿè®¡
            self.stage1.update_work_order_ai_status(
                db, work_id, 'PROCESSING',
                comment_count=result["comment_count"],
                has_comments=result["has_comments"]
            )
            
            logger.info(f"å·¥å• {work_id} å¤„ç†å®Œæˆï¼Œæœ‰è¯„è®º: {result['has_comments']}")
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†å·¥å• {work_id} å¤±è´¥: {e}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            try:
                self.stage1.update_work_order_ai_status(
                    db, work_id, 'FAILED', error_message=str(e)
                )
            except:
                pass
            
            return {
                "success": False,
                "work_id": work_id,
                "error": str(e),
                "message": "å¤„ç†å¤±è´¥"
            }
    
    # ==================== AIåˆ†æç»“æœä¿å­˜æ–¹æ³• ====================
    
    def _get_order_info_by_work_id(self, db: Session, work_id: int) -> tuple[Optional[int], Optional[str]]:
        """æ ¹æ®å·¥å•IDæŸ¥è¯¢è®¢å•IDå’Œè®¢å•ç¼–å·"""
        try:
            # è·å–å½“å‰å¹´ä»½ï¼Œæ„é€ å·¥å•è¡¨å
            current_year = datetime.now().year
            work_order_table = f"t_work_order_{current_year}"
            
            sql = f"""
            SELECT order_id, order_no FROM {work_order_table} 
            WHERE id = :work_id 
            LIMIT 1
            """
            
            logger.debug(f"ğŸ” æŸ¥è¯¢å·¥å• {work_id} çš„è®¢å•ä¿¡æ¯ï¼Œä½¿ç”¨è¡¨: {work_order_table}")
            
            result = db.execute(text(sql), {"work_id": work_id}).fetchone()
            
            if result:
                order_id = result[0]
                order_no = result[1]
                logger.debug(f"âœ… å·¥å• {work_id} å¯¹åº”çš„è®¢å•ä¿¡æ¯: order_id={order_id}, order_no={order_no}")
                return order_id, order_no
            else:
                logger.warning(f"âš ï¸ åœ¨è¡¨ {work_order_table} ä¸­æœªæ‰¾åˆ°å·¥å• {work_id} å¯¹åº”çš„è®¢å•ä¿¡æ¯")
                return None, None
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å·¥å• {work_id} çš„è®¢å•ä¿¡æ¯å¤±è´¥ï¼Œè¡¨: {work_order_table}, é”™è¯¯: {e}")
            return None, None

    def save_analysis_result(
        self,
        db: Session,
        work_id: int,
        analysis_result: Dict[str, Any]
    ) -> bool:
        """ä¿å­˜AIåˆ†æç»“æœåˆ°ç»“æœè¡¨"""
        logger.info(f"ğŸ’¾ ä¿å­˜å·¥å• {work_id} åˆ†æç»“æœ: é£é™©çº§åˆ«={analysis_result.get('risk_level', 'low')}, è§„é¿è´£ä»»={analysis_result.get('has_evasion', False)}")
        
        try:
            # æŸ¥è¯¢è®¢å•IDå’Œè®¢å•ç¼–å·
            order_id, order_no = self._get_order_info_by_work_id(db, work_id)
            
            # ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ INSERT ... ON DUPLICATE KEY UPDATE è¯­æ³•é¿å…é‡å¤æ’å…¥
            # è¿™é‡Œä½¿ç”¨ MySQL çš„ UPSERT è¯­æ³•ï¼Œå¯ä»¥åŸå­æ€§åœ°å¤„ç†æ’å…¥æˆ–æ›´æ–°
            upsert_sql = f"""
            INSERT INTO {self.results_table_name} (
                work_id, order_id, order_no, session_start_time, session_end_time,
                total_comments, customer_comments, service_comments,
                has_evasion, risk_level, confidence_score,
                evasion_types, evidence_sentences, improvement_suggestions,
                keyword_screening_score, matched_categories, matched_keywords, is_suspicious,
                sentiment, sentiment_intensity, conversation_text,
                llm_raw_response, analysis_details, analysis_note,
                llm_provider, llm_model, llm_tokens_used,
                analysis_time, created_at, updated_at
            ) VALUES (
                :work_id, :order_id, :order_no, :session_start_time, :session_end_time,
                :total_comments, :customer_comments, :service_comments,
                :has_evasion, :risk_level, :confidence_score,
                :evasion_types, :evidence_sentences, :improvement_suggestions,
                :keyword_screening_score, :matched_categories, :matched_keywords, :is_suspicious,
                :sentiment, :sentiment_intensity, :conversation_text,
                :llm_raw_response, :analysis_details, :analysis_note,
                :llm_provider, :llm_model, :llm_tokens_used,
                :analysis_time, :created_at, :updated_at
            ) ON DUPLICATE KEY UPDATE
                order_id = VALUES(order_id),
                order_no = VALUES(order_no),
                session_start_time = VALUES(session_start_time),
                session_end_time = VALUES(session_end_time),
                total_comments = VALUES(total_comments),
                customer_comments = VALUES(customer_comments),
                service_comments = VALUES(service_comments),
                has_evasion = VALUES(has_evasion),
                risk_level = VALUES(risk_level),
                confidence_score = VALUES(confidence_score),
                evasion_types = VALUES(evasion_types),
                evidence_sentences = VALUES(evidence_sentences),
                improvement_suggestions = VALUES(improvement_suggestions),
                keyword_screening_score = VALUES(keyword_screening_score),
                matched_categories = VALUES(matched_categories),
                matched_keywords = VALUES(matched_keywords),
                is_suspicious = VALUES(is_suspicious),
                sentiment = VALUES(sentiment),
                sentiment_intensity = VALUES(sentiment_intensity),
                conversation_text = VALUES(conversation_text),
                llm_raw_response = VALUES(llm_raw_response),
                analysis_details = VALUES(analysis_details),
                analysis_note = VALUES(analysis_note),
                llm_provider = VALUES(llm_provider),
                llm_model = VALUES(llm_model),
                llm_tokens_used = VALUES(llm_tokens_used),
                analysis_time = VALUES(analysis_time),
                updated_at = VALUES(updated_at)
            """
            
            params = self._build_analysis_params(work_id, analysis_result, order_id, order_no)
            params["created_at"] = datetime.now()
            params["updated_at"] = datetime.now()
            
            result = db.execute(text(upsert_sql), params)
            
            # æ£€æŸ¥æ˜¯æ’å…¥è¿˜æ˜¯æ›´æ–°
            if result.rowcount == 1:
                logger.info(f"âœ… æˆåŠŸæ’å…¥å·¥å• {work_id} çš„åˆ†æç»“æœ")
            elif result.rowcount == 2:
                logger.info(f"âœ… æˆåŠŸæ›´æ–°å·¥å• {work_id} çš„åˆ†æç»“æœ")
            else:
                logger.warning(f"âš ï¸ å·¥å• {work_id} ä¿å­˜ç»“æœå¼‚å¸¸: rowcount={result.rowcount}")
            
            db.commit()
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜å·¥å• {work_id} åˆ†æç»“æœå¤±è´¥: {e}")
            # ğŸ”¥ ä¿®å¤ï¼šå¦‚æœæ˜¯é‡å¤é”®é”™è¯¯ï¼Œå¯èƒ½æ˜¯å¹¶å‘å¯¼è‡´çš„ï¼Œä¸ç®—çœŸæ­£å¤±è´¥
            if "Duplicate entry" in str(e) or "UNIQUE constraint failed" in str(e):
                logger.warning(f"âš ï¸ å·¥å• {work_id} æ£€æµ‹åˆ°é‡å¤é”®ï¼Œå¯èƒ½æ˜¯å¹¶å‘æ’å…¥ï¼Œå¿½ç•¥æ­¤é”™è¯¯")
                db.rollback()
                return True  # é‡å¤é”®ä¸ç®—å¤±è´¥ï¼Œå› ä¸ºæ•°æ®å·²ç»å­˜åœ¨
            db.rollback()
            return False
    
    def _safe_truncate_text(self, text: str, max_length: int, suffix: str = "...") -> str:
        """å®‰å…¨æˆªæ–­æ–‡æœ¬ï¼Œç¡®ä¿ä¸è¶…å‡ºæŒ‡å®šé•¿åº¦"""
        if not text or len(text) <= max_length:
            return text
        
        actual_max = max_length - len(suffix)
        if actual_max <= 0:
            return suffix[:max_length]
        
        return text[:actual_max] + suffix
    
    def _safe_truncate_json(self, data: Any, max_length: int) -> str:
        """å®‰å…¨æˆªæ–­JSONæ•°æ®ï¼Œç¡®ä¿ä¸è¶…å‡ºæŒ‡å®šé•¿åº¦"""
        try:
            json_str = safe_json_dumps(data, ensure_ascii=False)
            if len(json_str) <= max_length:
                return json_str
            
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•å‡å°‘å…ƒç´ æ•°é‡
            if isinstance(data, list) and len(data) > 1:
                reduced_count = max(1, len(data) // 2)
                truncated_data = data[:reduced_count]
                # æ·»åŠ æˆªæ–­æ ‡è®°
                if isinstance(truncated_data[0], str):
                    truncated_data.append(f"... (å·²æˆªæ–­ï¼ŒåŸå§‹å…±{len(data)}é¡¹)")
                json_str = safe_json_dumps(truncated_data, ensure_ascii=False)
                
                # å¦‚æœè¿˜æ˜¯å¤ªé•¿ï¼Œç›´æ¥æˆªæ–­å­—ç¬¦ä¸²
                if len(json_str) > max_length:
                    return self._safe_truncate_text(json_str, max_length)
                return json_str
            
            # å¯¹äºå…¶ä»–ç±»å‹ï¼Œç›´æ¥æˆªæ–­å­—ç¬¦ä¸²
            return self._safe_truncate_text(json_str, max_length)
            
        except Exception as e:
            logger.warning(f"JSONæˆªæ–­å¤±è´¥: {e}")
            return f'{{"error": "æ•°æ®è¿‡é•¿å·²æˆªæ–­", "original_type": "{type(data).__name__}"}}'

    def _build_analysis_params(self, work_id: int, analysis_result: Dict[str, Any], order_id: Optional[int] = None, order_no: Optional[str] = None) -> Dict[str, Any]:
        """æ„å»ºåˆ†æç»“æœå‚æ•°ï¼Œç¡®ä¿æ‰€æœ‰å­—æ®µä¸è¶…å‡ºæ•°æ®åº“é™åˆ¶"""
        import json
        
        # å®šä¹‰å­—æ®µé•¿åº¦é™åˆ¶ï¼ˆæ ¹æ®æ•°æ®åº“è¡¨ç»“æ„è®¾ç½®ï¼‰
        FIELD_LIMITS = {
            "conversation_text": 8000,      # TEXTå­—æ®µé€šå¸¸8KBå·¦å³
            "llm_raw_response": 4000,       # JSONå­—æ®µ
            "analysis_details": 4000,       # JSONå­—æ®µ
            "evidence_sentences": 3000,     # JSONå­—æ®µ
            "improvement_suggestions": 2000, # JSONå­—æ®µ
            "evasion_types": 1000,          # JSONå­—æ®µ
            "matched_keywords": 2000,       # JSONå­—æ®µ
            "analysis_note": 1500,          # å·²åœ¨_build_enhanced_analysis_noteä¸­å¤„ç†
            "matched_categories": 500       # VARCHARå­—æ®µ
        }
        
        # è·å–å…³é”®è¯ç­›é€‰ç»“æœ
        keyword_screening = analysis_result.get("keyword_screening", {})
        
        # è·å–LLMåŸå§‹å“åº”
        llm_raw_response = analysis_result.get("llm_raw_response", {})
        
        # ç¡®å®šLLMæä¾›å•†å’Œæ¨¡å‹ä¿¡æ¯
        llm_provider = None
        llm_model = None
        llm_tokens_used = 0
        
        if isinstance(llm_raw_response, dict):
            llm_provider = llm_raw_response.get("provider") or getattr(settings, "llm_provider", "unknown")
            # æ ¹æ®æä¾›å•†è·å–æ¨¡å‹ä¿¡æ¯
            if settings.llm_provider == "volcengine":
                default_model = getattr(settings, "volcengine_model", "unknown")
            elif settings.llm_provider == "siliconflow":
                default_model = getattr(settings, "siliconflow_model", "unknown")
            else:
                default_model = "unknown"
            
            # è·å–æ¨¡å‹ä¿¡æ¯ - æ”¯æŒå¤šç§æ•°æ®ç»“æ„
            llm_model = llm_raw_response.get("model")
            if not llm_model and "raw_response" in llm_raw_response:
                raw_resp = llm_raw_response["raw_response"]
                if isinstance(raw_resp, dict):
                    llm_model = raw_resp.get("model")
            if not llm_model:
                llm_model = default_model
            
            # è§£ætokenæ¶ˆè€— - æ”¯æŒå¤šç§æ•°æ®ç»“æ„
            llm_tokens_used = 0
            if "tokens_used" in llm_raw_response:
                llm_tokens_used = llm_raw_response["tokens_used"]
            elif "usage" in llm_raw_response:
                usage = llm_raw_response["usage"]
                if isinstance(usage, dict):
                    llm_tokens_used = usage.get("total_tokens", 0)
            elif "raw_response" in llm_raw_response:
                raw_resp = llm_raw_response["raw_response"]
                if isinstance(raw_resp, dict) and "usage" in raw_resp:
                    usage = raw_resp["usage"]
                    if isinstance(usage, dict):
                        llm_tokens_used = usage.get("total_tokens", 0)
        
        # å®‰å…¨å¤„ç†åŒ¹é…ç±»åˆ«å­—æ®µ
        matched_categories_str = None
        if keyword_screening.get("matched_categories"):
            categories_list = keyword_screening["matched_categories"][:10]  # æœ€å¤š10ä¸ªç±»åˆ«
            categories_str = ",".join(categories_list)
            matched_categories_str = self._safe_truncate_text(categories_str, FIELD_LIMITS["matched_categories"])
        
        # æ„å»ºä¿å­˜å‚æ•°å­—å…¸ï¼Œåº”ç”¨é•¿åº¦é™åˆ¶
        save_params = {
            "work_id": work_id,
            "order_id": order_id,
            "order_no": order_no,
            "session_start_time": analysis_result.get("session_start_time"),
            "session_end_time": analysis_result.get("session_end_time"),
            "total_comments": analysis_result.get("total_comments", 0),
            "customer_comments": analysis_result.get("customer_messages", 0),
            "service_comments": analysis_result.get("service_messages", 0),
            "has_evasion": 1 if analysis_result.get("has_evasion", False) else 0,
            "risk_level": analysis_result.get("risk_level", "low"),
            "confidence_score": analysis_result.get("confidence_score", 0.0),
            # JSONå­—æ®µ - åº”ç”¨é•¿åº¦é™åˆ¶
            "evasion_types": self._safe_truncate_json(analysis_result.get("evasion_types", []), FIELD_LIMITS["evasion_types"]),
            "evidence_sentences": self._safe_truncate_json(analysis_result.get("evidence_sentences", []), FIELD_LIMITS["evidence_sentences"]),
            "improvement_suggestions": self._safe_truncate_json(analysis_result.get("improvement_suggestions", []), FIELD_LIMITS["improvement_suggestions"]),
            # å…³é”®è¯ç­›é€‰ç»“æœ
            "keyword_screening_score": keyword_screening.get("confidence_score", 0.0),
            "matched_categories": matched_categories_str,
            "matched_keywords": self._safe_truncate_json(keyword_screening.get("matched_details", {}), FIELD_LIMITS["matched_keywords"]) if keyword_screening.get("matched_details") else None,
            "is_suspicious": 1 if keyword_screening.get("is_suspicious", False) else 0,
            # æƒ…æ„Ÿåˆ†æç»“æœ
            "sentiment": analysis_result.get("sentiment", "neutral"),
            "sentiment_intensity": analysis_result.get("sentiment_intensity", 0.0),
            # åŸå§‹æ•°æ® - åº”ç”¨é•¿åº¦é™åˆ¶
            "conversation_text": self._safe_truncate_text(analysis_result.get("conversation_text", ""), FIELD_LIMITS["conversation_text"]),
            "llm_raw_response": self._safe_truncate_json(llm_raw_response, FIELD_LIMITS["llm_raw_response"]) if llm_raw_response else None,
            "analysis_details": self._safe_truncate_json(analysis_result, FIELD_LIMITS["analysis_details"]),
            "analysis_note": self._build_enhanced_analysis_note(analysis_result),  # å†…éƒ¨å·²å¤„ç†é•¿åº¦é™åˆ¶
            # LLMè°ƒç”¨ä¿¡æ¯
            "llm_provider": llm_provider,
            "llm_model": llm_model,
            "llm_tokens_used": llm_tokens_used,
            # æ—¶é—´æˆ³
            "analysis_time": datetime.now()
        }
        
        return save_params
    
    def mark_work_order_completed(
        self,
        db: Session,
        work_id: int,
        analysis_result: Optional[Dict[str, Any]] = None
    ) -> bool:
        """æ ‡è®°å·¥å•ä¸ºå·²å®Œæˆ"""
        try:
            # ä¿å­˜åˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
            if analysis_result:
                self.save_analysis_result(db, work_id, analysis_result)
            
            # æ›´æ–°å¾…å¤„ç†è¡¨çŠ¶æ€
            success = self.stage1.update_work_order_ai_status(
                db, work_id, 'COMPLETED'
            )
            
            if success:
                logger.info(f"å·¥å• {work_id} æ ‡è®°ä¸ºå·²å®Œæˆ")
            
            return success
            
        except Exception as e:
            logger.error(f"æ ‡è®°å·¥å• {work_id} ä¸ºå·²å®Œæˆå¤±è´¥: {e}")
            return False
    
    def mark_work_order_failed(
        self,
        db: Session,
        work_id: int,
        error_message: str
    ) -> bool:
        """æ ‡è®°å·¥å•å¤„ç†å¤±è´¥"""
        try:
            success = self.stage1.update_work_order_ai_status(
                db, work_id, 'FAILED', error_message=error_message
            )
            
            if success:
                logger.info(f"å·¥å• {work_id} æ ‡è®°ä¸ºå¤„ç†å¤±è´¥: {error_message}")
            
            return success
            
        except Exception as e:
            logger.error(f"æ ‡è®°å·¥å• {work_id} ä¸ºå¤±è´¥å¤±è´¥: {e}")
            return False
    
    def _atomic_mark_processing(self, db: Session, work_id: int) -> bool:
        """åŸå­æ€§åœ°æ ‡è®°å·¥å•ä¸ºå¤„ç†ä¸­çŠ¶æ€
        
        ä½¿ç”¨æ•°æ®åº“åŸå­æ“ä½œï¼Œåªæœ‰å½“å·¥å•çŠ¶æ€ä¸ºPENDINGæ—¶æ‰æ›´æ–°ä¸ºPROCESSING
        è¿™æ ·å¯ä»¥é˜²æ­¢å¤šä¸ªè¿›ç¨‹åŒæ—¶å¤„ç†åŒä¸€ä¸ªå·¥å•
        
        Returns:
            bool: Trueè¡¨ç¤ºæˆåŠŸæ ‡è®°ä¸ºå¤„ç†ä¸­ï¼ŒFalseè¡¨ç¤ºå·¥å•å·²åœ¨å¤„ç†ä¸­æˆ–ä¸å­˜åœ¨
        """
        try:
            # ğŸ”¥ åŸå­æ€§æ›´æ–°ï¼šåªæœ‰å½“çŠ¶æ€ä¸ºPENDINGæ—¶æ‰æ›´æ–°ä¸ºPROCESSING
            update_sql = f"""
            UPDATE {self.pending_table_name}
            SET 
                ai_status = 'PROCESSING',
                ai_process_start_time = :start_time,
                updated_at = :updated_at
            WHERE work_id = :work_id 
            AND ai_status = 'PENDING'
            """
            
            params = {
                "work_id": work_id,
                "start_time": datetime.now(),
                "updated_at": datetime.now()
            }
            
            result = db.execute(text(update_sql), params)
            db.commit()
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸæ›´æ–°ï¼ˆaffected_rows > 0 è¡¨ç¤ºæˆåŠŸï¼‰
            success = result.rowcount > 0
            
            if success:
                logger.debug(f"ğŸ”’ å·¥å• {work_id} æˆåŠŸæ ‡è®°ä¸ºå¤„ç†ä¸­çŠ¶æ€")
            else:
                logger.debug(f"âš ï¸ å·¥å• {work_id} æœªèƒ½æ ‡è®°ä¸ºå¤„ç†ä¸­ï¼ˆå¯èƒ½å·²åœ¨å¤„ç†ä¸­æˆ–ä¸å­˜åœ¨ï¼‰")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ åŸå­æ€§æ ‡è®°å·¥å• {work_id} ä¸ºå¤„ç†ä¸­å¤±è´¥: {e}")
            db.rollback()
            return False
    
    # ==================== åˆ†æä»»åŠ¡ç®¡ç†æ–¹æ³• ====================
    
    def get_analysis_queue_status(self, db: Session) -> Dict[str, Any]:
        """è·å–åˆ†æé˜Ÿåˆ—çŠ¶æ€"""
        try:
            sql = f"""
            SELECT 
                ai_status,
                COUNT(*) as count,
                AVG(comment_count) as avg_comments
            FROM {self.pending_table_name}
            WHERE has_comments = 1
            GROUP BY ai_status
            """
            
            result = db.execute(text(sql))
            
            queue_status = {}
            for row in result:
                queue_status[row.ai_status] = {
                    "count": row.count,
                    "avg_comments": float(row.avg_comments or 0)
                }
            
            return {
                "success": True,
                "queue_status": queue_status,
                "summary": {
                    "pending": queue_status.get("PENDING", {}).get("count", 0),
                    "processing": queue_status.get("PROCESSING", {}).get("count", 0),
                    "completed": queue_status.get("COMPLETED", {}).get("count", 0),
                    "failed": queue_status.get("FAILED", {}).get("count", 0)
                }
            }
            
        except Exception as e:
            logger.error(f"è·å–åˆ†æé˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def cleanup_old_results(
        self,
        db: Session,
        days_to_keep: int = 90
    ) -> Dict[str, Any]:
        """æ¸…ç†æ—§çš„åˆ†æç»“æœ"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            # æ¸…ç†åˆ†æç»“æœè¡¨
            results_sql = f"""
            DELETE FROM {self.results_table_name}
            WHERE created_at < :cutoff_date
            """
            
            result = db.execute(text(results_sql), {"cutoff_date": cutoff_date})
            results_deleted = result.rowcount
            
            # æ¸…ç†å·²å®Œæˆçš„å¾…å¤„ç†è®°å½•
            pending_sql = f"""
            DELETE FROM {self.pending_table_name}
            WHERE created_at < :cutoff_date
            AND ai_status IN ('COMPLETED', 'FAILED')
            """
            
            result = db.execute(text(pending_sql), {"cutoff_date": cutoff_date})
            pending_deleted = result.rowcount
            
            db.commit()
            
            logger.info(f"æ¸…ç†äº† {results_deleted} æ¡åˆ†æç»“æœï¼Œ{pending_deleted} æ¡å¾…å¤„ç†è®°å½•")
            
            return {
                "success": True,
                "results_deleted": results_deleted,
                "pending_deleted": pending_deleted,
                "cutoff_date": cutoff_date.strftime("%Y-%m-%d %H:%M:%S"),
                "message": f"æˆåŠŸæ¸…ç† {results_deleted + pending_deleted} æ¡æ—§è®°å½•"
            }
            
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§è®°å½•å¤±è´¥: {e}")
            db.rollback()
            return {
                "success": False,
                "error": str(e),
                "message": "æ¸…ç†å¤±è´¥"
            }
    
    # ==================== æ£€æµ‹å¼•æ“æ–¹æ³• ====================
    
    def _load_keywords_config(self, db: Session) -> Dict[str, Dict[str, Any]]:
        """ä»æ•°æ®åº“åŠ è½½å…³é”®è¯é…ç½®"""
        try:
            logger.debug("ä»æ•°æ®åº“åŠ è½½å…³é”®è¯é…ç½®")
            config = keyword_config_manager.get_analysis_keywords_config(db)
            if config:
                logger.info(f"æˆåŠŸä»æ•°æ®åº“åŠ è½½ {len(config)} ä¸ªå…³é”®è¯é…ç½®åˆ†ç±»")
                return config
            else:
                logger.warning("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å…³é”®è¯é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return self._get_fallback_keywords_config()
        except Exception as e:
            logger.error(f"ä»æ•°æ®åº“åŠ è½½å…³é”®è¯é…ç½®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self._get_fallback_keywords_config()

    def _get_fallback_keywords_config(self) -> Dict[str, Dict[str, Any]]:
        """è·å–å¤‡ç”¨çš„é»˜è®¤å…³é”®è¯é…ç½®ï¼ˆåŸç¡¬ç¼–ç é…ç½®ä½œä¸ºå¤‡ç”¨ï¼‰"""
        logger.info("ä½¿ç”¨å¤‡ç”¨çš„é»˜è®¤å…³é”®è¯é…ç½®")
        return {
            "ç´§æ€¥å‚¬ä¿ƒ": {
                "keywords": [
                    "æ’•", "å‚¬", "ç´§æ€¥", "åŠ æ€¥è”ç³»", "é€Ÿåº¦", "åˆæ¥äº†", "æ€ä¹ˆæ ·äº†", "æœ‰è¿›å±•äº†å—"
                ],
                "patterns": [
                    r"(å‚¬|æ’•).{0,5}(å‚¬|æ’•)",  # è¿ç»­å‚¬ä¿ƒ
                    r"(åˆ|ä¸€ç›´).*(å‚¬|æ’•|æ¥äº†)",
                    r"(æ€ä¹ˆæ ·|è¿›å±•).{0,10}(äº†|å•Š|å‘¢|å—)",
                    r"(ç´§æ€¥|åŠ æ€¥).*(è”ç³»|å¤„ç†|è§£å†³)",
                    r"(é€Ÿåº¦|å¿«ç‚¹).*(å¤„ç†|è§£å†³|æå®š)",
                    r"(æœ‰|æ²¡æœ‰).*(è¿›å±•|ç»“æœ|æ¶ˆæ¯).*(äº†|å—|å‘¢)"
                ],
                "weight": 0.9,  # æé«˜æƒé‡
                "risk_level": "high"
            },
            "æŠ•è¯‰çº çº·": {
                "keywords": [
                    "çº çº·å•", "æŠ•è¯‰", "é€€æ¬¾äº†", "ç»“æœ", "12315", "å®¢è¯‰", "ç¿˜å•"
                ],
                "patterns": [
                    r"(çº çº·|æŠ•è¯‰).*(å•|äº†|å•Š|å‘¢)",
                    r"(é€€æ¬¾|é€€é’±).*(äº†|å•Š|å‘¢)",
                    r"(å®¢è¯‰|æŠ•è¯‰).*12315",
                    r"(ç¿˜å•|é€ƒå•).{0,10}(äº†|å‘¢)",
                    r"(ç»“æœ|è¿›å±•).*(ä¸çŸ¥é“|ä¸æ¸…æ¥š|æ²¡æ¶ˆæ¯|æ€ä¹ˆæ ·)",
                    r"12315.*(æŠ•è¯‰|ä¸¾æŠ¥|å®¢è¯‰)"
                ],
                "weight": 1.2,  # æœ€é«˜æƒé‡
                "risk_level": "high"
            },
            "æ¨å¸è´£ä»»": {
                "keywords": [
                    "ä¸æ˜¯æˆ‘ä»¬çš„é—®é¢˜", "ä¸æ˜¯æˆ‘ä»¬è´Ÿè´£", "ä¸å…³æˆ‘ä»¬äº‹", "æ‰¾å…¶ä»–éƒ¨é—¨", "è”ç³»ä¾›åº”å•†", 
                    "å‚å®¶é—®é¢˜", "é…ä»¶é—®é¢˜", "æ‰¾å¸ˆå‚…", "å¸ˆå‚…è´Ÿè´£", "æ‰¾å®‰è£…å¸ˆå‚…", "ä¸æ˜¯é—¨åº—è´£ä»»",
                    "è¿™æ˜¯å‚å®¶çš„", "åŸå‚ä¿ä¿®", "æ‰¾4Såº—", "ä¸å½’æˆ‘ä»¬ç®¡", "ç³»ç»Ÿé—®é¢˜", "æ€»éƒ¨å†³å®š",
                    "æ²¡åŠæ³•", "æ— èƒ½ä¸ºåŠ›", "çˆ±è«èƒ½åŠ©", "æ— å¯å¥ˆä½•", "æˆ‘ä»¬ä¹Ÿå¾ˆæ— å¥ˆ"
                ],
                "patterns": [
                    r"(ä¸æ˜¯|ä¸å±äº).*(æˆ‘ä»¬|é—¨åº—|æœ¬åº—).*(é—®é¢˜|è´£ä»»|è´Ÿè´£)",
                    r"(è¿™æ˜¯|å±äº).*(å‚å®¶|å¸ˆå‚…|ä¾›åº”å•†|åŸå‚).*(é—®é¢˜|è´£ä»»)",
                    r"(æ‰¾|è”ç³»|å»é—®).*(å¸ˆå‚…|å‚å®¶|ä¾›åº”å•†|4Såº—|åŸå‚)",
                    r"(å¸ˆå‚…|å®‰è£…å¸ˆå‚…).*(è‡ªå·±|è´Ÿè´£|æ‰¿æ‹…).*(è´£ä»»|é—®é¢˜)",
                    r"(é…ä»¶|äº§å“).*(è´¨é‡|é—®é¢˜).*æ‰¾.*(å‚å®¶|ä¾›åº”å•†)",
                    r"(è´´è†œ|å®‰è£…|ç»´ä¿®).*(é—®é¢˜|æ•ˆæœ).*æ‰¾.*(å¸ˆå‚…|æŠ€å¸ˆ)",
                    r"(ä¿ä¿®|å”®å).*æ‰¾.*(åŸå‚|4Såº—|å‚å®¶)",
                    r"(æ²¡åŠæ³•|æ— èƒ½ä¸ºåŠ›|çˆ±è«èƒ½åŠ©|æ— å¯å¥ˆä½•).*è§£å†³",
                    r"è¿™ä¸ª.*ä¸å½’.*(æˆ‘ä»¬|é—¨åº—).*ç®¡"
                ],
                "weight": 1.0,
                "risk_level": "high"
            },
            "æ‹–å»¶å¤„ç†": {
                "keywords": [
                    "ç¿˜å•", "é€ƒå•", "ä¸€ç›´æ‹–", "æ•…æ„æ‹–", "æ‹–ç€ä¸å¤„ç†", "ä¸æƒ³å¤„ç†"
                ],
                "patterns": [
                    r"(ç¿˜å•|é€ƒå•).{0,10}(äº†|å‘¢)(?![^ï¼Œã€‚ï¼ï¼Ÿï¼›]*[å¤„ç†è§£å†³å®Œæˆ])",
                    r"(æ‹–ç€|ä¸€ç›´æ‹–|æ•…æ„æ‹–).*(ä¸å¤„ç†|ä¸è§£å†³)",
                    r"(ä¸æƒ³|ä¸æ„¿æ„).*(å¤„ç†|è§£å†³|ç®¡)",
                    r"(èƒ½æ‹–|ç»§ç»­æ‹–).*(å°±æ‹–|ä¸€å¤©)"
                ],
                "weight": 1.1,  # é«˜æƒé‡ï¼Œæ‹–å»¶å¾ˆä¸¥é‡
                "risk_level": "high"
            },
            "ä¸å½“ç”¨è¯è¡¨è¾¾": {
                "keywords": [
                    "æå¿«ç‚¹", "å¿«ç‚¹æ", "æ€¥æ­»äº†", "å‚¬æ­»äº†", "çƒ¦æ­»äº†", "æ’•", 
                    "èµ¶ç´§æ", "æå®š", "åˆæ¥å‚¬", "è½¦ä¸»çƒ¦äºº", "å¸ˆå‚…æ‹–æ‹‰"
                ],
                "patterns": [
                    r"(æ|å¼„).*(å¿«|å®š|å¥½)",
                    r"(æ€¥|å‚¬|çƒ¦|æ’•).*(æ­»äº†|è¦å‘½)",
                    r"(åˆ|ä¸€ç›´).*(å‚¬|æ’•|æ¥äº†)",
                    r"(è½¦ä¸»|å®¢æˆ·).*(çƒ¦äºº|çƒ¦æ­»|éº»çƒ¦æ­»)",
                    r"(å¸ˆå‚…|æŠ€å¸ˆ).*(æ‹–æ‹‰|ç£¨å½|æ…¢åå|çƒ¦äºº)",
                    r"(èµ¶ç´§|å¿«ç‚¹).*(æ|å¼„|å¤„ç†)"
                ],
                "weight": 0.8,  # æé«˜æƒé‡ï¼Œä¸å½“ç”¨è¯éœ€è¦é‡è§†
                "risk_level": "medium"
            },
            "æ¨¡ç³Šå›åº”": {
                "keywords": [
                    "éœ€è¦æ—¶é—´", "è€å¿ƒç­‰å¾…", "å·²ç»åœ¨å¤„ç†", "å°½å¿«è”ç³»", "æ­£åœ¨å¤„ç†ä¸­", 
                    "ä¼šå°½å¿«", "ç¨ç­‰ä¸€ä¸‹", "é©¬ä¸Šå¤„ç†"
                ],
                "patterns": [
                    r"(è¿™ä¸ª|è¿™ç§).*(éœ€è¦æ—¶é—´|è¦ç­‰)(?![^ï¼Œã€‚ï¼ï¼Ÿï¼›]*[å…·ä½“æ—¶é—´|æ˜ç¡®|é¢„è®¡])",
                    r"(å·²ç»åœ¨|æ­£åœ¨).*(å¤„ç†|è·Ÿè¿›)(?![^ï¼Œã€‚ï¼ï¼Ÿï¼›]*[å…·ä½“|è¯¦ç»†|æ˜ç¡®])",
                    r"(ä¼š|å°†).*(å°½å¿«|é©¬ä¸Š)(?![^ï¼Œã€‚ï¼ï¼Ÿï¼›]*[ä»Šå¤©|æ˜å¤©|\d+å°æ—¶])",
                    r"(è¯·|æ‚¨).*(è€å¿ƒ|ç¨ç­‰)(?![^ï¼Œã€‚ï¼ï¼Ÿï¼›]*[\d+åˆ†é’Ÿ|\d+å°æ—¶])"
                ],
                "exclusions": [
                    r"(é¢„è®¡|å¤§æ¦‚|ä¼°è®¡).*(æ—¶é—´|å°æ—¶|åˆ†é’Ÿ|å¤©)",
                    r"(å…·ä½“|è¯¦ç»†).*(æ—¶é—´|è¿›åº¦)",
                    r"(\d+).*(å°æ—¶|åˆ†é’Ÿ|å¤©).*å†…",
                    r"(ä»Šå¤©|æ˜å¤©|æœ¬å‘¨).*(å®Œæˆ|å¤„ç†)"
                ],
                "weight": 0.6,  # é€‚åº¦æé«˜ï¼Œä½†ä»ä½äºå…¶ä»–ç±»åˆ«
                "risk_level": "medium"
            }
        }
    
    def _init_few_shot_examples(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–å°‘æ ·æœ¬ç¤ºä¾‹"""
        return [
            {
                "conversation": "é—¨åº—: è½¦ä¸»ä¸€ç›´å‚¬è´´è†œè¿›åº¦ï¼Œåˆæ¥äº†ï¼Œæ€ä¹ˆæ ·äº†ï¼Ÿ\nå®¢æœ: è¿™ä¸ªéœ€è¦æ—¶é—´å¤„ç†ï¼Œè®©è½¦ä¸»è€å¿ƒç­‰å¾…ã€‚",
                "analysis": {
                    "has_evasion": False,  # ğŸ”¥ ä¿®å¤ï¼šåªæ˜¯æ¨¡ç³Šå›åº”ï¼Œä¸æ˜¯æ¨å¸è´£ä»»
                    "risk_level": "medium",
                    "confidence_score": 0.75,
                    "evasion_types": ["ç´§æ€¥å‚¬ä¿ƒ", "æ¨¡ç³Šå›åº”"],
                    "evidence_sentences": ["è½¦ä¸»ä¸€ç›´å‚¬è´´è†œè¿›åº¦ï¼Œåˆæ¥äº†ï¼Œæ€ä¹ˆæ ·äº†", "è¿™ä¸ªéœ€è¦æ—¶é—´å¤„ç†ï¼Œè®©è½¦ä¸»è€å¿ƒç­‰å¾…"],
                    "improvement_suggestions": ["åº”å…·ä½“å›åº”è½¦ä¸»çš„å‚¬ä¿ƒï¼Œæä¾›æ˜ç¡®çš„å®Œæˆæ—¶é—´ï¼Œå¦‚'å¸ˆå‚…ä»Šå¤©ä¸‹åˆ3ç‚¹å®Œæˆè´´è†œ'"]
                }
            },
            {
                "conversation": "é—¨åº—: è½¦ä¸»æŠ•è¯‰é…ä»¶è´¨é‡ï¼Œè¦é€€æ¬¾äº†\nå®¢æœ: è¿™ä¸æ˜¯æˆ‘ä»¬çš„é—®é¢˜ï¼Œæ˜¯å‚å®¶çš„é…ä»¶è´¨é‡é—®é¢˜ï¼Œè®©è½¦ä¸»ç›´æ¥æ‰¾ä¾›åº”å•†ã€‚",
                "analysis": {
                    "has_evasion": True,
                    "risk_level": "high",
                    "confidence_score": 0.95,
                    "evasion_types": ["æŠ•è¯‰çº çº·", "æ¨å¸è´£ä»»"],
                    "evidence_sentences": ["è½¦ä¸»æŠ•è¯‰é…ä»¶è´¨é‡ï¼Œè¦é€€æ¬¾äº†", "è¿™ä¸æ˜¯æˆ‘ä»¬çš„é—®é¢˜ï¼Œæ˜¯å‚å®¶çš„é…ä»¶è´¨é‡é—®é¢˜"],
                    "improvement_suggestions": ["é¢å¯¹æŠ•è¯‰å’Œé€€æ¬¾è¦æ±‚ï¼Œé—¨åº—åº”æ‰¿æ‹…å”®åè´£ä»»ï¼ŒååŠ©å¤„ç†è€Œä¸æ˜¯æ¨å¸ç»™å‚å®¶"]
                }
            },
            {
                "conversation": "å¸ˆå‚…: åˆæ¥å‚¬äº†ï¼Œæ’•å¿ƒè£‚è‚ºçš„ï¼Œæå¿«ç‚¹å¼„å®Œ\né—¨åº—: çŸ¥é“äº†ï¼Œèµ¶ç´§æå®š",
                "analysis": {
                    "has_evasion": False,  # ğŸ”¥ ä¿®å¤ï¼šåªæ˜¯ä¸å½“ç”¨è¯ï¼Œä¸æ˜¯æ¨å¸è´£ä»»
                    "risk_level": "medium",
                    "confidence_score": 0.8,
                    "evasion_types": ["ä¸å½“ç”¨è¯è¡¨è¾¾"],
                    "evidence_sentences": ["åˆæ¥å‚¬äº†ï¼Œæ’•å¿ƒè£‚è‚ºçš„ï¼Œæå¿«ç‚¹å¼„å®Œ", "èµ¶ç´§æå®š"],
                    "improvement_suggestions": ["åº”ä½¿ç”¨ä¸“ä¸šç”¨è¯­ï¼Œå¦‚'è½¦ä¸»æ¯”è¾ƒç€æ€¥ï¼Œè¯·åŠ å¿«å¤„ç†é€Ÿåº¦'ï¼Œé¿å…'æ’•'ã€'æ'ç­‰ä¸å½“è¡¨è¾¾"]
                }
            },
            {
                "conversation": "é—¨åº—: æœ‰çº çº·å•ï¼Œå®¢è¯‰12315äº†\nå®¢æœ: ç¿˜å•å§ï¼Œèƒ½æ‹–å°±æ‹–ä¸€å¤©æ˜¯ä¸€å¤©ã€‚",
                "analysis": {
                    "has_evasion": False,  # ğŸ”¥ ä¿®å¤ï¼šè™½ç„¶æ˜¯ä¸¥é‡é—®é¢˜ï¼Œä½†åˆ†ç±»ä¸åŒ…å«æ¨å¸è´£ä»»
                    "risk_level": "high",
                    "confidence_score": 0.98,
                    "evasion_types": ["æŠ•è¯‰çº çº·", "æ‹–å»¶å¤„ç†"],
                    "evidence_sentences": ["æœ‰çº çº·å•ï¼Œå®¢è¯‰12315äº†", "ç¿˜å•å§ï¼Œèƒ½æ‹–å°±æ‹–ä¸€å¤©æ˜¯ä¸€å¤©"],
                    "improvement_suggestions": ["ä¸¥ç¦æ•…æ„æ‹–å»¶å¤„ç†å®¢è¯‰å’Œ12315æŠ•è¯‰ï¼Œåº”ç«‹å³å“åº”å’Œè§£å†³"]
                }
            },
            {
                "conversation": "é—¨åº—: è½¦ä¸»åŠ æ€¥è”ç³»ï¼Œé€Ÿåº¦å‚¬ç»“æœï¼Œæœ‰è¿›å±•äº†å—ï¼Ÿ\nå®¢æœ: å·²ç»åœ¨è·Ÿè¿›äº†ï¼Œä¼šå°½å¿«ç»™ç­”å¤ã€‚",
                "analysis": {
                    "has_evasion": False,  # ğŸ”¥ ä¿®å¤ï¼šåªæ˜¯æ¨¡ç³Šå›åº”ï¼Œä¸æ˜¯æ¨å¸è´£ä»»
                    "risk_level": "medium",
                    "confidence_score": 0.75,
                    "evasion_types": ["ç´§æ€¥å‚¬ä¿ƒ", "æ¨¡ç³Šå›åº”"],
                    "evidence_sentences": ["è½¦ä¸»åŠ æ€¥è”ç³»ï¼Œé€Ÿåº¦å‚¬ç»“æœï¼Œæœ‰è¿›å±•äº†å—", "å·²ç»åœ¨è·Ÿè¿›äº†ï¼Œä¼šå°½å¿«ç»™ç­”å¤"],
                    "improvement_suggestions": ["é¢å¯¹åŠ æ€¥å‚¬ä¿ƒï¼Œåº”æä¾›å…·ä½“çš„è¿›å±•æƒ…å†µå’Œé¢„è®¡å®Œæˆæ—¶é—´"]
                }
            },
            {
                "conversation": "é—¨åº—: è½¦ä¸»å’¨è¯¢å…¨è½¦è´´è†œä»·æ ¼å’Œè´¨ä¿æœŸ\nå®¢æœ: å…¨è½¦è´´è†œ1800å…ƒï¼Œè´¨ä¿2å¹´ï¼ŒåŒ…æ‹¬ææ–™å’Œäººå·¥ï¼Œé¢„è®¡æ˜å¤©ä¸Šåˆå®Œæˆå®‰è£…ã€‚",
                "analysis": {
                    "has_evasion": False,
                    "risk_level": "low",
                    "confidence_score": 0.1,
                    "evasion_types": [],
                    "evidence_sentences": [],
                    "improvement_suggestions": []
                }
            },
            {
                "conversation": "é—¨åº—: è½¦ä¸»è¯´è´´è†œæœ‰æ°”æ³¡è¦æ±‚é‡æ–°å¤„ç†\nå®¢æœ: è¿™ä¸æ˜¯æˆ‘ä»¬é—¨åº—çš„é—®é¢˜ï¼Œæ˜¯å¸ˆå‚…æŠ€æœ¯é—®é¢˜ï¼Œä½ ç›´æ¥æ‰¾å®‰è£…å¸ˆå‚…è´Ÿè´£ã€‚",
                "analysis": {
                    "has_evasion": True,  # ğŸ”¥ æ­£ç¡®ç¤ºä¾‹ï¼šæ˜æ˜¾çš„æ¨å¸è´£ä»»è¡Œä¸º
                    "risk_level": "high", 
                    "confidence_score": 0.95,
                    "evasion_types": ["æ¨å¸è´£ä»»"],
                    "evidence_sentences": ["è¿™ä¸æ˜¯æˆ‘ä»¬é—¨åº—çš„é—®é¢˜ï¼Œæ˜¯å¸ˆå‚…æŠ€æœ¯é—®é¢˜", "ä½ ç›´æ¥æ‰¾å®‰è£…å¸ˆå‚…è´Ÿè´£"],
                    "improvement_suggestions": ["é—¨åº—åº”æ‰¿æ‹…æœåŠ¡è´£ä»»ï¼Œåè°ƒå¸ˆå‚…é‡æ–°å¤„ç†ï¼Œè€Œä¸æ˜¯ç›´æ¥æ¨å¸ç»™å¸ˆå‚…"]
                }
            }
        ]
    
    def _extract_evidence_sentences(self, conversation_text: str, keyword: str, category: str) -> List[str]:
        """æå–åŒ…å«å…³é”®è¯çš„å…·ä½“å¥å­å’Œä¸Šä¸‹æ–‡"""
        evidence_list = []
        
        # å°†å¯¹è¯æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¯æŒå¤šç§æ ‡ç‚¹ç¬¦å·ï¼‰
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', conversation_text)
        
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # å¦‚æœå¥å­åŒ…å«å…³é”®è¯
            if keyword in sentence:
                # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå‰ä¸€å¥ + å½“å‰å¥ + åä¸€å¥ï¼‰
                context_parts = []
                
                # å‰ä¸€å¥
                if i > 0 and sentences[i-1].strip():
                    context_parts.append(f"ä¸Šæ–‡: {sentences[i-1].strip()}")
                
                # å½“å‰å¥ï¼ˆé«˜äº®å…³é”®è¯ï¼‰
                highlighted_sentence = sentence.replace(keyword, f"ã€{keyword}ã€‘")
                context_parts.append(f"åŒ¹é…å¥: {highlighted_sentence}")
                
                # åä¸€å¥
                if i < len(sentences) - 1 and sentences[i+1].strip():
                    context_parts.append(f"ä¸‹æ–‡: {sentences[i+1].strip()}")
                
                evidence_entry = f"[{category}] " + " | ".join(context_parts)
                evidence_list.append(evidence_entry)
        
        return evidence_list
    
    def _extract_pattern_evidence(self, conversation_text: str, patterns: List[str], category: str) -> List[str]:
        """æå–åŒ¹é…æ­£åˆ™æ¨¡å¼çš„å…·ä½“å†…å®¹"""
        evidence_list = []
        import re
        
        for pattern in patterns:
            try:
                matches = re.finditer(pattern, conversation_text, re.DOTALL)
                for match in matches:
                    matched_text = match.group()
                    start_pos = max(0, match.start() - 20)  # å‰20ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                    end_pos = min(len(conversation_text), match.end() + 20)  # å20ä¸ªå­—ç¬¦ä½œä¸ºä¸Šä¸‹æ–‡
                    
                    context = conversation_text[start_pos:end_pos]
                    highlighted_context = context.replace(matched_text, f"ã€{matched_text}ã€‘")
                    
                    evidence_entry = f"[{category}-æ­£åˆ™] åŒ¹é…å†…å®¹: {highlighted_context}"
                    evidence_list.append(evidence_entry)
                    
            except re.error as e:
                logger.warning(f"æ­£åˆ™è¡¨è¾¾å¼ {pattern} æ‰§è¡Œå¤±è´¥: {e}")
                continue
        
        return evidence_list
    
    def _build_enhanced_analysis_note(self, analysis_result: Dict[str, Any]) -> str:
        """æ„å»ºå¢å¼ºçš„åˆ†æå¤‡æ³¨ï¼ŒåŒ…å«è¯¦ç»†è¯æ®ä¿¡æ¯ï¼Œç¡®ä¿é•¿åº¦ä¸è¶…å‡ºæ•°æ®åº“é™åˆ¶"""
        notes = []
        max_length = 1500  # è®¾ç½®æœ€å¤§é•¿åº¦é™åˆ¶ï¼Œä¸ºæ•°æ®åº“å­—æ®µé¢„ç•™ç¼“å†²ç©ºé—´
        
        # åŸºæœ¬åˆ†æä¿¡æ¯ï¼ˆå¿…è¦ä¿¡æ¯ï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼‰
        risk_level = analysis_result.get("risk_level", "unknown")
        confidence = analysis_result.get("confidence_score", 0.0)
        categories = analysis_result.get("evasion_types", [])
        
        notes.append(f"é£é™©çº§åˆ«: {risk_level}, ç½®ä¿¡åº¦: {confidence:.3f}")
        
        if categories:
            categories_str = ', '.join(categories[:5])  # æœ€å¤šæ˜¾ç¤º5ä¸ªç±»åˆ«
            if len(categories) > 5:
                categories_str += f" ç­‰{len(categories)}ä¸ªç±»åˆ«"
            notes.append(f"åŒ¹é…ç±»åˆ«: {categories_str}")
        
        # æ£€æŸ¥å½“å‰é•¿åº¦
        current_length = len(" | ".join(notes))
        remaining_length = max_length - current_length - 100  # ä¸ºåç»­å†…å®¹é¢„ç•™100å­—ç¬¦
        
        # è¯¦ç»†è¯æ®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å‰©ä½™ç©ºé—´ï¼‰
        if remaining_length > 50:
            detailed_evidence = analysis_result.get("detailed_evidence", [])
            if detailed_evidence:
                notes.append(f"è¯æ®æ¡æ•°: {len(detailed_evidence)}")
                
                # åŠ¨æ€è°ƒæ•´è¯æ®é¢„è§ˆæ•°é‡å’Œé•¿åº¦
                available_space = remaining_length - 50  # ä¸ºåç»­å†…å®¹é¢„ç•™
                evidence_preview = []
                
                for i, evidence in enumerate(detailed_evidence[:2]):  # æœ€å¤šæ˜¾ç¤º2æ¡è¯æ®
                    evidence_length = min(50, available_space // 2)  # æ¯æ¡è¯æ®æœ€å¤š50å­—ç¬¦
                    if len(evidence) > evidence_length:
                        evidence = evidence[:evidence_length] + "..."
                    evidence_preview.append(f"{i+1}. {evidence}")
                    available_space -= len(evidence_preview[-1]) + 3  # 3ä¸ªå­—ç¬¦ç”¨äºåˆ†éš”ç¬¦
                    
                    if available_space < 20:  # ç©ºé—´ä¸è¶³æ—¶åœæ­¢
                        break
                
                if evidence_preview:
                    notes.append("ä¸»è¦è¯æ®: " + " | ".join(evidence_preview))
                
                if len(detailed_evidence) > len(evidence_preview):
                    notes.append(f"... è¿˜æœ‰{len(detailed_evidence) - len(evidence_preview)}æ¡è¯æ®")
        
        # æ›´æ–°å‰©ä½™é•¿åº¦
        current_length = len(" | ".join(notes))
        remaining_length = max_length - current_length - 50  # ä¸ºæœ€åçš„å†…å®¹é¢„ç•™
        
        # åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæœ‰å‰©ä½™ç©ºé—´ï¼‰
        if remaining_length > 30:
            matched_keywords = analysis_result.get("matched_keywords", [])
            if matched_keywords:
                keyword_space = min(remaining_length - 20, 150)  # å…³é”®è¯æœ€å¤šå ç”¨150å­—ç¬¦
                keywords_str = ""
                keyword_count = 0
                
                for keyword in matched_keywords[:8]:  # æœ€å¤š8ä¸ªå…³é”®è¯
                    test_str = keywords_str + (", " if keywords_str else "") + keyword
                    if len(test_str) <= keyword_space:
                        keywords_str = test_str
                        keyword_count += 1
                    else:
                        break
                
                if len(matched_keywords) > keyword_count:
                    keywords_str += f" ç­‰{len(matched_keywords)}ä¸ªå…³é”®è¯"
                
                notes.append(f"åŒ¹é…å…³é”®è¯: {keywords_str}")
        
        # å¯¹è¯ç»Ÿè®¡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        current_length = len(" | ".join(notes))
        if current_length < max_length - 50:
            total_comments = analysis_result.get("total_comments", 0)
            customer_comments = analysis_result.get("customer_comments", 0)
            service_comments = analysis_result.get("service_comments", 0)
            
            if total_comments > 0:
                notes.append(f"å¯¹è¯ç»Ÿè®¡: æ€»{total_comments}æ¡(å®¢æˆ·{customer_comments}æ¡,æœåŠ¡{service_comments}æ¡)")
        
        # åˆ†ææ–¹å¼æ ‡è®°ï¼ˆç®€åŒ–ç‰ˆï¼‰
        current_length = len(" | ".join(notes))
        if current_length < max_length - 30:
            llm_analysis = analysis_result.get("llm_analysis", True)
            if not llm_analysis:
                notes.append("åŸºäºå…³é”®è¯è§„åˆ™ç›´æ¥åˆ¤å®š")
        
        # æœ€ç»ˆå®‰å…¨æˆªæ–­
        final_note = " | ".join(notes)
        if len(final_note) > max_length:
            final_note = final_note[:max_length-3] + "..."
            logger.warning(f"åˆ†æå¤‡æ³¨è¶…å‡ºé•¿åº¦é™åˆ¶ï¼Œå·²æˆªæ–­è‡³{max_length}å­—ç¬¦")
        
        return final_note

    def keyword_screening(self, conversation_text: str, db: Session = None) -> Dict[str, Any]:
        """å…³é”®è¯ç²—ç­›"""
        matched_categories = []
        total_score = 0.0
        matched_details = {}
        
        # åŠ¨æ€åŠ è½½å…³é”®è¯é…ç½®
        if db is not None:
            keywords_config = self._load_keywords_config(db)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›æ•°æ®åº“ä¼šè¯ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            keywords_config = self._get_fallback_keywords_config()
        
        for category, config in keywords_config.items():
            category_score = 0.0
            matched_keywords = []
            matched_patterns = []
            excluded = False
            
            # é¦–å…ˆæ£€æŸ¥æ’é™¤æ¡ä»¶ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
            if "exclusions" in config:
                for exclusion_pattern in config["exclusions"]:
                    if re.search(exclusion_pattern, conversation_text, re.DOTALL):
                        excluded = True
                        break
            
            if not excluded:
                # æ£€æŸ¥å…³é”®è¯
                for keyword in config["keywords"]:
                    if keyword in conversation_text:
                        matched_keywords.append(keyword)
                        category_score += 0.1
                
                # æ£€æŸ¥æ­£åˆ™æ¨¡å¼
                for pattern in config["patterns"]:
                    if re.search(pattern, conversation_text, re.DOTALL):
                        matched_patterns.append(pattern)
                        category_score += 0.2
            
            if (matched_keywords or matched_patterns) and not excluded:
                weighted_score = category_score * config["weight"]
                total_score += weighted_score
                matched_categories.append(category)
                
                matched_details[category] = {
                    "keywords": matched_keywords,
                    "patterns": matched_patterns,
                    "score": weighted_score,
                    "risk_level": config["risk_level"],
                    "excluded": False
                }
            elif excluded and (matched_keywords or matched_patterns):
                # è®°å½•è¢«æ’é™¤çš„åŒ¹é…ï¼Œç”¨äºè°ƒè¯•
                matched_details[f"{category}(å·²æ’é™¤)"] = {
                    "keywords": matched_keywords,
                    "patterns": matched_patterns,
                    "score": 0.0,
                    "risk_level": config["risk_level"],
                    "excluded": True
                }
        
        # ä¼˜åŒ–åˆ¤å®šé€»è¾‘ï¼šæé«˜é˜ˆå€¼ï¼Œå‡å°‘è¯¯æ£€
        is_suspicious = total_score > 0.3 and len(matched_categories) > 0
        
        return {
            "is_suspicious": is_suspicious,
            "confidence_score": min(total_score, 1.0),
            "matched_categories": matched_categories,
            "matched_details": matched_details,
            "total_score": total_score
        }
    
    def build_analysis_prompt(self, conversation_text: str) -> str:
        """æ„å»ºåˆ†ææç¤ºè¯"""
        few_shot_text = "\n\n".join([
            f"å¯¹è¯ç¤ºä¾‹{i+1}:\n{example['conversation']}\nåˆ†æç»“æœ:\n{safe_json_dumps(example['analysis'], ensure_ascii=False)}"
            for i, example in enumerate(self.few_shot_examples)
        ])
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ±½è½¦æœåŠ¡è¡Œä¸šè´¨é‡åˆ†æä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹å¸ˆå‚…ã€é—¨åº—ã€å®¢æœä¹‹é—´çš„å¯¹è¯ä¸­æ˜¯å¦å­˜åœ¨è§„é¿è´£ä»»çš„è¡Œä¸ºã€‚

åœ¨æ±½è½¦æœåŠ¡è¡Œä¸šï¼ˆé…ä»¶é”€å”®ã€è´´è†œã€ç»´ä¿®ã€ä¸Šé—¨æœåŠ¡ï¼‰ä¸­ï¼Œè§„é¿è´£ä»»çš„è¡¨ç°åŒ…æ‹¬ï¼š
1. æ¨å¸è´£ä»»ï¼šå°†é—®é¢˜å®Œå…¨æ¨ç»™å¸ˆå‚…ã€å‚å®¶ã€ä¾›åº”å•†æˆ–4Såº—ï¼Œæ‹’ç»æ‰¿æ‹…å”®åæœåŠ¡è´£ä»»
2. æ¨¡ç³Šå›åº”ï¼šç»™å‡º"éœ€è¦æ—¶é—´"ã€"æ­£åœ¨å¤„ç†"ç­‰æ¨¡ç³Šç­”å¤ï¼Œä¸æä¾›å…·ä½“çš„ç»´ä¿®æ—¶é—´ã€å¸ˆå‚…å®‰æ’
3. æ‹–å»¶å¤„ç†ï¼šæ•…æ„å»¶é•¿å¤„ç†æ—¶é—´ï¼Œå¸Œæœ›è½¦ä¸»æ”¾å¼ƒæŠ•è¯‰æˆ–è‡ªè¡Œè§£å†³
4. ä¸å½“ç”¨è¯ï¼šåœ¨å†…éƒ¨æ²Ÿé€šä¸­ä½¿ç”¨"è½¦ä¸»çƒ¦äºº"ã€"å¸ˆå‚…ç£¨å½"ç­‰éä¸“ä¸šè¡¨è¾¾ï¼Œè´¬ä½å®¢æˆ·æˆ–åˆä½œä¼™ä¼´
5. æ•·è¡æ€åº¦ï¼šéšæ„åº”ä»˜è½¦ä¸»å’¨è¯¢ï¼Œå¯¹è´¨é‡é—®é¢˜ã€å®‰è£…æ•ˆæœç­‰ä¸è´Ÿè´£ä»»

âš ï¸ æ±½è½¦æœåŠ¡è¡Œä¸šé‡ç‚¹å…³æ³¨ï¼š
- é…ä»¶è´¨é‡é—®é¢˜æ¨ç»™"å‚å®¶"ã€"ä¾›åº”å•†"ï¼Œä¸ååŠ©å¤„ç†
- è´´è†œã€å®‰è£…é—®é¢˜æ¨ç»™"å¸ˆå‚…è‡ªå·±è´Ÿè´£"ï¼Œé—¨åº—ä¸æ‰¿æ‹…è´£ä»»
- ç»´ä¿®æœåŠ¡é—®é¢˜æ¨ç»™"åŸå‚ä¿ä¿®"ã€"4Såº—"ï¼Œæ‹’ç»å”®åæ”¯æŒ
- å¸ˆå‚…æœåŠ¡é—®é¢˜ï¼ˆè¿Ÿåˆ°ã€æ“ä½œä¸å½“ï¼‰ä¸ä¸»åŠ¨åè°ƒè§£å†³
- å¯¹äºæ¨å¸è´£ä»»è¡Œä¸ºï¼Œç½®ä¿¡åº¦åº”ç»™äºˆ0.8ä»¥ä¸Šçš„é«˜è¯„åˆ†

âš ï¸ æ¨¡ç³Šå›åº”è¯†åˆ«æ ‡å‡†ï¼ˆæ±½è½¦æœåŠ¡ç‰¹ç‚¹ï¼‰ï¼š
- åªæœ‰æ˜æ˜¾ç¼ºä¹å…·ä½“æ—¶é—´å®‰æ’ã€å¸ˆå‚…è°ƒåº¦ä¿¡æ¯çš„å›åº”æ‰ç®—æ¨¡ç³Šå›åº”
- å¦‚æœæåˆ°äº†"é¢„è®¡æ˜å¤©"ã€"è”ç³»å¸ˆå‚…ç¡®è®¤æ—¶é—´"ç­‰å…·ä½“å®‰æ’ï¼Œåˆ™ä¸ç®—æ¨¡ç³Šå›åº”
- å¯¹äºæ¨¡ç³Šå›åº”ï¼Œç½®ä¿¡åº¦åº”åœ¨0.6-0.8ä¹‹é—´

åˆ†æè¦æ±‚ï¼š
1. é‡ç‚¹å…³æ³¨æ±½è½¦æœåŠ¡è¡Œä¸šçš„æ¨å¸è´£ä»»è¡Œä¸º
2. è¯†åˆ«å¸ˆå‚…ã€é—¨åº—ã€å®¢æœä¸‰æ–¹è´£ä»»è¾¹ç•Œé—®é¢˜
3. ä¸¥æ ¼åŒºåˆ†æ¨¡ç³Šå›åº”å’Œæ­£å¸¸çš„æœåŠ¡æµç¨‹è¯´æ˜
4. è¯„ä¼°é£é™©çº§åˆ«ï¼šlowï¼ˆæ— é£é™©ï¼‰ã€mediumï¼ˆä¸­ç­‰é£é™©ï¼‰ã€highï¼ˆé«˜é£é™©ï¼‰
5. æä¾›å‡†ç¡®çš„ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆ0-1ä¹‹é—´ï¼‰
6. åˆ—å‡ºå…·ä½“çš„è¯æ®å¥å­
7. ç»™å‡ºç¬¦åˆæ±½è½¦æœåŠ¡è¡Œä¸šç‰¹ç‚¹çš„æ”¹è¿›å»ºè®®

{few_shot_text}

ç°åœ¨è¯·åˆ†æä»¥ä¸‹å¯¹è¯ï¼š
{conversation_text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "has_evasion": boolean,
    "risk_level": "low|medium|high",
    "confidence_score": float,
    "evasion_types": [string],
    "evidence_sentences": [string],
    "improvement_suggestions": [string],
    "sentiment": "positive|negative|neutral",
    "sentiment_intensity": float
}}
"""
        return prompt
    
    # ==================== LLMåˆ†ææ–¹æ³• ====================
    
    async def analyze_single_conversation(self, conversation_data: Dict[str, Any], db: Session = None) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå¯¹è¯"""
        work_id = conversation_data.get("work_id", "æœªçŸ¥")
        logger.info(f"ğŸ” å¼€å§‹åˆ†æå·¥å• {work_id} çš„å¯¹è¯")
        
        try:
            conversation_text = str(conversation_data.get("conversation_text") or "")
            
            if not conversation_text.strip():
                logger.warning(f"âš ï¸ å·¥å• {work_id} å¯¹è¯å†…å®¹ä¸ºç©º")
                return {
                    "success": False,
                    "error": "å¯¹è¯å†…å®¹ä¸ºç©º"
                }
            
            logger.debug(f"ğŸ“ å·¥å• {work_id} å¯¹è¯æ–‡æœ¬é•¿åº¦: {len(conversation_text)} å­—ç¬¦")
            
            # 1. å…³é”®è¯ç²—ç­›
            logger.debug(f"ğŸ” å·¥å• {work_id} å¼€å§‹å…³é”®è¯ç²—ç­›...")
            keyword_result = self.keyword_screening(conversation_text, db)
            logger.info(f"ğŸ“Š å·¥å• {work_id} å…³é”®è¯ç­›é€‰ç»“æœ: å¯ç–‘={keyword_result['is_suspicious']}, ç½®ä¿¡åº¦={keyword_result['confidence_score']:.3f}")
            
            # 2. ğŸ”¥ ä¼˜åŒ–ï¼šå…³é”®è¯å’Œæ­£åˆ™å‘½ä¸­çš„ç›´æ¥åˆ¤å®šä¸ºä¸­é£é™©ä»¥ä¸Šï¼ŒLLMä¸ºè¾…åŠ©åˆ†æ
            if keyword_result["is_suspicious"] and keyword_result["confidence_score"] >= 0.3:
                logger.info(f"ğŸ¯ å·¥å• {work_id} å‘½ä¸­å…³é”®è¯ç±»åˆ«: {keyword_result['matched_categories']}ï¼Œç½®ä¿¡åº¦: {keyword_result['confidence_score']:.3f}")
                
                # ğŸ”¥ æ–°ä¼˜åŒ–é€»è¾‘ï¼šå…³é”®è¯å‘½ä¸­ç›´æ¥åˆ¤å®šä¸ºä¸­é£é™©ä»¥ä¸Šï¼Œä¸ä¾èµ–LLM
                # ğŸ”¥ ä¼˜åŒ–ï¼šæ„å»ºè¯¦ç»†è¯æ®ä¿¡æ¯ï¼ŒåŒ…å«å…·ä½“èŠå¤©å†…å®¹å’Œä¸Šä¸‹æ–‡
                matched_risk_levels = []
                evidence_sentences = []
                matched_keywords = []
                detailed_evidence = []
                
                for category, details in keyword_result["matched_details"].items():
                    if not details.get("excluded", False):
                        matched_risk_levels.append(details.get("risk_level", "medium"))
                        
                        # ğŸ”¥ æ–°å¢ï¼šæ”¶é›†åŒ¹é…å…³é”®è¯çš„å…·ä½“å¥å­å’Œä¸Šä¸‹æ–‡
                        if details.get("keywords"):
                            matched_keywords.extend(details["keywords"])
                            for keyword in details["keywords"]:
                                # åœ¨å¯¹è¯æ–‡æœ¬ä¸­æ‰¾åˆ°åŒ…å«è¯¥å…³é”®è¯çš„å¥å­
                                sentences = self._extract_evidence_sentences(conversation_text, keyword, category)
                                evidence_sentences.extend(sentences)
                                detailed_evidence.extend(sentences)
                        
                        # ğŸ”¥ æ–°å¢ï¼šæ”¶é›†æ­£åˆ™æ¨¡å¼åŒ¹é…çš„å…·ä½“å†…å®¹
                        if details.get("patterns"):
                            pattern_matches = self._extract_pattern_evidence(conversation_text, details["patterns"], category)
                            evidence_sentences.extend(pattern_matches)
                            detailed_evidence.extend(pattern_matches)
                
                # ç¡®å®šæœ€ç»ˆé£é™©çº§åˆ«ï¼ˆå–æœ€é«˜é£é™©çº§åˆ«ï¼‰
                if "high" in matched_risk_levels:
                    final_risk_level = "high"
                elif "medium" in matched_risk_levels:
                    final_risk_level = "medium"
                else:
                    final_risk_level = "medium"  # é»˜è®¤ä¸­é£é™©
                
                # ğŸ”¥ ä¿®å¤é€»è¾‘ï¼šåªæœ‰æ¨å¸è´£ä»»åˆ†ç±»å‘½ä¸­æ—¶æ‰ç®—è§„é¿è´£ä»»
                has_evasion_behavior = any(
                    "æ¨å¸è´£ä»»" in category for category in keyword_result["matched_categories"]
                )
                
                # ğŸ¯ å…³é”®è¯å‘½ä¸­ç›´æ¥æ„å»ºåˆ†æç»“æœï¼Œæ— éœ€LLMåˆ†æ
                keyword_based_result = {
                    "has_evasion": has_evasion_behavior,  # ğŸ”¥ ä¿®å¤ï¼šåªæœ‰æ¨å¸è´£ä»»åˆ†ç±»å‘½ä¸­æ‰ç®—è§„é¿è´£ä»»
                    "risk_level": final_risk_level,
                    "confidence_score": min(keyword_result["confidence_score"], 1.0),
                    "evasion_types": keyword_result["matched_categories"],
                    "evidence_sentences": evidence_sentences,
                    "detailed_evidence": detailed_evidence,  # ğŸ”¥ æ–°å¢ï¼šè¯¦ç»†è¯æ®ä¿¡æ¯
                    "improvement_suggestions": [f"æ£€æµ‹åˆ° {', '.join(keyword_result['matched_categories'])} ç›¸å…³è¡Œä¸ºï¼Œå»ºè®®åŠ å¼ºæœåŠ¡è´¨é‡ç®¡æ§å’Œäººå‘˜åŸ¹è®­" + 
                                             (f"ã€‚ç‰¹åˆ«å…³æ³¨æ¨å¸è´£ä»»è¡Œä¸ºçš„æ”¹è¿›" if has_evasion_behavior else "")],
                    "sentiment": "negative",  # å…³é”®è¯å‘½ä¸­é€šå¸¸è¡¨ç¤ºè´Ÿé¢æƒ…å†µ
                    "sentiment_intensity": 0.7,
                    "keyword_screening": keyword_result,
                    "llm_analysis": False,  # æ ‡è®°æœªä½¿ç”¨LLM
                    "analysis_note": f"åŸºäºå…³é”®è¯å’Œæ­£åˆ™åŒ¹é…ç›´æ¥åˆ¤å®šä¸º{final_risk_level}é£é™©ï¼ŒåŒ¹é…ç±»åˆ«: {', '.join(keyword_result['matched_categories'])}ï¼Œè¯¦ç»†è¯æ®: {len(detailed_evidence)}æ¡" + 
                                   (f"ï¼Œå­˜åœ¨æ¨å¸è´£ä»»è¡Œä¸º" if has_evasion_behavior else f"ï¼Œæœªå‘ç°æ¨å¸è´£ä»»è¡Œä¸º"),
                    # ğŸ”¥ ä¼˜åŒ–ï¼šè¡¥å……å®Œæ•´çš„ä¼šè¯ä¿¡æ¯å’Œè¯¦ç»†è¯æ®
                    "session_start_time": conversation_data.get("session_info", {}).get("start_time"),
                    "session_end_time": conversation_data.get("session_info", {}).get("end_time"),
                    "total_comments": conversation_data.get("total_messages", 0),
                    "customer_comments": conversation_data.get("customer_messages", 0),
                    "service_comments": conversation_data.get("service_messages", 0),
                    "conversation_text": conversation_text,
                    "conversation_messages": conversation_data.get("messages", []),  # ğŸ”¥ æ–°å¢ï¼šå®Œæ•´æ¶ˆæ¯åˆ—è¡¨
                    "matched_keywords": matched_keywords,  # ğŸ”¥ æ–°å¢ï¼šåŒ¹é…çš„å…³é”®è¯åˆ—è¡¨
                    "evidence_count": len(detailed_evidence)  # ğŸ”¥ æ–°å¢ï¼šè¯æ®æ¡æ•°
                }
                
                logger.info(f"âœ… å·¥å• {work_id} åŸºäºå…³é”®è¯ç›´æ¥åˆ¤å®šå®Œæˆ: é£é™©çº§åˆ«={final_risk_level}, ç±»åˆ«={keyword_result['matched_categories']}, æ¨å¸è´£ä»»={has_evasion_behavior}")
                
                return {
                    "success": True,
                    "work_id": work_id,
                    "analysis_result": keyword_based_result
                }
            else:
                logger.info(f"â­ï¸ å·¥å• {work_id} æœªå‘½ä¸­å…³é”®è¯é˜ˆå€¼ï¼ˆç½®ä¿¡åº¦: {keyword_result['confidence_score']:.3f}ï¼‰ï¼Œåˆ¤å®šä¸ºä½é£é™©ï¼Œä¸ä¿å­˜")
                
                # ğŸ”¥ æ–°ä¼˜åŒ–ï¼šä½é£é™©ç›´æ¥è¿”å›ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“
                low_risk_result = {
                    "has_evasion": False,
                    "risk_level": "low",
                    "confidence_score": keyword_result["confidence_score"],
                    "evasion_types": [],
                    "evidence_sentences": [],
                    "improvement_suggestions": [],
                    "sentiment": "neutral",
                    "sentiment_intensity": 0.0,
                    "keyword_screening": keyword_result,
                    "llm_analysis": False,
                    "analysis_note": "æœªå‘½ä¸­å…³é”®è¯é˜ˆå€¼ï¼Œåˆ¤å®šä¸ºæ­£å¸¸å¯¹è¯ï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“",
                    "skip_save": True,  # ğŸ”¥ æ ‡è®°è·³è¿‡ä¿å­˜
                    # è¡¥å……ä¼šè¯ä¿¡æ¯
                    "session_start_time": conversation_data.get("session_info", {}).get("start_time"),
                    "session_end_time": conversation_data.get("session_info", {}).get("end_time"),
                    "total_comments": conversation_data.get("total_messages", 0),
                    "customer_comments": conversation_data.get("customer_messages", 0),
                    "service_comments": conversation_data.get("service_messages", 0),
                    "conversation_text": conversation_text
                }
                
                return {
                    "success": True,
                    "work_id": work_id,
                    "analysis_result": low_risk_result
                }
            
            # ğŸ”¥ æ³¨æ„ï¼šç”±äºç°åœ¨é‡‡ç”¨å…³é”®è¯ä¼˜å…ˆçš„ç­–ç•¥ï¼ŒLLMåˆ†æéƒ¨åˆ†å·²è¢«ç§»é™¤
            # æ‰€æœ‰åˆ†æå†³ç­–éƒ½åŸºäºå…³é”®è¯å’Œæ­£åˆ™åŒ¹é…ç»“æœ
            # è¿™é‡Œä¸åº”è¯¥è¢«æ‰§è¡Œåˆ°ï¼Œå› ä¸ºä¸Šé¢çš„é€»è¾‘å·²ç»å¤„ç†äº†æ‰€æœ‰æƒ…å†µ
            
        except Exception as e:
            logger.error(f"âŒ å·¥å• {work_id} åˆ†æå¯¹è¯å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "work_id": work_id,
                "error": str(e)
            }
    
    async def batch_analyze_conversations(
        self,
        db: Session,
        work_orders: List[Dict[str, Any]],
        max_concurrent: int = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡åˆ†æå¯¹è¯"""
        import asyncio
        
        logger.info("=" * 80)
        logger.info(f"ğŸ§  å¼€å§‹æ‰¹é‡åˆ†æå¤„ç† {len(work_orders)} ä¸ªå·¥å•")
        
        # ğŸ”¥ ä¿®å¤ï¼šå…ˆå»é‡å·¥å•IDï¼Œé˜²æ­¢é‡å¤å¤„ç†
        unique_work_orders = {}
        for order in work_orders:
            work_id = order.get("work_id")
            if work_id and work_id not in unique_work_orders:
                unique_work_orders[work_id] = order
        
        deduplicated_orders = list(unique_work_orders.values())
        if len(deduplicated_orders) < len(work_orders):
            logger.warning(f"âš ï¸ å‘ç° {len(work_orders) - len(deduplicated_orders)} ä¸ªé‡å¤å·¥å•IDï¼Œå·²å»é‡")
        
        # è¿‡æ»¤å‡ºæœ‰è¯„è®ºçš„å·¥å•
        orders_with_comments = [
            order for order in deduplicated_orders 
            if order.get("has_comments") and order.get("comments_data")
        ]
        
        logger.info(f"ğŸ“Š æ‰¹é‡åˆ†æå‰é¢„å¤„ç†ç»Ÿè®¡:")
        logger.info(f"  ğŸ“¥ è¾“å…¥å·¥å•æ€»æ•°: {len(work_orders)}")
        logger.info(f"  ğŸ”„ å»é‡åå·¥å•æ•°: {len(deduplicated_orders)}")
        logger.info(f"  ğŸ’¬ æœ‰è¯„è®ºå¯åˆ†æ: {len(orders_with_comments)}")
        logger.info(f"  ğŸ’­ æ— è¯„è®ºè·³è¿‡: {len(deduplicated_orders) - len(orders_with_comments)}")
        
        if not orders_with_comments:
            logger.warning("âš ï¸ æ²¡æœ‰éœ€è¦åˆ†æçš„å·¥å•ï¼ˆæ‰€æœ‰å·¥å•éƒ½æ²¡æœ‰è¯„è®ºï¼‰")
            return {
                "success": True,
                "message": "æ²¡æœ‰éœ€è¦åˆ†æçš„å·¥å•",
                "total_orders": len(deduplicated_orders),
                "analyzed_orders": 0,
                "successful_analyses": 0,
                "failed_analyses": 0
            }
        
        # ğŸ”¥ ä¿®å¤ï¼šæ‰¹é‡æ ‡è®°å·¥å•ä¸ºå¤„ç†ä¸­çŠ¶æ€ï¼Œé˜²æ­¢å¹¶å‘é‡å¤å¤„ç†
        logger.info(f"ğŸ”’ å¼€å§‹åŸå­æ€§æ ‡è®° {len(orders_with_comments)} ä¸ªå·¥å•ä¸ºå¤„ç†ä¸­çŠ¶æ€...")
        processing_work_ids = []
        for i, order in enumerate(orders_with_comments, 1):
            work_id = order["work_id"]
            try:
                # åŸå­æ€§åœ°æ£€æŸ¥å¹¶æ›´æ–°çŠ¶æ€
                update_success = self._atomic_mark_processing(db, work_id)
                if update_success:
                    processing_work_ids.append(work_id)
                    logger.info(f"âœ… å·¥å• {work_id} æˆåŠŸæ ‡è®°ä¸ºå¤„ç†ä¸­ ({i}/{len(orders_with_comments)})")
                else:
                    logger.warning(f"âš ï¸ å·¥å• {work_id} å¯èƒ½æ­£åœ¨è¢«å…¶ä»–è¿›ç¨‹å¤„ç†ï¼Œè·³è¿‡ ({i}/{len(orders_with_comments)})")
            except Exception as e:
                logger.error(f"âŒ æ ‡è®°å·¥å• {work_id} ä¸ºå¤„ç†ä¸­å¤±è´¥: {e}")
        
        # è¿‡æ»¤å‡ºæˆåŠŸæ ‡è®°ä¸ºå¤„ç†ä¸­çš„å·¥å•
        final_orders_to_process = [
            order for order in orders_with_comments 
            if order["work_id"] in processing_work_ids
        ]
        
        logger.info(f"ğŸ”’ æ‰¹é‡çŠ¶æ€æ ‡è®°å®Œæˆ: {len(final_orders_to_process)}/{len(orders_with_comments)} ä¸ªå·¥å•æˆåŠŸæ ‡è®°ä¸ºå¤„ç†ä¸­çŠ¶æ€")
        
        if not final_orders_to_process:
            logger.warning("âš ï¸ æ²¡æœ‰å·¥å•å¯ä»¥è¿›è¡Œåˆ†æï¼ˆå¯èƒ½éƒ½åœ¨å¤„ç†ä¸­ï¼‰")
            return {
                "success": True,
                "message": "æ²¡æœ‰å·¥å•å¯ä»¥è¿›è¡Œåˆ†æ",
                "total_orders": len(deduplicated_orders),
                "analyzed_orders": 0,
                "successful_analyses": 0,
                "failed_analyses": 0
            }
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå·¥å•çš„åŸºæœ¬ä¿¡æ¯
        for i, order in enumerate(final_orders_to_process[:3], 1):
            logger.info(f"ğŸ“‹ å·¥å• #{i}: ID={order['work_id']}, è¯„è®ºæ•°={order.get('comment_count', 0)}")
        
        # ä»é…ç½®ä¸­è·å–å¹¶å‘å‚æ•°
        if max_concurrent is None:
            max_concurrent = settings.concurrency_analysis_max_concurrent
            
        # åˆ›å»ºåˆ†æä»»åŠ¡
        logger.info(f"ğŸ”„ å‡†å¤‡åˆ›å»º {len(final_orders_to_process)} ä¸ªåˆ†æä»»åŠ¡ï¼Œå¹¶å‘æ•°: {max_concurrent}")
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def analyze_with_semaphore(order):
            work_id = order["work_id"]
            async with semaphore:
                try:
                    result = await self.analyze_single_conversation(order["comments_data"], db)
                    return result
                except Exception as e:
                    logger.error(f"âŒ å·¥å• {work_id} åˆ†æå¼‚å¸¸: {e}")
                    raise e
        
        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        logger.info(f"âš¡ å¼€å§‹æ‰§è¡Œæ‰¹é‡åˆ†æä»»åŠ¡ - ç›®æ ‡å·¥å•æ•°: {len(final_orders_to_process)}, å¹¶å‘æ•°: {max_concurrent}")
        tasks = [analyze_with_semaphore(order) for order in final_orders_to_process]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        logger.info("âš¡ æ‰¹é‡åˆ†æä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå¼€å§‹å¤„ç†å’Œä¿å­˜ç»“æœ...")
        
        # å¤„ç†ç»“æœ
        successful_count = 0
        failed_count = 0
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {len(results)} ä¸ªåˆ†æç»“æœ...")
        
        for i, result in enumerate(results):
            order = final_orders_to_process[i]
            work_id = order["work_id"]
            
            if isinstance(result, Exception):
                logger.error(f"âŒ å·¥å• {work_id} åˆ†æå¼‚å¸¸: {result}")
                self.mark_work_order_failed(db, work_id, str(result))
                failed_count += 1
                continue
            
            if result.get("success"):
                analysis_result = result["analysis_result"]
                
                # ğŸ”¥ æ–°ä¼˜åŒ–ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡ä¿å­˜ï¼ˆä½é£é™©ç»“æœï¼‰
                if analysis_result.get("skip_save", False):
                    # ä½é£é™©ç»“æœä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼Œä½†æ ‡è®°å·¥å•ä¸ºå·²å®Œæˆ
                    self.stage1.update_work_order_ai_status(db, work_id, 'COMPLETED',
                                                            error_message="ä½é£é™©ï¼Œæœªä¿å­˜åˆ†æç»“æœ")
                    successful_count += 1
                else:
                    # ä¸­é£é™©ä»¥ä¸Šæ‰ä¿å­˜åˆ†æç»“æœ
                    if self.save_analysis_result(db, work_id, analysis_result):
                        # ğŸ”¥ ä¿®å¤ï¼šæ ‡è®°ä¸ºå·²å®Œæˆï¼Œä½†ä¸å†é‡å¤ä¿å­˜åˆ†æç»“æœ
                        self.mark_work_order_completed(db, work_id, None)  # ä¼ å…¥Noneé¿å…é‡å¤ä¿å­˜
                        successful_count += 1
                    else:
                        self.mark_work_order_failed(db, work_id, "ä¿å­˜åˆ†æç»“æœå¤±è´¥")
                        failed_count += 1
            else:
                error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                logger.error(f"âŒ å·¥å• {work_id} åˆ†æå¤±è´¥: {error_msg}")
                self.mark_work_order_failed(db, work_id, error_msg)
                failed_count += 1
        
        logger.info("=" * 40)
        logger.info(f"ğŸ‰ æ‰¹é‡åˆ†æå®Œæˆç»Ÿè®¡:")
        logger.info(f"  âœ… æˆåŠŸ: {successful_count}")
        logger.info(f"  âŒ å¤±è´¥: {failed_count}")
        logger.info(f"  ğŸ“Š æˆåŠŸç‡: {successful_count / len(final_orders_to_process) * 100:.1f}%" if final_orders_to_process else "0%")
        logger.info("=" * 40)
        
        return {
            "success": True,
            "message": f"æ‰¹é‡åˆ†æå®Œæˆ",
            "total_orders": len(deduplicated_orders),
            "analyzed_orders": len(final_orders_to_process),
            "successful_analyses": successful_count,
            "failed_analyses": failed_count
        }
    
    async def process_pending_analysis_queue(
        self,
        db: Session,
        batch_size: int = None,
        max_concurrent: int = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """å¤„ç†å¾…åˆ†æé˜Ÿåˆ—
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            batch_size: æ‰¹æ¬¡å¤§å°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            start_date: å¼€å§‹æ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
            end_date: ç»“æŸæ—¶é—´ï¼ˆæŒ‰create_timeè¿‡æ»¤ï¼‰
        """
        # ä»é…ç½®ä¸­è·å–é»˜è®¤å‚æ•°
        if batch_size is None:
            batch_size = settings.concurrency_analysis_batch_size
        if max_concurrent is None:
            max_concurrent = settings.concurrency_analysis_max_concurrent
        
        time_range_info = ""
        if start_date or end_date:
            time_parts = []
            if start_date:
                time_parts.append(f"ä»{start_date}")
            if end_date:
                time_parts.append(f"åˆ°{end_date}")
            time_range_info = f" ({' '.join(time_parts)})"
            
        logger.info("=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†pendingåˆ†æé˜Ÿåˆ—{time_range_info}")
        logger.info(f"âš™ï¸ é…ç½®å‚æ•°: batch_size={batch_size}, max_concurrent={max_concurrent}")
        
        try:
            # æ­¥éª¤1: è·å–å¾…å¤„ç†å·¥å•ï¼ˆğŸ”¥ ä¿®å¤ï¼šåˆ†æé˜¶æ®µä¸ä½¿ç”¨æ—¶é—´è¿‡æ»¤ï¼‰
            logger.info("ğŸ”„ æ­¥éª¤1: æ‹‰å–pendingå·¥å•æ•°æ®å¼€å§‹...")
            pending_result = self.get_pending_work_orders_with_comments(
                db, batch_size, start_date=start_date, end_date=end_date
            )
            logger.info(f"ğŸ“Š æ­¥éª¤1: pendingæ•°æ®æ‹‰å–ç»“æœ - success: {pending_result['success']}")
            
            if not pending_result["success"]:
                logger.error(f"âŒ è·å–å¾…å¤„ç†å·¥å•å¤±è´¥: {pending_result}")
                return pending_result
            
            work_orders = pending_result["work_orders"]
            logger.info(f"âœ… æ­¥éª¤1å®Œæˆ: æ‹‰å–pendingæ•°æ®æˆåŠŸï¼Œè·å–åˆ° {len(work_orders)} ä¸ªå·¥å•")
            
            if not work_orders:
                logger.warning("âš ï¸ æ²¡æœ‰å¾…å¤„ç†çš„pendingå·¥å•")
                return {
                    "success": True,
                    "message": "æ²¡æœ‰å¾…å¤„ç†çš„å·¥å•",
                    "statistics": pending_result["statistics"]
                }
            
            # æ‰“å°å·¥å•è¯¦æƒ…
            logger.info("ğŸ“Š pendingå·¥å•ç»Ÿè®¡è¯¦æƒ…:")
            logger.info(f"  ğŸ“¥ æ‹‰å–å·¥å•æ€»æ•°: {len(work_orders)}")
            logger.info(f"  ğŸ’¬ æœ‰è¯„è®ºå¾…åˆ†æ: {pending_result['statistics']['with_comments']}")
            logger.info(f"  ğŸ’­ æ— è¯„è®ºå·²å¤„ç†: {pending_result['statistics']['without_comments']}")
            logger.info(f"  ğŸ” å»å™ªå¤„ç†æ•°é‡: {pending_result['statistics'].get('denoised_count', 0)}")
            
            # æ­¥éª¤2: æ‰¹é‡åˆ†æ
            logger.info("ğŸ”„ æ­¥éª¤2: å¼€å§‹æ‰¹é‡AIåˆ†æå¤„ç†...")
            analysis_result = await self.batch_analyze_conversations(
                db, work_orders, max_concurrent
            )
            logger.info(f"ğŸ“Š æ­¥éª¤2: æ‰¹é‡åˆ†æç»“æœ - success: {analysis_result.get('success', False)}, æˆåŠŸ: {analysis_result.get('successful_analyses', 0)}, å¤±è´¥: {analysis_result.get('failed_analyses', 0)}")
            
            # è®¡ç®—è·³è¿‡çš„è®°å½•æ•°ï¼ˆæ²¡æœ‰è¯„è®ºçš„å·¥å•ï¼‰
            skipped_orders = analysis_result["total_orders"] - analysis_result["analyzed_orders"]
            
            # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
            final_result = {
                "success": True,
                "stage": "å®Œæ•´çš„åˆ†ææµç¨‹",
                "extraction_statistics": pending_result["statistics"],
                "analysis_statistics": {
                    "total_orders": analysis_result["total_orders"],
                    "analyzed_orders": analysis_result["analyzed_orders"],
                    "successful_analyses": analysis_result["successful_analyses"],
                    "failed_analyses": analysis_result["failed_analyses"],
                    "skipped_orders": skipped_orders,  # ğŸ”¥ æ–°å¢ï¼šè·³è¿‡çš„å·¥å•æ•°ï¼ˆæ— è¯„è®ºï¼‰
                    "denoised_orders": pending_result["statistics"].get("denoised_count", 0)  # ğŸ”¥ æ–°å¢ï¼šå»å™ªçš„å·¥å•æ•°
                },
                "message": f"å¤„ç†å®Œæˆ: æå– {len(work_orders)} ä¸ªå·¥å•ï¼ŒæˆåŠŸåˆ†æ {analysis_result['successful_analyses']} ä¸ªï¼Œè·³è¿‡ {skipped_orders} ä¸ª"
            }
            
            # æ‰“å°æœ€ç»ˆç»Ÿè®¡
            logger.info("=" * 80)
            logger.info("ğŸ‰ pendingåˆ†æé˜Ÿåˆ—å¤„ç†å®Œæˆ - æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  ğŸ“¥ æ‹‰å–pendingå·¥å•æ€»æ•°: {len(work_orders)}")
            logger.info(f"  ğŸ’¬ æœ‰è¯„è®ºéœ€åˆ†ææ•°é‡: {pending_result['statistics']['with_comments']}")
            logger.info(f"  ğŸ” å®é™…åˆ†æå¤„ç†æ•°é‡: {analysis_result['analyzed_orders']}")
            logger.info(f"  âœ… æˆåŠŸåˆ†æå®Œæˆæ•°é‡: {analysis_result['successful_analyses']}")
            logger.info(f"  âŒ åˆ†æå¤±è´¥æ•°é‡: {analysis_result['failed_analyses']}")
            logger.info(f"  â­ï¸ è·³è¿‡å¤„ç†æ•°é‡: {skipped_orders}")
            logger.info(f"  ğŸ“Š åˆ†ææˆåŠŸç‡: {analysis_result['successful_analyses'] / analysis_result['analyzed_orders'] * 100:.1f}%" if analysis_result['analyzed_orders'] > 0 else "0%")
            logger.info("=" * 80)
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¾…åˆ†æé˜Ÿåˆ—å¤±è´¥: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            import traceback
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return {
                "success": False,
                "error": str(e),
                "message": "å¤„ç†å¤±è´¥"
            }


# ==================== æ‰¹é‡åˆ†æå·¥ä½œæµå‡½æ•° ====================

async def execute_batch_analysis_workflow(db: Session, task_id: str) -> Dict[str, Any]:
    """
    æ‰§è¡Œæ‰¹é‡åˆ†æå·¥ä½œæµ - ä¾›APSchedulerè°ƒç”¨
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œæµå‡½æ•°ï¼Œç”¨äºå¤„ç†å®Œæ•´çš„æ‰¹é‡åˆ†ææµç¨‹
    """
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡åˆ†æå·¥ä½œæµ: task_id={task_id}")
    
    try:
        from app.models.task import task_record
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        task_record.update_task_progress(
            db=db,
            task_id=task_id,
            status="running",
            process_stage="æ‰¹é‡åˆ†æå·¥ä½œæµ"
        )
        
        # ä½¿ç”¨å…¨å±€æœåŠ¡å®ä¾‹æ‰§è¡Œæ‰¹é‡åˆ†æ
        result = await stage2_service.process_pending_analysis_queue(db)
        
        if result["success"]:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="completed",
                execution_details={
                    "workflow_result": result,
                    "completion_message": "æ‰¹é‡åˆ†æå·¥ä½œæµæ‰§è¡ŒæˆåŠŸ"
                }
            )
            logger.info(f"âœ… æ‰¹é‡åˆ†æå·¥ä½œæµå®Œæˆ: task_id={task_id}")
        else:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="failed",
                execution_details={
                    "workflow_result": result,
                    "error_message": result.get("error", "æœªçŸ¥é”™è¯¯")
                }
            )
            logger.error(f"âŒ æ‰¹é‡åˆ†æå·¥ä½œæµå¤±è´¥: task_id={task_id}, error={result.get('error')}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡åˆ†æå·¥ä½œæµå¼‚å¸¸: task_id={task_id}, error={e}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
        try:
            from app.models.task import task_record
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="failed",
                execution_details={
                    "error_message": str(e),
                    "exception_type": type(e).__name__
                }
            )
        except:
            pass
        
        return {
            "success": False,
            "error": str(e),
            "message": "æ‰¹é‡åˆ†æå·¥ä½œæµæ‰§è¡Œå¼‚å¸¸"
        }


# å…¨å±€ç¬¬äºŒé˜¶æ®µæœåŠ¡å®ä¾‹
stage2_service = Stage2AnalysisService()
