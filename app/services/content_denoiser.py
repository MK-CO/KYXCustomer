"""
å†…å®¹å»å™ªæœåŠ¡
ç”¨äºè¿‡æ»¤å·¥å•è¯„è®ºä¸­çš„æ— æ•ˆæ•°æ®å’Œæ­£å¸¸æ“ä½œè®°å½•
"""
import re
import logging
import time
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)


class ContentDenoiser:
    """å†…å®¹å»å™ªå™¨ï¼Œç”¨äºè¿‡æ»¤å·¥å•è¯„è®ºä¸­çš„å™ªéŸ³æ•°æ®"""
    
    def __init__(self):
        """åˆå§‹åŒ–å»å™ªå™¨"""
        self.normal_operation_patterns = self._init_normal_operation_patterns()
        self.invalid_data_patterns = self._init_invalid_data_patterns()
        self.system_keywords = self._init_system_keywords()
    
    def _init_normal_operation_patterns(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–æ­£å¸¸æ“ä½œæ¨¡å¼"""
        return [
            {
                "name": "å·¥å•å…³é—­æ“ä½œ",
                "pattern": r"ã€å®Œç»“ã€‘.*?å…³é—­å·¥å•",
                "description": "å·¥å•æ­£å¸¸å…³é—­æ“ä½œ",
                "action": "filter_out"
            },
            {
                "name": "è‡ªåŠ¨å®Œç»“å·¥å•",
                "pattern": r"ã€è‡ªåŠ¨å®Œç»“å·¥å•ã€‘.*?(å·²æ’¤å•|è®¢å•å·²æ´¾å•|å·²å®Œæˆ|å·²å…³é—­|è‡ªåŠ¨å…³é—­|ç³»ç»Ÿå…³é—­)",
                "description": "ç³»ç»Ÿè‡ªåŠ¨å®Œç»“å·¥å•æ“ä½œ",
                "action": "filter_out"
            },
            {
                "name": "è‡ªåŠ¨å®Œç»“é€šçŸ¥",
                "pattern": r"ã€è‡ªåŠ¨å®Œç»“å·¥å•ã€‘.*",
                "description": "ç³»ç»Ÿè‡ªåŠ¨å®Œç»“é€šçŸ¥",
                "action": "filter_out"
            },
            {
                "name": "ç³»ç»ŸçŠ¶æ€æ›´æ–°",
                "pattern": r"ã€.*?ã€‘.*?(çŠ¶æ€|æ›´æ–°|å˜æ›´)",
                "description": "ç³»ç»Ÿè‡ªåŠ¨çŠ¶æ€æ›´æ–°",
                "action": "filter_out"
            },
            {
                "name": "å·¥å•åˆ›å»ºé€šçŸ¥",
                "pattern": r"å·¥å•.*?(åˆ›å»º|æäº¤|ç”Ÿæˆ)",
                "description": "å·¥å•åˆ›å»ºç³»ç»Ÿé€šçŸ¥",
                "action": "filter_out"
            },
            {
                "name": "è‡ªåŠ¨åˆ†é…é€šçŸ¥",
                "pattern": r"(è‡ªåŠ¨åˆ†é…|ç³»ç»Ÿåˆ†é…|å·²åˆ†é…ç»™)",
                "description": "ç³»ç»Ÿè‡ªåŠ¨åˆ†é…é€šçŸ¥",
                "action": "filter_out"
            },
            {
                "name": "å‚¬å•æé†’",
                "pattern": r"(å‚¬å•|æé†’|è¶…æ—¶)",
                "description": "ç³»ç»Ÿå‚¬å•æé†’",
                "action": "filter_out"
            },
            {
                "name": "è®¢å•çŠ¶æ€å˜æ›´",
                "pattern": r".*?(å·²æ’¤å•|è®¢å•å·²æ´¾å•|å·²æ´¾å•|è®¢å•çŠ¶æ€|æ´¾å•æˆåŠŸ|æ’¤å•æˆåŠŸ)",
                "description": "è®¢å•çŠ¶æ€è‡ªåŠ¨å˜æ›´é€šçŸ¥",
                "action": "filter_out"
            }
        ]
    
    def _init_invalid_data_patterns(self) -> List[Dict[str, Any]]:
        """åˆå§‹åŒ–æ— æ•ˆæ•°æ®æ¨¡å¼"""
        return [
            {
                "name": "é‡å¤æ•°å­—",
                "pattern": r"^(\d)\1{2,}$",  # åŒ¹é… 111, 222, 1111 ç­‰
                "description": "é‡å¤çš„æ•°å­—ä¸²",
                "action": "filter_out"
            },
            {
                "name": "å•å­—ç¬¦é‡å¤",
                "pattern": r"^(.)\1{2,}$",  # åŒ¹é… aaa, BBB ç­‰
                "description": "é‡å¤çš„å•å­—ç¬¦",
                "action": "filter_out"
            },
            {
                "name": "çº¯æ•°å­—çŸ­å†…å®¹",
                "pattern": r"^[\d\s]{1,5}$",  # 1-5ä½çº¯æ•°å­—
                "description": "è¿‡çŸ­çš„çº¯æ•°å­—å†…å®¹",
                "action": "filter_out"
            },
            {
                "name": "æµ‹è¯•å†…å®¹",
                "pattern": r"^(test|æµ‹è¯•|TEST|Test|\.\.\.|ã€‚ã€‚ã€‚)$",
                "description": "æ˜æ˜¾çš„æµ‹è¯•å†…å®¹",
                "action": "filter_out"
            },
            {
                "name": "ç©ºç™½æˆ–ç¬¦å·",
                "pattern": r"^[\s\-_=+\*\.]{1,10}$",
                "description": "åªå«ç©ºç™½å­—ç¬¦æˆ–ç®€å•ç¬¦å·",
                "action": "filter_out"
            },
            {
                "name": "æ„ä¹‰ä¸æ˜çš„çŸ­å†…å®¹",
                "pattern": r"^[a-zA-Z]{1,3}$",  # 1-3ä¸ªå­—æ¯
                "description": "è¿‡çŸ­æ— æ„ä¹‰å­—æ¯",
                "action": "filter_out"
            }
        ]
    
    def _init_system_keywords(self) -> List[str]:
        """åˆå§‹åŒ–ç³»ç»Ÿå…³é”®è¯"""
        return [
            "ç³»ç»Ÿ", "è‡ªåŠ¨", "é€šçŸ¥", "æé†’", "åˆ†é…", "è½¬æ´¾",
            "ã€å®Œç»“ã€‘", "ã€å¤„ç†ä¸­ã€‘", "ã€å¾…å¤„ç†ã€‘", "ã€å·²åˆ†é…ã€‘", "ã€è‡ªåŠ¨å®Œç»“å·¥å•ã€‘",
            "å·¥å•åˆ›å»º", "å·¥å•å…³é—­", "çŠ¶æ€å˜æ›´", "ä¼˜å…ˆçº§è°ƒæ•´",
            "å·²æ’¤å•", "è®¢å•å·²æ´¾å•", "æ´¾å•æˆåŠŸ", "æ’¤å•æˆåŠŸ", "è®¢å•çŠ¶æ€"
        ]
    
    def is_normal_operation(self, content: str, user_type: str = None, name: str = None) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ­£å¸¸æ“ä½œè®°å½•
        
        Args:
            content: è¯„è®ºå†…å®¹
            user_type: ç”¨æˆ·ç±»å‹ (system, service, customer)
            name: ç”¨æˆ·åç§°
            
        Returns:
            (æ˜¯å¦ä¸ºæ­£å¸¸æ“ä½œ, åŒ¹é…çš„è§„åˆ™æè¿°)
        """
        if not content or not isinstance(content, str):
            return False, ""
        
        content = content.strip()
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºç³»ç»Ÿç”¨æˆ·çš„æ“ä½œ
        if user_type == "system":
            return True, "ç³»ç»Ÿç”¨æˆ·æ“ä½œ"
        
        # 2. æ£€æŸ¥æ˜¯å¦åŒ…å«ç³»ç»Ÿå…³é”®è¯
        for keyword in self.system_keywords:
            if keyword in content:
                return True, f"åŒ…å«ç³»ç»Ÿå…³é”®è¯: {keyword}"
        
        # 3. æ£€æŸ¥æ­£å¸¸æ“ä½œæ¨¡å¼
        for pattern_config in self.normal_operation_patterns:
            if re.search(pattern_config["pattern"], content, re.IGNORECASE):
                return True, pattern_config["description"]
        
        # 4. æ£€æŸ¥ç‰¹å®šæ ¼å¼çš„æ­£å¸¸æ“ä½œ
        # å·¥å•å®¢æœçš„æ ‡å‡†æ“ä½œæ ¼å¼
        if name and "å·¥å•å®¢æœ" in name:
            if re.match(r"^\d+$", content) and len(content) <= 10:
                return True, "å·¥å•å®¢æœæ•°å­—æ ‡è®°"
        
        return False, ""
    
    def is_invalid_data(self, content: str) -> Tuple[bool, str]:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ— æ•ˆæ•°æ®
        
        Args:
            content: è¯„è®ºå†…å®¹
            
        Returns:
            (æ˜¯å¦ä¸ºæ— æ•ˆæ•°æ®, åŒ¹é…çš„è§„åˆ™æè¿°)
        """
        if not content or not isinstance(content, str):
            return True, "ç©ºå†…å®¹"
        
        content = content.strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–åªæœ‰ç©ºç™½å­—ç¬¦
        if not content:
            return True, "ç©ºç™½å†…å®¹"
        
        # æ£€æŸ¥æ— æ•ˆæ•°æ®æ¨¡å¼
        for pattern_config in self.invalid_data_patterns:
            if re.match(pattern_config["pattern"], content, re.IGNORECASE):
                return True, pattern_config["description"]
        
        # æ£€æŸ¥å†…å®¹é•¿åº¦ï¼ˆå¤ªçŸ­å¯èƒ½æ— æ„ä¹‰ï¼‰
        if len(content) <= 2 and not re.match(r'^[\u4e00-\u9fff]+$', content):  # éä¸­æ–‡ä¸”è¿‡çŸ­
            return True, "å†…å®¹è¿‡çŸ­ä¸”éä¸­æ–‡"
        
        return False, ""
    
    def should_filter_comment(self, comment: Dict[str, Any]) -> Tuple[bool, str]:
        """
        åˆ¤æ–­è¯„è®ºæ˜¯å¦åº”è¯¥è¢«è¿‡æ»¤
        
        Args:
            comment: è¯„è®ºæ•°æ®å­—å…¸ï¼ŒåŒ…å«content, user_type, nameç­‰å­—æ®µ
            
        Returns:
            (æ˜¯å¦åº”è¯¥è¿‡æ»¤, è¿‡æ»¤åŸå› )
        """
        content = str(comment.get("content", "")).strip()
        user_type = comment.get("user_type", "")
        name = comment.get("name", "")
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºæ­£å¸¸æ“ä½œ
        is_normal, normal_reason = self.is_normal_operation(content, user_type, name)
        if is_normal:
            return True, f"æ­£å¸¸æ“ä½œ: {normal_reason}"
        
        # 2. æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆæ•°æ®
        is_invalid, invalid_reason = self.is_invalid_data(content)
        if is_invalid:
            return True, f"æ— æ•ˆæ•°æ®: {invalid_reason}"
        
        return False, ""
    
    def filter_comments(self, comments: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        è¿‡æ»¤è¯„è®ºåˆ—è¡¨ï¼Œå»é™¤å™ªéŸ³æ•°æ®
        
        Args:
            comments: è¯„è®ºåˆ—è¡¨
            
        Returns:
            {
                "filtered_comments": è¿‡æ»¤åçš„è¯„è®ºåˆ—è¡¨,
                "original_count": åŸå§‹è¯„è®ºæ•°é‡,
                "filtered_count": è¿‡æ»¤åè¯„è®ºæ•°é‡,
                "removed_count": è¢«ç§»é™¤çš„è¯„è®ºæ•°é‡,
                "filter_statistics": è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
            }
        """
        if not comments:
            return {
                "filtered_comments": [],
                "original_count": 0,
                "filtered_count": 0,
                "removed_count": 0,
                "filter_statistics": {}
            }
        
        logger.info(f"ğŸ” å¼€å§‹è¿‡æ»¤è¯„è®ºï¼ŒåŸå§‹æ•°é‡: {len(comments)}")
        
        filtered_comments = []
        removed_comments = []
        filter_reasons = {}
        
        for i, comment in enumerate(comments):
            should_filter, reason = self.should_filter_comment(comment)
            
            if should_filter:
                removed_comments.append({
                    "index": i,
                    "comment": comment,
                    "reason": reason
                })
                
                # ç»Ÿè®¡è¿‡æ»¤åŸå› 
                filter_reasons[reason] = filter_reasons.get(reason, 0) + 1
                
                logger.debug(f"âš ï¸ è¿‡æ»¤è¯„è®º #{i}: {comment.get('content', '')[:50]}... - åŸå› : {reason}")
            else:
                filtered_comments.append(comment)
        
        result = {
            "filtered_comments": filtered_comments,
            "original_count": len(comments),
            "filtered_count": len(filtered_comments),
            "removed_count": len(removed_comments),
            "filter_statistics": {
                "filter_reasons": filter_reasons,
                "removed_details": removed_comments[:10]  # åªä¿ç•™å‰10ä¸ªè¢«ç§»é™¤çš„è¯¦æƒ…
            }
        }
        
        logger.info(f"âœ… è¯„è®ºè¿‡æ»¤å®Œæˆ:")
        logger.info(f"  ğŸ“¥ åŸå§‹æ•°é‡: {result['original_count']}")
        logger.info(f"  ğŸ“¤ è¿‡æ»¤åæ•°é‡: {result['filtered_count']}")
        logger.info(f"  ğŸ—‘ï¸ ç§»é™¤æ•°é‡: {result['removed_count']}")
        logger.info(f"  ğŸ“Š è¿‡æ»¤ç‡: {(result['removed_count'] / result['original_count'] * 100):.1f}%" if result['original_count'] > 0 else "0%")
        
        if filter_reasons:
            logger.info("ğŸ” è¿‡æ»¤åŸå› ç»Ÿè®¡:")
            for reason, count in filter_reasons.items():
                logger.info(f"  - {reason}: {count} æ¡")
        
        return result
    
    def filter_comments_with_record(
        self, 
        comments: List[Dict[str, Any]], 
        work_id: int,
        db: Optional[Session] = None,
        save_record: bool = True
    ) -> Dict[str, Any]:
        """
        è¿‡æ»¤è¯„è®ºå¹¶ä¿å­˜å»å™ªè®°å½•
        
        Args:
            comments: è¯„è®ºåˆ—è¡¨
            work_id: å·¥å•ID
            db: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼‰
            save_record: æ˜¯å¦ä¿å­˜å»å™ªè®°å½•
            
        Returns:
            è¿‡æ»¤ç»“æœå’Œè®°å½•ä¿¡æ¯
        """
        # æ‰§è¡Œæ­£å¸¸çš„è¿‡æ»¤
        filter_result = self.filter_comments(comments)
        
        # å¦‚æœéœ€è¦ä¿å­˜è®°å½•ä¸”æœ‰æ•°æ®åº“è¿æ¥
        if save_record and db and work_id:
            try:
                from app.models.denoise import denoise_record_manager
                import time
                
                # ç”Ÿæˆå•ç‹¬çš„æ‰¹æ¬¡ID
                batch_id = denoise_record_manager.generate_batch_id()
                
                # åˆ›å»ºæ‰¹æ¬¡è®°å½•
                denoise_record_manager.create_batch_record(db, batch_id, 1)
                
                # ä¿å­˜å·¥å•å»å™ªè®°å½•
                start_time = time.time()
                success = denoise_record_manager.save_work_order_denoise_record(
                    db, work_id, batch_id, filter_result
                )
                processing_time_ms = int((time.time() - start_time) * 1000)
                
                if success:
                    # æ›´æ–°æ‰¹æ¬¡ç»Ÿè®¡
                    batch_statistics = {
                        "total_work_orders": 1,
                        "total_original_comments": filter_result["original_count"],
                        "total_filtered_comments": filter_result["filtered_count"],
                        "total_removed_comments": filter_result["removed_count"],
                        "overall_filter_rate": (filter_result["removed_count"] / filter_result["original_count"] * 100) if filter_result["original_count"] > 0 else 0.0,
                        "filter_reasons": filter_result["filter_statistics"]["filter_reasons"],
                        "total_processing_time_ms": processing_time_ms
                    }
                    
                    denoise_record_manager.update_batch_statistics(
                        db, batch_id, batch_statistics, "COMPLETED"
                    )
                    
                    logger.info(f"âœ… ä¿å­˜å·¥å• {work_id} å»å™ªè®°å½•æˆåŠŸï¼Œæ‰¹æ¬¡: {batch_id}")
                    
                    # æ·»åŠ è®°å½•ä¿¡æ¯åˆ°ç»“æœä¸­
                    filter_result["denoise_record"] = {
                        "batch_id": batch_id,
                        "saved": True,
                        "processing_time_ms": processing_time_ms
                    }
                else:
                    logger.warning(f"âš ï¸ ä¿å­˜å·¥å• {work_id} å»å™ªè®°å½•å¤±è´¥")
                    filter_result["denoise_record"] = {"saved": False, "error": "ä¿å­˜å¤±è´¥"}
                    
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜å·¥å• {work_id} å»å™ªè®°å½•å¼‚å¸¸: {e}")
                filter_result["denoise_record"] = {"saved": False, "error": str(e)}
        else:
            filter_result["denoise_record"] = {"saved": False, "reason": "æœªæ»¡è¶³ä¿å­˜æ¡ä»¶"}
        
        return filter_result
    
    def check_comment_quality(self, comment: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•æ¡è¯„è®ºçš„è´¨é‡
        
        Args:
            comment: è¯„è®ºæ•°æ®
            
        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        content = str(comment.get("content", "")).strip()
        user_type = comment.get("user_type", "")
        name = comment.get("name", "")
        
        result = {
            "content": content,
            "length": len(content),
            "user_type": user_type,
            "name": name,
            "is_valid": True,
            "quality_score": 1.0,
            "issues": []
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ­£å¸¸æ“ä½œ
        is_normal, normal_reason = self.is_normal_operation(content, user_type, name)
        if is_normal:
            result["is_valid"] = False
            result["quality_score"] = 0.0
            result["issues"].append(f"æ­£å¸¸æ“ä½œ: {normal_reason}")
            return result
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆæ•°æ®
        is_invalid, invalid_reason = self.is_invalid_data(content)
        if is_invalid:
            result["is_valid"] = False
            result["quality_score"] = 0.0
            result["issues"].append(f"æ— æ•ˆæ•°æ®: {invalid_reason}")
            return result
        
        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_score = 1.0
        
        # é•¿åº¦è¯„åˆ†
        if len(content) < 5:
            quality_score -= 0.3
            result["issues"].append("å†…å®¹è¾ƒçŸ­")
        elif len(content) < 10:
            quality_score -= 0.1
            result["issues"].append("å†…å®¹åçŸ­")
        
        # å­—ç¬¦å¤šæ ·æ€§è¯„åˆ†
        unique_chars = len(set(content))
        if unique_chars < 3:
            quality_score -= 0.3
            result["issues"].append("å­—ç¬¦é‡å¤åº¦é«˜")
        
        # æ˜¯å¦åŒ…å«æœ‰æ„ä¹‰çš„ä¿¡æ¯
        if re.match(r'^[\d\s\-_=+\*\.]+$', content):
            quality_score -= 0.4
            result["issues"].append("ä¸»è¦ä¸ºæ•°å­—æˆ–ç¬¦å·")
        
        result["quality_score"] = max(0.0, quality_score)
        
        return result
    
    def batch_filter_work_orders(
        self, 
        work_orders: List[Dict[str, Any]], 
        db: Optional[Session] = None,
        save_records: bool = True
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡è¿‡æ»¤å·¥å•çš„è¯„è®ºæ•°æ®
        
        Args:
            work_orders: å·¥å•åˆ—è¡¨ï¼Œæ¯ä¸ªå·¥å•åŒ…å«comments_data
            db: æ•°æ®åº“ä¼šè¯ï¼ˆå¯é€‰ï¼Œç”¨äºä¿å­˜è®°å½•ï¼‰
            save_records: æ˜¯å¦ä¿å­˜å»å™ªè®°å½•åˆ°æ•°æ®åº“
            
        Returns:
            æ‰¹é‡è¿‡æ»¤ç»“æœ
        """
        start_time = time.time()
        logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡è¿‡æ»¤ {len(work_orders)} ä¸ªå·¥å•çš„è¯„è®º")
        
        # å¯¼å…¥è®°å½•ç®¡ç†å™¨
        batch_id = None
        if save_records and db:
            from app.models.denoise import denoise_record_manager
            batch_id = denoise_record_manager.generate_batch_id()
            denoise_record_manager.create_batch_record(db, batch_id, len(work_orders))
            logger.info(f"ğŸ·ï¸ åˆ›å»ºæ‰¹æ¬¡è®°å½•: {batch_id}")
        
        total_original = 0
        total_filtered = 0
        total_removed = 0
        global_filter_reasons = {}
        
        processed_orders = []
        
        for order in work_orders:
            work_id = order.get("work_id", "æœªçŸ¥")
            comments_data = order.get("comments_data", {})
            
            if not comments_data or "messages" not in comments_data:
                logger.debug(f"â­ï¸ å·¥å• {work_id} æ— è¯„è®ºæ•°æ®ï¼Œè·³è¿‡")
                processed_orders.append(order)
                continue
            
            # è¿‡æ»¤è¯„è®º
            filter_result = self.filter_comments(comments_data["messages"])
            
            # ä¿å­˜å•ä¸ªå·¥å•çš„å»å™ªè®°å½•
            if save_records and db and batch_id:
                work_start_time = time.time()
                denoise_record_manager.save_work_order_denoise_record(
                    db, work_id, batch_id, filter_result
                )
                work_processing_time = int((time.time() - work_start_time) * 1000)
                logger.debug(f"ğŸ’¾ ä¿å­˜å·¥å• {work_id} å»å™ªè®°å½•ï¼Œè€—æ—¶: {work_processing_time}ms")
            
            # æ›´æ–°ç»Ÿè®¡
            total_original += filter_result["original_count"]
            total_filtered += filter_result["filtered_count"]
            total_removed += filter_result["removed_count"]
            
            # åˆå¹¶è¿‡æ»¤åŸå› ç»Ÿè®¡
            for reason, count in filter_result["filter_statistics"]["filter_reasons"].items():
                global_filter_reasons[reason] = global_filter_reasons.get(reason, 0) + count
            
            # æ›´æ–°å·¥å•æ•°æ®
            updated_order = order.copy()
            if filter_result["filtered_count"] > 0:
                # é‡æ–°æ„å»ºcomments_data
                updated_comments_data = comments_data.copy()
                updated_comments_data["messages"] = filter_result["filtered_comments"]
                updated_comments_data["total_messages"] = filter_result["filtered_count"]
                
                # é‡æ–°æ„å»ºå¯¹è¯æ–‡æœ¬
                from app.services.stage1_work_extraction import stage1_service
                updated_comments_data["conversation_text"] = stage1_service.build_conversation_text(
                    filter_result["filtered_comments"]
                )
                
                updated_order["comments_data"] = updated_comments_data
                updated_order["comment_count"] = filter_result["filtered_count"]
                updated_order["has_comments"] = filter_result["filtered_count"] > 0
            else:
                # å¦‚æœæ‰€æœ‰è¯„è®ºéƒ½è¢«è¿‡æ»¤æ‰äº†ï¼Œæ ‡è®°ä¸ºæ— è¯„è®º
                updated_order["comments_data"] = None
                updated_order["comment_count"] = 0
                updated_order["has_comments"] = False
            
            # æ·»åŠ è¿‡æ»¤ä¿¡æ¯
            updated_order["denoise_info"] = {
                "original_comment_count": filter_result["original_count"],
                "filtered_comment_count": filter_result["filtered_count"],
                "removed_comment_count": filter_result["removed_count"],
                "filter_applied": True
            }
            
            processed_orders.append(updated_order)
            
            logger.debug(f"ğŸ“‹ å·¥å• {work_id}: {filter_result['original_count']} -> {filter_result['filtered_count']} æ¡è¯„è®º")
        
        # è®¡ç®—æ€»å¤„ç†æ—¶é—´
        total_processing_time_ms = int((time.time() - start_time) * 1000)
        
        # æ„å»ºç»“æœ
        result = {
            "processed_orders": processed_orders,
            "total_work_orders": len(work_orders),
            "batch_id": batch_id,
            "statistics": {
                "total_original_comments": total_original,
                "total_filtered_comments": total_filtered,
                "total_removed_comments": total_removed,
                "overall_filter_rate": (total_removed / total_original * 100) if total_original > 0 else 0,
                "filter_reasons": global_filter_reasons,
                "total_processing_time_ms": total_processing_time_ms
            }
        }
        
        # ä¿å­˜æ‰¹æ¬¡ç»Ÿè®¡
        if save_records and db and batch_id:
            try:
                denoise_record_manager.update_batch_statistics(
                    db, batch_id, result["statistics"], "COMPLETED"
                )
                logger.info(f"âœ… ä¿å­˜æ‰¹æ¬¡ç»Ÿè®¡: {batch_id}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜æ‰¹æ¬¡ç»Ÿè®¡å¤±è´¥: {e}")
                denoise_record_manager.update_batch_statistics(
                    db, batch_id, result["statistics"], "FAILED", str(e)
                )
        
        logger.info("ğŸ‰ æ‰¹é‡è¿‡æ»¤å®Œæˆ:")
        logger.info(f"  ğŸ“‹ å¤„ç†å·¥å•æ•°: {result['total_work_orders']}")
        logger.info(f"  ğŸ“¥ åŸå§‹è¯„è®ºæ€»æ•°: {total_original}")
        logger.info(f"  ğŸ“¤ è¿‡æ»¤åè¯„è®ºæ€»æ•°: {total_filtered}")
        logger.info(f"  ğŸ—‘ï¸ ç§»é™¤è¯„è®ºæ€»æ•°: {total_removed}")
        logger.info(f"  ğŸ“Š æ•´ä½“è¿‡æ»¤ç‡: {result['statistics']['overall_filter_rate']:.1f}%")
        
        return result


# å…¨å±€å»å™ªå™¨å®ä¾‹
content_denoiser = ContentDenoiser()
