"""
ä»»åŠ¡ç®¡ç†APIæ¥å£
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import text

from app.db.database import get_db
from app.core.auth import get_current_user
from app.models.task import task_record
from app.services.apscheduler_service import apscheduler_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/tasks", tags=["ä»»åŠ¡ç®¡ç†"])


@router.get("/records", summary="è·å–ä»»åŠ¡è®°å½•åˆ—è¡¨")
async def get_task_records(
    limit: int = Query(50, description="æ¯é¡µè®°å½•æ•°", ge=1, le=200),
    offset: int = Query(0, description="åç§»é‡", ge=0),
    task_type: Optional[str] = Query(None, description="ä»»åŠ¡ç±»å‹ç­›é€‰"),
    status: Optional[str] = Query(None, description="çŠ¶æ€ç­›é€‰"),
    trigger_type: Optional[str] = Query(None, description="è§¦å‘ç±»å‹ç­›é€‰"),
    start_date: Optional[str] = Query(None, description="å¼€å§‹æ—¥æœŸç­›é€‰ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ç»“æŸæ—¥æœŸç­›é€‰ (YYYY-MM-DD)"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–ä»»åŠ¡æ‰§è¡Œè®°å½•åˆ—è¡¨
    
    - **limit**: æ¯é¡µè®°å½•æ•° (1-200)
    - **offset**: åç§»é‡
    - **task_type**: ä»»åŠ¡ç±»å‹ç­›é€‰ (batch_analysis, manual_analysis, cleanupç­‰)
    - **status**: çŠ¶æ€ç­›é€‰ (running, completed, failed, cancelled)
    - **trigger_type**: è§¦å‘ç±»å‹ç­›é€‰ (scheduled, manual)
    - **start_date**: å¼€å§‹æ—¥æœŸç­›é€‰
    - **end_date**: ç»“æŸæ—¥æœŸç­›é€‰
    """
    try:
        # è§£ææ—¥æœŸå‚æ•°
        start_datetime = None
        end_datetime = None
        
        if start_date:
            try:
                start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        
        if end_date:
            try:
                end_datetime = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
            except ValueError:
                raise HTTPException(status_code=400, detail="ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        
        # è·å–ä»»åŠ¡è®°å½•
        records = task_record.get_task_records(
            db=db,
            limit=limit,
            offset=offset,
            task_type=task_type,
            status=status,
            trigger_type=trigger_type,
            start_date=start_datetime,
            end_date=end_datetime
        )
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªä»»åŠ¡è®°å½•æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
        enhanced_records = []
        for record in records:
            enhanced_record = record.copy()
            
            # æ·»åŠ æ˜¯å¦å¯ä»¥ç»ˆæ­¢çš„æ ‡è¯†
            task_status = record.get("status", "")
            enhanced_record["can_terminate"] = task_status in ["running", "pending"]
            
            # æ·»åŠ çŠ¶æ€æ˜¾ç¤ºå‹å¥½æ–‡æœ¬
            status_display_map = {
                "running": "è¿è¡Œä¸­",
                "completed": "å·²å®Œæˆ", 
                "failed": "å¤±è´¥",
                "cancelled": "å·²å–æ¶ˆ",
                "pending": "ç­‰å¾…ä¸­"
            }
            enhanced_record["status_display"] = status_display_map.get(task_status, task_status)
            
            enhanced_records.append(enhanced_record)
        
        return {
            "success": True,
            "data": enhanced_records,
            "pagination": {
                "limit": limit,
                "offset": offset,
                "total": len(records)
            },
            "filters": {
                "task_type": task_type,
                "status": status,
                "trigger_type": trigger_type,
                "start_date": start_date,
                "end_date": end_date
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è®°å½•å¤±è´¥: {str(e)}")


@router.get("/records/{task_id}", summary="è·å–å•ä¸ªä»»åŠ¡è®°å½•è¯¦æƒ…")
async def get_task_record_detail(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """è·å–æŒ‡å®šä»»åŠ¡çš„è¯¦ç»†è®°å½•"""
    try:
        record = task_record.get_task_record(db, task_id)
        
        if not record:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡è®°å½•ä¸å­˜åœ¨: {task_id}")
        
        # ğŸ”¥ ä¸ºä»»åŠ¡è®°å½•æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
        enhanced_record = record.copy()
        task_status = record.get("status", "")
        enhanced_record["can_terminate"] = task_status in ["running", "pending"]
        
        # æ·»åŠ çŠ¶æ€æ˜¾ç¤ºå‹å¥½æ–‡æœ¬
        status_display_map = {
            "running": "è¿è¡Œä¸­",
            "completed": "å·²å®Œæˆ", 
            "failed": "å¤±è´¥",
            "cancelled": "å·²å–æ¶ˆ",
            "pending": "ç­‰å¾…ä¸­"
        }
        enhanced_record["status_display"] = status_display_map.get(task_status, task_status)
        
        return {
            "success": True,
            "data": enhanced_record
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®°å½•è¯¦æƒ…å¤±è´¥: {task_id}, {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è®°å½•è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/status/{task_id}", summary="è·å–ä»»åŠ¡å®æ—¶çŠ¶æ€ - ä¾›å‰ç«¯è½®è¯¢ä½¿ç”¨")
async def get_task_status(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–ä»»åŠ¡å®æ—¶çŠ¶æ€ - ä¸“ä¸ºå‰ç«¯è½®è¯¢è®¾è®¡
    
    **è¿”å›**: 
    - **status**: ä»»åŠ¡çŠ¶æ€ (running, completed, failed, cancelled, pending)
    - **progress**: è¿›åº¦ä¿¡æ¯ (ç™¾åˆ†æ¯”ã€å·²å¤„ç†æ•°é‡ç­‰)
    - **stage**: å½“å‰æ‰§è¡Œé˜¶æ®µ
    - **message**: çŠ¶æ€æè¿°ä¿¡æ¯
    - **can_terminate**: æ˜¯å¦å¯ä»¥ç»ˆæ­¢
    - **last_update**: æœ€åæ›´æ–°æ—¶é—´
    """
    try:
        record = task_record.get_task_record(db, task_id)
        
        if not record:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        task_status = record.get("status", "")
        process_stage = record.get("process_stage", "")
        
        # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
        total_records = record.get("total_records", 0)
        processed_records = record.get("processed_records", 0)
        
        progress_percentage = 0
        if total_records > 0:
            progress_percentage = min(100, round((processed_records / total_records) * 100, 1))
        elif task_status == "completed":
            progress_percentage = 100
        
        # æ„å»ºè¿›åº¦ä¿¡æ¯
        progress_info = {
            "percentage": progress_percentage,
            "total_records": total_records,
            "processed_records": processed_records,
            "success_records": record.get("success_records", 0),
            "failed_records": record.get("failed_records", 0),
            "extracted_records": record.get("extracted_records", 0),
            "analyzed_records": record.get("analyzed_records", 0),
            "skipped_records": record.get("skipped_records", 0)
        }
        
        # ç”ŸæˆçŠ¶æ€æ¶ˆæ¯
        status_messages = {
            "running": f"æ­£åœ¨æ‰§è¡Œ - {process_stage}" if process_stage else "æ­£åœ¨æ‰§è¡Œ",
            "completed": f"æ‰§è¡Œå®Œæˆ - å¤„ç†äº†{processed_records}æ¡è®°å½•",
            "failed": f"æ‰§è¡Œå¤±è´¥ - {record.get('error_message', 'æœªçŸ¥é”™è¯¯')}",
            "cancelled": f"å·²å–æ¶ˆ - {record.get('error_message', 'ç”¨æˆ·å–æ¶ˆ')}",
            "pending": "ç­‰å¾…æ‰§è¡Œ"
        }
        
        # æ£€æŸ¥å¼‚æ­¥ä»»åŠ¡ç®¡ç†å™¨ä¸­çš„çŠ¶æ€
        from app.core.concurrency import async_task_manager
        async_task_status = None
        
        # å°è¯•ä»æ‰§è¡Œè¯¦æƒ…ä¸­è·å–async_task_id
        execution_details = record.get("execution_details", {})
        if isinstance(execution_details, str):
            import json
            try:
                execution_details = json.loads(execution_details)
            except:
                execution_details = {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å¼‚æ­¥ä»»åŠ¡
        async_task_id = None
        for running_task_id in async_task_manager.running_tasks.keys():
            if task_id in running_task_id:
                async_task_id = running_task_id
                break
        
        if async_task_id:
            async_task_status = async_task_manager.get_task_status(async_task_id)
        
        return {
            "success": True,
            "task_id": task_id,
            "status": task_status,
            "status_display": {
                "running": "è¿è¡Œä¸­",
                "completed": "å·²å®Œæˆ", 
                "failed": "å¤±è´¥",
                "cancelled": "å·²å–æ¶ˆ",
                "pending": "ç­‰å¾…ä¸­"
            }.get(task_status, task_status),
            "stage": process_stage,
            "progress": progress_info,
            "message": status_messages.get(task_status, f"çŠ¶æ€: {task_status}"),
            "can_terminate": task_status in ["running", "pending"],
            "is_active": task_status in ["running", "pending"],
            "last_update": record.get("updated_at"),
            "created_at": record.get("created_at"),
            "async_task_info": {
                "async_task_id": async_task_id,
                "async_status": async_task_status
            } if async_task_id else None,
            "execution_details": execution_details
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_id}, {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")


@router.get("/statistics", summary="è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯")
async def get_task_statistics(
    days: int = Query(7, description="ç»Ÿè®¡å¤©æ•°", ge=1, le=90),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """è·å–ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = task_record.get_task_statistics(db, days)
        
        return {
            "success": True,
            "data": stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.post("/manual-analysis", summary="æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡ - åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_manual_analysis(
    limit: int = Query(200, description="åˆ†æè®°å½•æ•°é™åˆ¶", ge=1, le=2000),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰‹åŠ¨è§¦å‘åˆ†æä»»åŠ¡ - åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
    
    - **limit**: åˆ†æè®°å½•æ•°é™åˆ¶ (1-2000)
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        running_tasks = task_record.get_task_records(
            db=db,
            limit=10,
            status="running"
        )
        
        if running_tasks:
            return {
                "success": False,
                "message": "æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†æ‰§è¡Œ",
                "running_tasks": [{"task_id": t["task_id"], "task_name": t["task_name"]} for t in running_tasks]
            }
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨è§¦å‘åˆ†æä»»åŠ¡ï¼Œé™åˆ¶: {limit} æ¡")
        
        # ğŸ”¥ åˆ›å»ºä»»åŠ¡è®°å½•
        main_task_id = task_record.create_task_record(
            db=db,
            task_name="æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡",
            task_type="manual_analysis",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "æ‰‹åŠ¨è§¦å‘çš„åˆ†æä»»åŠ¡",
                "analysis_limit": limit,
                "requested_by": username
            }
        )
        
        # ğŸ”¥ æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"manual_analysis_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_manual_analysis_async(db, limit, username, main_task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": main_task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"æ‰‹åŠ¨åˆ†æä»»åŠ¡å·²æäº¤åˆ°åå°æ‰§è¡Œï¼Œé™åˆ¶: {limit} æ¡",
            "analysis_limit": limit,
            "check_status_url": f"/api/v1/tasks/status/{main_task_id}"
        }
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/manual-extraction", summary="æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡ - åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_manual_extraction(
    target_date: Optional[str] = Query(None, description="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰‹åŠ¨è§¦å‘æ•°æ®æŠ½å–ä»»åŠ¡ - åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
    
    - **target_date**: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
    """
    try:
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date:
            try:
                target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        else:
            target_datetime = datetime.now() - timedelta(days=1)
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        task_id = task_record.create_task_record(
            db=db,
            task_name="æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡",
            task_type="manual_extraction",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "æ‰‹åŠ¨è§¦å‘çš„æ•°æ®æŠ½å–ä»»åŠ¡",
                "target_date": target_date or target_datetime.strftime("%Y-%m-%d"),
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨è§¦å‘æ•°æ®æŠ½å–ä»»åŠ¡: {task_id}, ç›®æ ‡æ—¥æœŸ: {target_datetime.date()}")
        
        # ğŸ”¥ æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"manual_extraction_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_manual_extraction_async(db, target_datetime, username, task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"æ•°æ®æŠ½å–ä»»åŠ¡å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "target_date": target_datetime.strftime("%Y-%m-%d"),
            "check_status_url": f"/api/v1/tasks/status/{task_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/scheduler/status", summary="è·å–è°ƒåº¦å™¨çŠ¶æ€")
async def get_scheduler_status(
    current_user: dict = Depends(get_current_user)
):
    """è·å–APSchedulerè°ƒåº¦å™¨çŠ¶æ€"""
    try:
        status = apscheduler_service.get_status()
        
        return {
            "success": True,
            "data": status
        }
        
    except Exception as e:
        logger.error(f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {str(e)}")


@router.post("/cleanup", summary="æ¸…ç†æ—§ä»»åŠ¡è®°å½•")
async def cleanup_old_records(
    days_to_keep: int = Query(30, description="ä¿ç•™å¤©æ•°", ge=7, le=180),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½•"""
    try:
        username = current_user.get("username", "unknown")
        logger.info(f"ğŸ§¹ ç”¨æˆ· {username} è§¦å‘æ¸…ç†æ—§ä»»åŠ¡è®°å½•ï¼Œä¿ç•™ {days_to_keep} å¤©")
        
        deleted_count = task_record.cleanup_old_records(db, days_to_keep)
        
        return {
            "success": True,
            "message": f"æˆåŠŸæ¸…ç† {deleted_count} æ¡ {days_to_keep} å¤©å‰çš„ä»»åŠ¡è®°å½•",
            "deleted_count": deleted_count,
            "days_to_keep": days_to_keep
        }
        
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ—§ä»»åŠ¡è®°å½•å¤±è´¥: {str(e)}")


@router.post("/full-task", summary="æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ - åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_full_task(
    target_date: Optional[str] = Query(None, description="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©"),
    analysis_limit: int = Query(200, description="åˆ†æè®°å½•æ•°é™åˆ¶", ge=1, le=2000),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼šå…ˆæŠ½å–ååˆ†æ
    
    - **target_date**: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
    - **analysis_limit**: åˆ†æè®°å½•æ•°é™åˆ¶ (1-2000)
    """
    try:
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date:
            try:
                target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        else:
            target_datetime = datetime.now() - timedelta(days=1)
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # åˆ›å»ºä¸»ä»»åŠ¡è®°å½•
        main_task_id = task_record.create_task_record(
            db=db,
            task_name="å®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆæŠ½å–+åˆ†æï¼‰",
            task_type="batch_analysis",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "å®Œæ•´ä»»åŠ¡æµç¨‹ï¼šå…ˆæŠ½å–ååˆ†æ",
                "target_date": target_date or target_datetime.strftime("%Y-%m-%d"),
                "analysis_limit": analysis_limit,
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸš€ ç”¨æˆ· {username} è§¦å‘å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}")
        
        # æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"full_task_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_full_task_async(db, target_datetime, analysis_limit, username, main_task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": main_task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"å®Œæ•´ä»»åŠ¡æµç¨‹å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "target_date": target_datetime.strftime("%Y-%m-%d"),
            "analysis_limit": analysis_limit,
            "check_status_url": f"/api/v1/tasks/status/{main_task_id}"
        }
        

        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {str(e)}")


@router.post("/full-task-range", summary="æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰æ—¶é—´èŒƒå›´+å¾ªç¯åˆ†æï¼‰- åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_full_task_range(
    start_time: str = Query(..., description="å¼€å§‹æ—¶é—´ (YYYY-MM-DDTHH:MM:SS)"),
    end_time: str = Query(..., description="ç»“æŸæ—¶é—´ (YYYY-MM-DDTHH:MM:SS)"),
    loop_analysis: bool = Query(True, description="æ˜¯å¦å¾ªç¯åˆ†æç›´åˆ°å®Œæˆ"),
    batch_size: int = Query(200, description="åˆ†ææ‰¹æ¬¡å¤§å°", ge=1, le=1000),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼šè‡ªå®šä¹‰æ—¶é—´èŒƒå›´æŠ½å–+å¾ªç¯åˆ†æ - åå°å¼‚æ­¥æ‰§è¡Œ
    
    - **start_time**: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDTHH:MM:SS
    - **end_time**: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDTHH:MM:SS
    - **loop_analysis**: æ˜¯å¦å¾ªç¯åˆ†æï¼ˆTrue=æ— é™å¾ªç¯ç›´åˆ°å®Œæˆï¼ŒFalse=ä»…å¤„ç†1è½®ï¼‰
    - **batch_size**: æ¯æ‰¹æ¬¡åˆ†æçš„è®°å½•æ•° (å»ºè®®50-200)
    
    **ğŸ”¥ æ€§èƒ½ä¼˜åŒ–**: 
    - å·²ä¿®å¤é˜»å¡é—®é¢˜ï¼šé‡‡ç”¨å¼‚æ­¥åˆ†æ‰¹å¤„ç†ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
    - loop_analysis=true: æ— é™å¾ªç¯å¤„ç†ï¼Œç›´åˆ°æ‰€æœ‰PENDINGæ•°æ®åˆ†æå®Œæˆ
    - loop_analysis=false: å•æ‰¹æ¬¡å¤„ç†ï¼Œç«‹å³è¿”å›ï¼Œé€‚åˆå¿«é€Ÿå¤„ç†åœºæ™¯
    
    **è¿”å›**: ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œå¯é€šè¿‡ `/api/v1/tasks/status/{task_id}` æŸ¥è¯¢è¿›åº¦
    """
    try:
        # è§£ææ—¶é—´
        try:
            start_datetime = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
            end_datetime = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DDTHH:MM:SS æ ¼å¼")
        
        if start_datetime >= end_datetime:
            raise HTTPException(status_code=400, detail="å¼€å§‹æ—¶é—´å¿…é¡»æ—©äºç»“æŸæ—¶é—´")
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # è®¡ç®—æ—¶é—´è·¨åº¦
        duration_hours = round((end_datetime - start_datetime).total_seconds() / 3600, 2)
        
        # åˆ›å»ºä¸»ä»»åŠ¡è®°å½•
        main_task_id = task_record.create_task_record(
            db=db,
            task_name="å®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰èŒƒå›´+å¾ªç¯åˆ†æï¼‰",
            task_type="batch_analysis",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "å®Œæ•´ä»»åŠ¡æµç¨‹ï¼šè‡ªå®šä¹‰æ—¶é—´èŒƒå›´æŠ½å–+å¾ªç¯åˆ†æ",
                "start_time": start_time,
                "end_time": end_time,
                "duration_hours": duration_hours,
                "loop_analysis": loop_analysis,
                "batch_size": batch_size,
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸš€ ç”¨æˆ· {username} è§¦å‘å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
        
        # ğŸ”¥ ä¿®å¤ï¼šæäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"full_task_range_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_full_task_range_async(db, start_datetime, end_datetime, loop_analysis, batch_size, username, main_task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": main_task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"å®Œæ•´ä»»åŠ¡æµç¨‹å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "time_range": f"{start_time} ~ {end_time}",
            "duration_hours": duration_hours,
            "loop_analysis": loop_analysis,
            "batch_size": batch_size,
            "check_status_url": f"/api/v1/tasks/status/{main_task_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {str(e)}")


async def run_full_task_range_async(
    db: Session, 
    start_datetime: datetime, 
    end_datetime: datetime, 
    loop_analysis: bool, 
    batch_size: int, 
    username: str, 
    main_task_id: str
):
    """å¼‚æ­¥æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰æ—¶é—´èŒƒå›´+å¾ªç¯åˆ†æï¼‰"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, ç”¨æˆ·: {username}")
        
        # é˜¶æ®µ1ï¼šå›ºå®šæ¬¡æ•°åˆ†æ‰¹æ•°æ®æŠ½å–
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="å¼€å§‹æ•°æ®æŠ½å– - æŸ¥è¯¢æ€»æ•°é‡"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        # é‡æ„ï¼šå…ˆæŸ¥è¯¢æ€»æ•°é‡ï¼Œç„¶åå›ºå®šæ¬¡æ•°å¾ªç¯
        async def async_extract_in_batches():
            import asyncio
            batch_size = 1000
            current_offset = 0
            total_extracted = 0
            total_inserted = 0
            total_skipped = 0
            
            logger.info(f"ğŸ“Š é‡æ„åæŠ½å–é…ç½®: æ¯æ‰¹{batch_size}æ¡ï¼Œå…ˆæŸ¥è¯¢æ€»æ•°é‡å†å›ºå®šå¾ªç¯")
            
            # 1. å…ˆæŸ¥è¯¢éœ€è¦æŠ½å–çš„å·¥å•æ€»æ•°é‡
            try:
                current_year = start_datetime.year
                work_table_name = f"t_work_{current_year}"
                
                count_sql = f"""
                SELECT COUNT(*) as total_count
                FROM {work_table_name}
                WHERE create_time >= :start_time 
                AND create_time < :end_time
                AND deleted = 0
                AND state = 'FINISH'
                """
                
                logger.info(f"ğŸ” æŸ¥è¯¢å·¥å•æ€»æ•°é‡ï¼Œæ—¶é—´èŒƒå›´: {start_datetime} ~ {end_datetime}")
                count_result = db.execute(text(count_sql), {
                    "start_time": start_datetime,
                    "end_time": end_datetime
                })
                total_count = count_result.fetchone()[0]
                
                logger.info(f"ğŸ“Š æŸ¥è¯¢åˆ°éœ€è¦æŠ½å–çš„å·¥å•æ€»æ•°: {total_count}æ¡")
                
                if total_count == 0:
                    logger.info("âš ï¸ æ²¡æœ‰éœ€è¦æŠ½å–çš„å·¥å•")
                    return {
                        "success": True,
                        "stage": "æ•°æ®æŠ½å–",
                        "statistics": {"extracted": 0, "inserted": 0, "skipped": 0, "batches_processed": 0},
                        "message": "æ²¡æœ‰éœ€è¦æŠ½å–çš„å·¥å•"
                    }
                
                # 2. è®¡ç®—éœ€è¦çš„å¾ªç¯æ¬¡æ•°
                total_batches = (total_count + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
                logger.info(f"ğŸ“Š è®¡ç®—æ‰¹æ¬¡æ•°: æ€»è®¡{total_count}æ¡ Ã· {batch_size}æ¡/æ‰¹ = {total_batches}æ‰¹æ¬¡")
                
            except Exception as e:
                logger.error(f"âŒ æŸ¥è¯¢å·¥å•æ€»æ•°å¤±è´¥: {e}")
                return {
                    "success": False,
                    "stage": "æ•°æ®æŠ½å–",
                    "error": str(e),
                    "message": "æŸ¥è¯¢å·¥å•æ€»æ•°å¤±è´¥"
                }
            
            # 3. å›ºå®šæ¬¡æ•°å¾ªç¯æŠ½å–
            for batch_num in range(1, total_batches + 1):
                logger.info(f"ğŸ”„ å¼€å§‹ç¬¬{batch_num}/{total_batches}æ‰¹æŠ½å– (åç§»: {current_offset})")
                
                # æ›´æ–°ä»»åŠ¡è¿›åº¦
                task_record.update_task_progress(
                    db=db,
                    task_id=main_task_id,
                    process_stage=f"æ•°æ®æŠ½å–ä¸­ - ç¬¬{batch_num}/{total_batches}æ‰¹",
                    extracted_records=total_extracted
                )
                
                try:
                    # åˆ†æ‰¹æŠ½å–å·¥å•æ•°æ®
                    batch_orders = stage1_service.extract_work_orders_by_time_range(
                        db=db,
                        start_time=start_datetime,
                        end_time=end_datetime,
                        limit=batch_size,
                        offset=current_offset
                    )
                    
                    if not batch_orders:
                        logger.info(f"âœ… ç¬¬{batch_num}æ‰¹æ— æ•°æ®ï¼Œæå‰å®Œæˆ")
                        break
                    
                    # åˆ†æ‰¹æ’å…¥å¾…å¤„ç†è¡¨
                    insertion_result = stage1_service.insert_pending_analysis_records(
                        db=db, 
                        work_orders=batch_orders
                    )
                    
                    batch_inserted = insertion_result.get("inserted", 0)
                    batch_skipped = insertion_result.get("skipped", 0)
                    
                    total_extracted += len(batch_orders)
                    total_inserted += batch_inserted
                    total_skipped += batch_skipped
                    current_offset += len(batch_orders)
                    
                    logger.info(f"ğŸ“ˆ ç¬¬{batch_num}/{total_batches}æ‰¹å®Œæˆ: æŠ½å–{len(batch_orders)}æ¡ï¼Œæ’å…¥{batch_inserted}æ¡ï¼Œè·³è¿‡{batch_skipped}æ¡")
                        
                except Exception as e:
                    logger.error(f"âŒ ç¬¬{batch_num}æ‰¹æŠ½å–å¤±è´¥: {e}")
                    continue
                
                # å¼‚æ­¥è®©å‡ºæ‰§è¡Œæƒï¼Œé¿å…é•¿æ—¶é—´é˜»å¡
                await asyncio.sleep(0.2)
            
            return {
                "success": True,
                "stage": "å›ºå®šæ¬¡æ•°æ•°æ®æŠ½å–",
                "statistics": {
                    "extracted": total_extracted,
                    "inserted": total_inserted, 
                    "skipped": total_skipped,
                    "batches_processed": total_batches,
                    "planned_batches": total_batches
                },
                "message": f"å›ºå®šæ¬¡æ•°æŠ½å–å®Œæˆ: è®¡åˆ’{total_batches}æ‰¹ï¼ŒæŠ½å–{total_extracted}æ¡ï¼Œæ’å…¥{total_inserted}æ¡"
            }
        
        extraction_result = await async_extract_in_batches()
        
        if not extraction_result.get("success"):
            raise Exception(f"æ•°æ®æŠ½å–å¤±è´¥: {extraction_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        stats = extraction_result.get("statistics", {})
        extracted = stats.get("extracted", 0)
        inserted = stats.get("inserted", 0)
        skipped = stats.get("skipped", 0)
        
        # æ›´æ–°æŠ½å–é˜¶æ®µå®ŒæˆçŠ¶æ€
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–å®Œæˆï¼Œå‡†å¤‡å¼€å§‹åˆ†æ",
            extracted_records=extracted,
            skipped_records=skipped,
            duplicate_records=skipped
        )
        
        logger.info(f"âœ… é˜¶æ®µ1å¼‚æ­¥æŠ½å–å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡é‡å¤{skipped}æ¡")
        
        # é˜¶æ®µ2ï¼šå¾ªç¯åˆ†æï¼ˆç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºï¼‰
        from app.services.stage2_analysis_service import stage2_service
        
        # ğŸ”¥ åœ¨åˆ†æé˜¶æ®µå¼€å§‹æ—¶ï¼ŒæŸ¥è¯¢å®é™…çš„å¾…åˆ†æå·¥å•æ•°é‡ï¼ˆæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰ï¼Œè®¾ç½®æ­£ç¡®çš„è¿›åº¦åŸºå‡†
        pending_count_sql = f"""
        SELECT COUNT(*) as count 
        FROM {stage1_service.pending_table_name} 
        WHERE ai_status = 'PENDING' 
        AND create_time >= :start_time 
        AND create_time <= :end_time
        """
        pending_result = db.execute(text(pending_count_sql), {
            "start_time": start_datetime,
            "end_time": end_datetime
        })
        total_pending_orders = pending_result.fetchone()[0]
        
        # ğŸ”¥ ç°åœ¨è®¾ç½®åˆ†æé˜¶æ®µçš„è¿›åº¦åŸºå‡†ï¼šåŸºäºå®é™…å¾…åˆ†æçš„å·¥å•æ•°é‡
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="å¾ªç¯åˆ†æä¸­",
            total_records=total_pending_orders,
            processed_records=0
        )
        
        logger.info(f"ğŸ“Š åˆ†æé˜¶æ®µå¼€å§‹: å¾…åˆ†æå·¥å•æ•°={total_pending_orders}")
        
        total_successful = 0
        total_failed = 0
        total_analyzed = 0
        total_skipped = 0
        total_batches = 0
        
        # ğŸ”¥ ä¼˜åŒ–ï¼šå…ˆæŸ¥è¯¢pendingè¡¨æ€»æ•°æ®é‡ï¼Œåˆç†è®¾ç½®å¾ªç¯æ¬¡æ•°
        current_cycle = 0
        
        # é‡æ„ï¼šæ ¹æ®loop_analysiså‚æ•°å†³å®šå¤„ç†æ–¹å¼ï¼Œå…ˆæŸ¥è¯¢æ€»æ•°å†å›ºå®šå¾ªç¯
        if loop_analysis:
            logger.info("ğŸ”„ å¯ç”¨å¾ªç¯åˆ†ææ¨¡å¼ï¼Œé‡æ„ä¸ºå›ºå®šæ¬¡æ•°å¾ªç¯")
            
            # 1. å…ˆæŸ¥è¯¢pendingè¡¨ä¸­çš„æ€»å·¥å•æ•°é‡
            initial_pending_result = db.execute(text(pending_count_sql), {
                "start_time": start_datetime,
                "end_time": end_datetime
            })
            total_pending_count = initial_pending_result.fetchone()[0]
            
            # 2. è®¡ç®—éœ€è¦çš„å›ºå®šå¾ªç¯æ¬¡æ•°
            if total_pending_count > 0:
                total_cycles = (total_pending_count + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´
            else:
                total_cycles = 0
            
            logger.info(f"ğŸ“Š é‡æ„ååˆ†æè§„åˆ’:")
            logger.info(f"  ğŸ“¥ æ€»PENDINGå·¥å•æ•°: {total_pending_count}")
            logger.info(f"  ğŸ“¦ æ¯æ‰¹å¤„ç†æ•°é‡: {batch_size}")
            logger.info(f"  ğŸ”„ å›ºå®šå¾ªç¯æ¬¡æ•°: {total_cycles}")
            
            if total_pending_count == 0:
                logger.info("ğŸ“ æ²¡æœ‰å¾…å¤„ç†çš„PENDINGå·¥å•ï¼Œè·³è¿‡åˆ†æ")
            else:
                # 3. å›ºå®šæ¬¡æ•°å¾ªç¯å¤„ç†
                for cycle_num in range(1, total_cycles + 1):
                    current_cycle = cycle_num
                    logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {cycle_num}/{total_cycles} è½®åˆ†æ")
                    
                    # æ£€æŸ¥å½“å‰PENDINGçŠ¶æ€çš„å·¥å•æ•°é‡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                    pending_result = db.execute(text(pending_count_sql), {
                        "start_time": start_datetime,
                        "end_time": end_datetime
                    })
                    current_pending = pending_result.fetchone()[0]
                    
                    logger.info(f"ğŸ“‹ ç¬¬{cycle_num}è½®å¼€å§‹å‰: å‰©ä½™PENDINGå·¥å•æ•°={current_pending}")
                    
                    # å¦‚æœæ²¡æœ‰PENDINGå·¥å•äº†ï¼Œå¯ä»¥æå‰ç»“æŸ
                    if current_pending == 0:
                        logger.info("âœ… æ‰€æœ‰PENDINGå·¥å•å·²å¤„ç†å®Œæˆï¼Œæå‰ç»“æŸå¾ªç¯")
                        break
                    
                    # æ‰§è¡Œåˆ†æé˜Ÿåˆ—å¤„ç†
                    batch_result = await stage2_service.process_pending_analysis_queue(
                        db=db,
                        batch_size=batch_size,
                        max_concurrent=5,
                        start_date=start_datetime,
                        end_date=end_datetime
                    )
                    
                    # ç´¯è®¡ç»Ÿè®¡
                    analysis_stats = batch_result.get("analysis_statistics", {})
                    batch_successful = analysis_stats.get("successful_analyses", 0)
                    batch_failed = analysis_stats.get("failed_analyses", 0)
                    batch_analyzed = analysis_stats.get("analyzed_orders", 0)
                    batch_skipped = analysis_stats.get("skipped_orders", 0)
                    
                    total_successful += batch_successful
                    total_failed += batch_failed
                    total_analyzed += batch_analyzed
                    total_skipped += batch_skipped
                    total_batches += 1
                    
                    # æ›´æ–°ä»»åŠ¡è¿›åº¦
                    task_record.update_task_progress(
                        db=db,
                        task_id=main_task_id,
                        process_stage=f"åˆ†æç¬¬{cycle_num}/{total_cycles}è½®",
                        success_records=total_successful,
                        failed_records=total_failed,
                        analyzed_records=total_analyzed
                    )
                    
                    logger.info(f"ğŸ“ˆ ç¬¬{cycle_num}/{total_cycles}è½®å®Œæˆ: æˆåŠŸ{batch_successful}, å¤±è´¥{batch_failed}, åˆ†æ{batch_analyzed}, è·³è¿‡{batch_skipped}")
                    logger.info(f"ğŸ“Š ç´¯è®¡è¿›åº¦: æˆåŠŸ{total_successful}, å¤±è´¥{total_failed}, ç´¯è®¡è·³è¿‡{total_skipped}")
                    
                    # å¼‚æ­¥è®©å‡ºæ‰§è¡Œæ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´å ç”¨
                    import asyncio
                    await asyncio.sleep(0.5)
        else:
            logger.info("ğŸ¯ å•æ‰¹æ¬¡åˆ†ææ¨¡å¼ï¼Œå¤„ç†ä¸€æ‰¹åç«‹å³è¿”å›")
            
            # å…ˆæŸ¥è¯¢pendingè¡¨ä¸­çš„æ€»å·¥å•æ•°é‡
            initial_pending_result = db.execute(text(pending_count_sql), {
                "start_time": start_datetime,
                "end_time": end_datetime
            })
            total_pending_count = initial_pending_result.fetchone()[0]
            
            logger.info(f"ğŸ“Š å•æ‰¹æ¬¡åˆ†ææƒ…å†µ:")
            logger.info(f"  ğŸ“¥ æ€»PENDINGå·¥å•æ•°: {total_pending_count}")
            logger.info(f"  ğŸ“¦ æœ¬æ‰¹å¤„ç†æ•°é‡: {batch_size}")
            logger.info(f"  ğŸ“‹ é¢„è®¡å‰©ä½™è½®æ¬¡: {max(0, (total_pending_count - batch_size + batch_size - 1) // batch_size)}")
            
            current_cycle = 1
            
            # ğŸ”¥ ä¿®å¤ï¼šæ·»åŠ æ—¶é—´èŒƒå›´å‚æ•°åˆ°åˆ†æé˜Ÿåˆ—å¤„ç†
            batch_result = await stage2_service.process_pending_analysis_queue(
                db=db,
                batch_size=batch_size,
                max_concurrent=5,
                start_date=start_datetime,
                end_date=end_datetime
            )
            
            # ç´¯è®¡ç»Ÿè®¡ - ä¿®æ­£å­—æ®µåç§°
            analysis_stats = batch_result.get("analysis_statistics", {})
            batch_successful = analysis_stats.get("successful_analyses", 0)
            batch_failed = analysis_stats.get("failed_analyses", 0)
            batch_analyzed = analysis_stats.get("analyzed_orders", 0)
            batch_skipped = analysis_stats.get("skipped_orders", 0)
            
            total_successful += batch_successful
            total_failed += batch_failed
            total_analyzed += batch_analyzed
            total_skipped += batch_skipped
            total_batches += 1
            
            # æ›´æ–°ä»»åŠ¡è¿›åº¦
            task_record.update_task_progress(
                db=db,
                task_id=main_task_id,
                process_stage=f"å•æ‰¹æ¬¡åˆ†æå®Œæˆ",
                success_records=total_successful,
                failed_records=total_failed,
                analyzed_records=total_analyzed
            )
            
            logger.info(f"ğŸ“ˆ å•æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ{batch_successful}, å¤±è´¥{batch_failed}, åˆ†æ{batch_analyzed}, è·³è¿‡{batch_skipped}")
            logger.info("ğŸ¯ å•æ‰¹æ¬¡æ¨¡å¼ï¼Œå¤„ç†å®Œæˆç«‹å³è¿”å›")
            
            if not batch_result.get("success"):
                logger.warning(f"âš ï¸ ç¬¬{current_cycle}è½®åˆ†æå¤±è´¥: {batch_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
            
            # ğŸ”¥ å•æ‰¹æ¬¡æ¨¡å¼å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­å¤„ç†
            logger.info("ğŸ“ å•æ‰¹æ¬¡åˆ†ææ¨¡å¼å®Œæˆ")
            
            # ğŸ”¥ ä¿®å¤è¿›åº¦è®¡ç®—ï¼šprocessed_recordsåº”è¯¥åŒ…å«æ‰€æœ‰å·²å¤„ç†çš„è®°å½•ï¼ˆæˆåŠŸ+å¤±è´¥+è·³è¿‡ï¼‰
            final_total_processed = total_successful + total_failed + total_skipped
            
            # æ›´æ–°è¿›åº¦
            task_record.update_task_progress(
                db=db,
                task_id=main_task_id,
                processed_records=final_total_processed,
                success_records=total_successful,
                failed_records=total_failed,
                analyzed_records=total_successful
            )
            

            
        # ğŸ”¥ æ–°å¢ï¼šåˆ†æå®Œæˆåæ£€æŸ¥å‰©ä½™pendingæ•°é‡
        final_pending_result = db.execute(text(pending_count_sql), {
            "start_time": start_datetime,
            "end_time": end_datetime
        })
        final_pending_count = final_pending_result.fetchone()[0]
        
        logger.info("=" * 80)
        logger.info("ğŸ“ åˆ†æé˜¶æ®µå®Œæˆæ€»ç»“:")
        logger.info(f"  ğŸ”„ å®Œæˆå¾ªç¯è½®æ¬¡: {current_cycle}")
        logger.info(f"  âœ… æˆåŠŸåˆ†ææ•°é‡: {total_successful}")
        logger.info(f"  âŒ åˆ†æå¤±è´¥æ•°é‡: {total_failed}")
        logger.info(f"  â­ï¸ è·³è¿‡å¤„ç†æ•°é‡: {total_skipped}")
        logger.info(f"  ğŸ“‹ å‰©ä½™PENDINGæ•°: {final_pending_count}")
        if final_pending_count > 0:
            logger.warning(f"âš ï¸ è¿˜æœ‰ {final_pending_count} ä¸ªå·¥å•éœ€è¦ç»§ç»­å¤„ç†")
        else:
            logger.info("ğŸ‰ æ‰€æœ‰PENDINGå·¥å•å·²å®Œæˆå¤„ç†ï¼")
        logger.info("=" * 80)
        
        # æ„å»ºæœ€ç»ˆæ‰§è¡Œè¯¦æƒ…
        duration_hours = round((end_datetime - start_datetime).total_seconds() / 3600, 2)
        final_details = {
            "extraction_phase": {
                "extracted": extracted,
                "inserted": inserted,
                "skipped": skipped,
                "duration_hours": duration_hours,
                "time_range": f"{start_datetime} ~ {end_datetime}"
            },
            "analysis_phase": {
                "total_cycles": current_cycle,  # ğŸ”¥ å®é™…æ‰§è¡Œçš„å¾ªç¯æ¬¡æ•°
                "analyzed": total_analyzed,
                "successful": total_successful,
                "failed": total_failed,
                "skipped": total_skipped,
                "remaining_pending": final_pending_count,  # ğŸ”¥ æ–°å¢ï¼šå‰©ä½™pendingæ•°é‡
                "batch_size": batch_size,
                "loop_analysis": loop_analysis,
                "processing_mode": "æ— é™å¾ªç¯åˆ†æ" if loop_analysis else "å•æ‰¹æ¬¡åˆ†æ",  # ğŸ”¥ ä¿®å¤ï¼šæ›´æ–°å¤„ç†æ¨¡å¼
                "completion_status": "å®Œæˆ" if final_pending_count == 0 else f"æœªå®Œæˆ(å‰©ä½™{final_pending_count}æ¡)"  # ğŸ”¥ æ–°å¢ï¼šå®ŒæˆçŠ¶æ€
            },
            "performance_optimization": {  # ğŸ”¥ æ€§èƒ½ä¼˜åŒ–ä¿¡æ¯
                "blocking_prevention": "é‡‡ç”¨å¼‚æ­¥åˆ†æ‰¹å¤„ç†ï¼Œé¿å…é•¿æ—¶é—´é˜»å¡",
                "cycle_delay": "0.5ç§’",
                "termination_reason": "æ‰€æœ‰PENDINGæ•°æ®å¤„ç†å®Œæˆ" if final_pending_count == 0 else f"å¤„ç†å®Œæˆ(å‰©ä½™{final_pending_count}æ¡pending)"
            },
            "completion_summary": f"æŠ½å–{extracted}æ¡ï¼Œåˆ†æ{current_cycle}è½®ï¼ŒæˆåŠŸ{total_successful}æ¡ï¼Œå¤±è´¥{total_failed}æ¡ï¼Œè·³è¿‡{total_skipped}æ¡ï¼Œå‰©ä½™{final_pending_count}æ¡pending"
        }
        
        # å®Œæˆä¸»ä»»åŠ¡
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="completed",
            execution_details=final_details
        )
        
        logger.info(f"ğŸ‰ å®Œæ•´ä»»åŠ¡æµç¨‹å®Œæˆ: æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{total_successful}æ¡ï¼Œå‰©ä½™pending{final_pending_count}æ¡")
        
        return final_details
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä¸»ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}


@router.post("/stop/{task_id}", summary="åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
async def stop_task(
    task_id: str,
    reason: str = Query("æ‰‹åŠ¨åœæ­¢", description="åœæ­¢åŸå› "),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    try:
        username = current_user.get("username", "unknown")
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = task_record.get_task_by_id(db, task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        if task.get("status") not in ["running", "pending"]:
            return {
                "success": False,
                "message": f"ä»»åŠ¡å½“å‰çŠ¶æ€ä¸º {task.get('status')}ï¼Œæ— æ³•åœæ­¢"
            }
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²åœæ­¢
        success = task_record.complete_task(
            db=db,
            task_id=task_id,
            status="cancelled",
            error_message=f"ç”¨æˆ· {username} æ‰‹åŠ¨åœæ­¢: {reason}"
        )
        
        if success:
            logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²è¢«ç”¨æˆ· {username} åœæ­¢: {reason}")
            return {
                "success": True,
                "message": f"ä»»åŠ¡å·²åœæ­¢: {reason}",
                "task_id": task_id,
                "stopped_by": username
            }
        else:
            raise HTTPException(status_code=500, detail="åœæ­¢ä»»åŠ¡å¤±è´¥")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {task_id}, {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/stop/batch", summary="æ‰¹é‡åœæ­¢ä»»åŠ¡")
async def stop_tasks_batch(
    task_ids: list[str],
    reason: str = Query("æ‰¹é‡æ‰‹åŠ¨åœæ­¢", description="åœæ­¢åŸå› "),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """æ‰¹é‡åœæ­¢å¤šä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    try:
        username = current_user.get("username", "unknown")
        results = []
        
        for task_id in task_ids:
            try:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
                task = task_record.get_task_by_id(db, task_id)
                if not task:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
                    })
                    continue
                
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if task.get("status") not in ["running", "pending"]:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": f"ä»»åŠ¡çŠ¶æ€ä¸º {task.get('status')}ï¼Œæ— æ³•åœæ­¢"
                    })
                    continue
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²åœæ­¢
                success = task_record.complete_task(
                    db=db,
                    task_id=task_id,
                    status="cancelled",
                    error_message=f"ç”¨æˆ· {username} æ‰¹é‡åœæ­¢: {reason}"
                )
                
                if success:
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²è¢«ç”¨æˆ· {username} æ‰¹é‡åœæ­¢: {reason}")
                    results.append({
                        "task_id": task_id,
                        "success": True,
                        "message": f"ä»»åŠ¡å·²åœæ­¢: {reason}"
                    })
                else:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": "åœæ­¢ä»»åŠ¡å¤±è´¥"
                    })
                    
            except Exception as e:
                logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {task_id}, {e}")
                results.append({
                    "task_id": task_id,
                    "success": False,
                    "message": f"åœæ­¢å¤±è´¥: {str(e)}"
                })
        
        successful_count = sum(1 for r in results if r["success"])
        
        return {
            "success": successful_count > 0,
            "message": f"æ‰¹é‡åœæ­¢å®Œæˆï¼šæˆåŠŸ {successful_count}/{len(task_ids)} ä¸ªä»»åŠ¡",
            "results": results,
            "summary": {
                "total": len(task_ids),
                "successful": successful_count,
                "failed": len(task_ids) - successful_count,
                "stopped_by": username
            }
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/types", summary="è·å–ä»»åŠ¡ç±»å‹åˆ—è¡¨")
async def get_task_types(
    current_user: dict = Depends(get_current_user)
):
    """è·å–æ”¯æŒçš„ä»»åŠ¡ç±»å‹åˆ—è¡¨"""
    try:
        task_types = [
            {
                "type": "data_extraction",
                "name": "æ•°æ®æŠ½å–",
                "description": "ä»æ•°æ®åº“æŠ½å–å·¥å•æ•°æ®"
            },
            {
                "type": "batch_analysis", 
                "name": "æ‰¹é‡åˆ†æ",
                "description": "æ‰¹é‡åˆ†æå·¥å•è¯„è®º"
            },
            {
                "type": "cleanup",
                "name": "æ•°æ®æ¸…ç†", 
                "description": "æ¸…ç†æ—§çš„æ•°æ®è®°å½•"
            },
            {
                "type": "scheduled",
                "name": "å®šæ—¶ä»»åŠ¡",
                "description": "ç³»ç»Ÿå®šæ—¶æ‰§è¡Œçš„ä»»åŠ¡"
            }
        ]
        
        return {
            "success": True,
            "task_types": task_types,
            "total": len(task_types)
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç±»å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡ç±»å‹å¤±è´¥: {str(e)}")


@router.post("/validate-cron", summary="éªŒè¯CRONè¡¨è¾¾å¼")
async def validate_cron_expression_api(
    cron_data: dict,
    current_user: dict = Depends(get_current_user)
):
    """éªŒè¯CRONè¡¨è¾¾å¼æ ¼å¼å’Œæœ‰æ•ˆæ€§"""
    try:
        cron_expr = cron_data.get("cron_expression", "").strip()
        
        if not cron_expr:
            return {
                "success": False,
                "message": "cronè¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º"
            }
        
        # ä½¿ç”¨éªŒè¯å‡½æ•°
        from app.services.apscheduler_service import validate_cron_expression
        validation_result = validate_cron_expression(cron_expr)
        
        return {
            "success": validation_result["valid"],
            "message": validation_result["message"],
            "description": validation_result.get("description", ""),
            "next_runs": validation_result.get("next_runs", []),
            "cron_expression": cron_expr
        }
        
    except Exception as e:
        logger.error(f"éªŒè¯cronè¡¨è¾¾å¼å¤±è´¥: {e}")
        return {
            "success": False,
            "message": f"éªŒè¯å¤±è´¥: {str(e)}"
        }


@router.get("/configs", summary="è·å–ä»»åŠ¡é…ç½®åˆ—è¡¨")
async def get_task_configs(
    current_user: dict = Depends(get_current_user)
):
    """è·å–æ‰€æœ‰ä»»åŠ¡é…ç½®"""
    try:
        from app.models.task_config import task_config
        from app.db.database import get_db
        
        db = next(get_db())
        try:
            configs = task_config.get_all_tasks(db, enabled_only=False)
            
            # è·å–å½“å‰è¿è¡Œä¸­çš„ä»»åŠ¡
            running_tasks = task_record.get_task_records(
                db=db,
                limit=10,
                status="running"
            )
            
            # åˆ›å»ºè¿è¡Œä»»åŠ¡çš„å¿«é€ŸæŸ¥æ‰¾æ˜ å°„
            running_task_keys = set()
            for running_task in running_tasks:
                task_config_key = running_task.get("task_config_key")
                if task_config_key:
                    running_task_keys.add(task_config_key)
            
            # ä¼˜åŒ–æ¯ä¸ªé…ç½®é¡¹
            optimized_configs = []
            for config in configs:
                task_key = config.get("task_key")
                optimized_config = config.copy()
                
                # æ·»åŠ è¿è¡ŒçŠ¶æ€ä¿¡æ¯
                optimized_config["is_running"] = task_key in running_task_keys
                
                # æ ¼å¼åŒ–æ—¶é—´å’Œè°ƒåº¦æ˜¾ç¤º
                optimized_config["last_execution_display"] = _format_datetime_display(config.get("last_execution_time"))
                optimized_config["next_execution_display"] = _format_datetime_display(config.get("next_execution_time"))
                optimized_config["schedule_display"] = _format_schedule_display(config)
                
                optimized_configs.append(optimized_config)
            
        finally:
            db.close()
        
        return {
            "success": True,
            "data": optimized_configs,
            "summary": {
                "total_configs": len(configs),
                "enabled_configs": sum(1 for cfg in configs if cfg.get("is_enabled", False)),
                "running_tasks": len(running_task_keys)
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {str(e)}")


@router.post("/configs/{task_key}/toggle", summary="åˆ‡æ¢ä»»åŠ¡å¯ç”¨çŠ¶æ€")
async def toggle_task_enabled(
    task_key: str,
    enabled: bool = Query(..., description="æ˜¯å¦å¯ç”¨"),
    current_user: dict = Depends(get_current_user)
):
    """åˆ‡æ¢ä»»åŠ¡çš„å¯ç”¨/ç¦ç”¨çŠ¶æ€"""
    try:
        username = current_user.get("username", "unknown")
        
        # ä½¿ç”¨APSchedulerå¤„ç†ä»»åŠ¡åˆ‡æ¢
        success = await apscheduler_service.toggle_task_enabled(task_key, enabled)
        
        if success:
            status_text = "å¯ç”¨" if enabled else "ç¦ç”¨"
            logger.info(f"ğŸ”§ ç”¨æˆ· {username} {status_text}äº†ä»»åŠ¡: {task_key}")
            
            return {
                "success": True,
                "message": f"ä»»åŠ¡ {task_key} å·²{status_text}",
                "task_key": task_key,
                "enabled": enabled,
                "operator": username
            }
        else:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_key}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ‡æ¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")


@router.put("/configs/{task_key}", summary="æ›´æ–°ä»»åŠ¡é…ç½®")
async def update_task_config(
    task_key: str,
    updates: dict,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°ä»»åŠ¡é…ç½®"""
    try:
        username = current_user.get("username", "unknown")
        
        # éªŒè¯æ›´æ–°å­—æ®µ
        allowed_fields = {
            'task_name', 'task_description', 'schedule_interval', 'schedule_cron',
            'default_batch_size', 'priority', 'timeout_seconds', 'retry_times'
        }
        
        filtered_updates = {k: v for k, v in updates.items() if k in allowed_fields}
        
        if not filtered_updates:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æœ‰æ•ˆçš„æ›´æ–°å­—æ®µ")
        
        # éªŒè¯cronè¡¨è¾¾å¼ï¼ˆå¦‚æœæä¾›äº†ï¼‰
        if 'schedule_cron' in filtered_updates:
            cron_expr = filtered_updates['schedule_cron']
            if cron_expr and cron_expr.strip():
                from app.services.apscheduler_service import validate_cron_expression
                validation = validate_cron_expression(cron_expr)
                if not validation['valid']:
                    raise HTTPException(
                        status_code=400,
                        detail=f"cronè¡¨è¾¾å¼æ— æ•ˆ: {validation['message']}"
                    )
                logger.info(f"âœ… cronè¡¨è¾¾å¼éªŒè¯é€šè¿‡: {cron_expr} - {validation.get('description', '')}")
        
        # ä½¿ç”¨APScheduleræ›´æ–°ä»»åŠ¡é…ç½®
        success = await apscheduler_service.update_task_config(task_key, filtered_updates)
        
        if success:
            logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ›´æ–°äº†ä»»åŠ¡é…ç½®: {task_key}")
            
            return {
                "success": True,
                "message": f"ä»»åŠ¡é…ç½® {task_key} å·²æ›´æ–°",
                "task_key": task_key,
                "updates": filtered_updates,
                "operator": username
            }
        else:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_key}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡é…ç½®å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä»»åŠ¡é…ç½®å¤±è´¥: {str(e)}")


@router.post("/manual-execution/{task_key}", summary="æ‰‹åŠ¨æ‰§è¡ŒæŒ‡å®šä»»åŠ¡")
async def manual_execute_task(
    task_key: str,
    current_user: dict = Depends(get_current_user)
):
    """æ‰‹åŠ¨æ‰§è¡ŒæŒ‡å®šçš„ä»»åŠ¡é…ç½®"""
    try:
        from app.models.task_config import task_config as tc
        from app.db.database import get_db
        
        db = next(get_db())
        try:
            task_config = tc.get_task_by_key(db, task_key)
        finally:
            db.close()
        
        if not task_config:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡é…ç½®ä¸å­˜åœ¨: {task_key}")
        
        username = current_user.get("username", "unknown")
        task_handler = task_config.get("task_handler")
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡: {task_key} ({task_handler})")
        
        # æ ¹æ®ä»»åŠ¡å¤„ç†å™¨ç±»å‹è°ƒç”¨ç›¸åº”çš„æ‰§è¡Œæ–¹æ³•
        if task_handler == "batch_analysis":
            from app.services.stage2_analysis_service import stage2_service
            
            result = await stage2_service.process_pending_analysis_queue(
                db=next(get_db()),
                batch_size=task_config.get("default_batch_size", 50),
                max_concurrent=5
            )
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            if result.get("success"):
                analysis_stats = result.get("analysis_statistics", {})
                result["task_id"] = f"manual_{task_key}_{int(datetime.now().timestamp())}"
                
        elif task_handler == "cleanup":
            from app.db.connection_manager import get_db_session
            from sqlalchemy import text
            
            with get_db_session() as db:
                task_id = task_record.create_task_record(
                    db=db,
                    task_name=f"æ‰‹åŠ¨æ‰§è¡Œ - {task_config.get('task_name')}",
                    task_type="cleanup",
                    trigger_type="manual",
                    trigger_user=username,
                    task_config_key=task_key,
                    execution_details={
                        "task_key": task_key,
                        "description": "æ‰‹åŠ¨è§¦å‘çš„æ¸…ç†ä»»åŠ¡",
                        "requested_by": username
                    }
                )
                
                # æ‰§è¡Œæ¸…ç†é€»è¾‘
                cutoff_date = datetime.now() - timedelta(days=30)
                
                cleanup_sql = """
                DELETE FROM ai_work_pending_analysis 
                WHERE updated_at < :cutoff_date 
                AND ai_status IN ('COMPLETED', 'FAILED')
                """
                
                result = db.execute(text(cleanup_sql), {"cutoff_date": cutoff_date})
                deleted_count = result.rowcount
                
                if deleted_count > 0:
                    db.commit()
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä»»åŠ¡å®Œæˆ: åˆ é™¤äº†{deleted_count}æ¡è¿‡æœŸå¾…åˆ†æè®°å½•")
                else:
                    logger.info("âœ¨ æ¸…ç†ä»»åŠ¡å®Œæˆ: æ²¡æœ‰è¿‡æœŸè®°å½•éœ€è¦æ¸…ç†")
                
                # å®Œæˆä»»åŠ¡
                task_record.complete_task(
                    db=db,
                    task_id=task_id,
                    status="completed",
                    execution_details={
                        "completed_at": datetime.now().isoformat(),
                        "task_key": task_key,
                        "deleted_count": deleted_count
                    }
                )
                
                result = {
                    "success": True,
                    "task_id": task_id,
                    "message": f"æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œåˆ é™¤äº†{deleted_count}æ¡è®°å½•"
                }
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„ä»»åŠ¡å¤„ç†å™¨: {task_handler}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/configs-and-records", summary="è·å–ä»»åŠ¡é…ç½®å’Œæ‰§è¡Œè®°å½•çš„è”åˆè§†å›¾")
async def get_tasks_combined_view(
    limit: int = Query(50, description="è®°å½•æ•°é™åˆ¶", ge=1, le=200),
    current_user: dict = Depends(get_current_user)
):
    """è·å–ä»»åŠ¡é…ç½®å’Œæœ€è¿‘æ‰§è¡Œè®°å½•çš„è”åˆè§†å›¾"""
    try:
        from app.models.task_config import task_config as tc
        from app.db.database import get_db
        
        db = next(get_db())
        
        configs = tc.get_all_tasks(db, enabled_only=False)
        
        try:
            # è·å–æœ€è¿‘æ‰§è¡Œè®°å½•
            recent_records = task_record.get_task_records(
                db=db,
                limit=limit,
                offset=0
            )
            
            # è·å–å½“å‰è¿è¡Œä¸­çš„ä»»åŠ¡
            running_tasks = task_record.get_task_records(
                db=db,
                limit=10,
                status="running"
            )
            
        finally:
            db.close()
        
        # åˆå¹¶æ•°æ®
        combined_data = []
        
        # åˆ›å»ºè¿è¡Œä»»åŠ¡çš„å¿«é€ŸæŸ¥æ‰¾æ˜ å°„
        running_task_keys = set()
        for running_task in running_tasks:
            task_config_key = running_task.get("task_config_key")
            if task_config_key:
                running_task_keys.add(task_config_key)
        
        # å¤„ç†æ‰€æœ‰é…ç½®çš„ä»»åŠ¡
        for config in configs:
            task_key = config.get("task_key")
            
            # æŸ¥æ‰¾è¯¥ä»»åŠ¡çš„æœ€è¿‘æ‰§è¡Œè®°å½•
            latest_record = None
            execution_stats = {
                "total_executions": 0,
                "successful_executions": 0,
                "failed_executions": 0
            }
            
            for record in recent_records:
                if record.get("task_config_key") == task_key:
                    if not latest_record:
                        latest_record = record
                    
                    # ç»Ÿè®¡æ‰§è¡Œæƒ…å†µ
                    execution_stats["total_executions"] += 1
                    if record.get("status") == "completed":
                        execution_stats["successful_executions"] += 1
                    elif record.get("status") == "failed":
                        execution_stats["failed_executions"] += 1
            
            # ä¼˜åŒ–åçš„ä»»åŠ¡é¡¹ç›®æ•°æ®ç»“æ„
            combined_item = {
                "type": "configured_task",
                "task_key": task_key,
                "task_name": config.get("task_name"),
                "task_description": config.get("task_description"),
                "task_type": config.get("task_handler"),
                "status": "ç”Ÿæ•ˆ" if config.get("is_enabled", False) else "ä¸ç”Ÿæ•ˆ",
                "is_enabled": config.get("is_enabled", False),
                "is_running": task_key in running_task_keys,
                "schedule_interval": config.get("schedule_interval"),
                "schedule_display": _format_schedule_display(config),
                "priority": config.get("priority", 5),
                
                # æ—¶é—´ä¿¡æ¯
                "last_execution_time": config.get("last_execution_time"),
                "last_execution_display": _format_datetime_display(config.get("last_execution_time")),
                "next_execution_time": config.get("next_execution_time"),
                "next_execution_display": _format_datetime_display(config.get("next_execution_time")),
                
                # æ‰§è¡Œç»Ÿè®¡
                "execution_stats": {
                    "config_count": config.get("execution_count", 0),
                    "config_success": config.get("success_count", 0),
                    "config_failure": config.get("failure_count", 0),
                    "actual_total": execution_stats["total_executions"],
                    "actual_success": execution_stats["successful_executions"],
                    "actual_failed": execution_stats["failed_executions"],
                    "display_total": max(config.get("execution_count", 0), execution_stats["total_executions"]),
                    "display_success": max(config.get("success_count", 0), execution_stats["successful_executions"]),
                    "display_failed": max(config.get("failure_count", 0), execution_stats["failed_executions"])
                },
                
                # æœ€è¿‘æ‰§è¡Œè®°å½• - ğŸ”¥ æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
                "latest_record": latest_record,
                "has_recent_execution": latest_record is not None,
                
                # ğŸ”¥ ä¸ºæœ€è¿‘æ‰§è¡Œè®°å½•æ·»åŠ ç»ˆæ­¢èƒ½åŠ›
                "latest_record_can_terminate": (
                    latest_record.get("status") in ["running", "pending"] 
                    if latest_record else False
                )
            }
            
            combined_data.append(combined_item)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_running = len(running_task_keys)
        enabled_count = sum(1 for cfg in configs if cfg.get("is_enabled", False))
        
        return {
            "success": True,
            "data": {
                "combined_tasks": combined_data,
                "summary": {
                    "total_configured": len(configs),
                    "enabled_tasks": enabled_count,
                    "disabled_tasks": len(configs) - enabled_count,
                    "running_tasks": total_running,
                    "recent_executions": len(recent_records)
                }
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è”åˆè§†å›¾å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è”åˆè§†å›¾å¤±è´¥: {str(e)}")


def _format_schedule_display(config: dict) -> str:
    """æ ¼å¼åŒ–è°ƒåº¦æ˜¾ç¤º"""
    try:
        schedule_cron = config.get("schedule_cron")
        schedule_interval = config.get("schedule_interval", 0)
        
        # ä¼˜å…ˆæ˜¾ç¤ºcronè¡¨è¾¾å¼
        if schedule_cron and schedule_cron.strip():
            try:
                from app.services.apscheduler_service import _describe_cron_expression
                description = _describe_cron_expression(schedule_cron.strip())
                return f"CRON: {description} ({schedule_cron})"
            except:
                return f"CRON: {schedule_cron}"
        
        # é™çº§æ˜¾ç¤ºinterval
        elif schedule_interval:
            return f"é—´éš”: {_format_interval(schedule_interval)}"
        
        else:
            return "æœªè®¾ç½®"
            
    except Exception as e:
        return f"æ ¼å¼é”™è¯¯: {str(e)[:20]}"


def _format_interval(seconds: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´é—´éš”æ˜¾ç¤º"""
    if not seconds:
        return "æœªè®¾ç½®"
    
    if seconds < 60:
        return f"{seconds}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        return f"{minutes}åˆ†é’Ÿ"
    elif seconds < 86400:
        hours = seconds // 3600
        return f"{hours}å°æ—¶"
    else:
        days = seconds // 86400
        return f"{days}å¤©"


def _format_datetime_display(datetime_str: Optional[str]) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´æ˜¾ç¤º"""
    if not datetime_str:
        return "æ— æ•°æ®"
    
    try:
        if isinstance(datetime_str, str):
            clean_str = datetime_str.replace('Z', '').replace('+00:00', '')
            dt = datetime.fromisoformat(clean_str)
        else:
            dt = datetime_str
            
        now = datetime.now()
        diff = dt - now
        
        # æœªæ¥æ—¶é—´ï¼ˆä¸‹æ¬¡æ‰§è¡Œï¼‰
        if diff.total_seconds() > 0:
            if diff.days > 0:
                return f"{diff.days}å¤©å"
            elif diff.seconds > 3600:
                hours = diff.seconds // 3600
                return f"{hours}å°æ—¶å"
            elif diff.seconds > 60:
                minutes = diff.seconds // 60
                return f"{minutes}åˆ†é’Ÿå"
            else:
                return "å³å°†æ‰§è¡Œ"
        
        # è¿‡å»æ—¶é—´ï¼ˆä¸Šæ¬¡æ‰§è¡Œï¼‰
        else:
            diff = now - dt
            if diff.days > 0:
                return f"{diff.days}å¤©å‰"
            elif diff.seconds > 3600:
                hours = diff.seconds // 3600
                return f"{hours}å°æ—¶å‰"
            elif diff.seconds > 60:
                minutes = diff.seconds // 60
                return f"{minutes}åˆ†é’Ÿå‰"
            else:
                return "åˆšåˆš"
                
    except Exception as e:
        return f"æ ¼å¼é”™è¯¯: {str(e)[:20]}"


# ==================== åå°å¼‚æ­¥æ‰§è¡Œå‡½æ•° ====================

async def run_manual_extraction_async(db: Session, target_datetime: datetime, username: str, main_task_id: str):
    """å¼‚æ­¥æ‰§è¡Œæ‰‹åŠ¨æ•°æ®æŠ½å–ä»»åŠ¡"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ‰‹åŠ¨æ•°æ®æŠ½å–ä»»åŠ¡: {main_task_id}, ç”¨æˆ·: {username}, ç›®æ ‡æ—¥æœŸ: {target_datetime.date()}")
        
        # æ›´æ–°è¿›åº¦
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–ä¸­"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        # æ‰§è¡Œæ•°æ®æŠ½å–
        extraction_result = stage1_service.extract_work_data_by_time_range(
            db=db,
            target_date=target_datetime
        )
        
        if extraction_result.get("success"):
            stats = extraction_result.get("statistics", {})
            extracted = stats.get("extracted", 0)
            inserted = stats.get("inserted", 0)
            skipped = stats.get("skipped", 0)
            
            # å®Œæˆä»»åŠ¡
            final_details = {
                "extraction_result": extraction_result,
                "statistics": stats,
                "target_date": target_datetime.strftime("%Y-%m-%d"),
                "completion_message": f"æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡{skipped}æ¡"
            }
            
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="completed",
                execution_details=final_details
            )
            
            # æ›´æ–°ç»Ÿè®¡
            task_record.update_task_progress(
                db=db,
                task_id=main_task_id,
                total_records=extracted,
                extracted_records=extracted,
                success_records=inserted,
                skipped_records=skipped
            )
            
            logger.info(f"âœ… æ‰‹åŠ¨æ•°æ®æŠ½å–å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡{skipped}æ¡")
            return final_details
        else:
            raise Exception(f"æ•°æ®æŠ½å–å¤±è´¥: {extraction_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥æ‰‹åŠ¨æ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}


async def run_manual_analysis_async(db: Session, limit: int, username: str, main_task_id: str):
    """å¼‚æ­¥æ‰§è¡Œæ‰‹åŠ¨åˆ†æä»»åŠ¡"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ‰‹åŠ¨åˆ†æä»»åŠ¡: {main_task_id}, ç”¨æˆ·: {username}, é™åˆ¶: {limit}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ‰¹é‡åˆ†æä¸­"
        )
        
        # æ‰§è¡Œåˆ†æ
        from app.services.stage2_analysis_service import stage2_service
        
        result = await stage2_service.process_pending_analysis_queue(
            db=db,
            batch_size=limit,
            max_concurrent=5
        )
        
        if result.get("success"):
            analysis_stats = result.get("analysis_statistics", {})
            successful = analysis_stats.get("successful_analyses", 0)
            failed = analysis_stats.get("failed_analyses", 0)
            total_analyzed = analysis_stats.get("analyzed_orders", 0)
            skipped_orders = analysis_stats.get("skipped_orders", 0)
            
            # å®Œæˆä»»åŠ¡
            final_details = {
                "analysis_summary": {
                    "analyzed": total_analyzed,
                    "successful": successful,
                    "failed": failed,
                    "skipped": skipped_orders,
                    "limit": limit
                },
                "completion_message": f"æˆåŠŸåˆ†æ{successful}æ¡ï¼Œå¤±è´¥{failed}æ¡ï¼Œè·³è¿‡{skipped_orders}æ¡"
            }
            
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="completed",
                execution_details=final_details
            )
            
            logger.info(f"âœ… æ‰‹åŠ¨åˆ†æä»»åŠ¡å®Œæˆ: æˆåŠŸ{successful}æ¡ï¼Œå¤±è´¥{failed}æ¡")
            return final_details
        else:
            raise Exception(f"åˆ†æå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥æ‰‹åŠ¨åˆ†æä»»åŠ¡å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}


async def run_full_task_async(db: Session, target_datetime: datetime, analysis_limit: int, username: str, main_task_id: str):
    """å¼‚æ­¥æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, ç”¨æˆ·: {username}")
        
        # é˜¶æ®µ1ï¼šæ•°æ®æŠ½å–
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–ä¸­"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        extraction_result = stage1_service.extract_work_data_by_time_range(
            db=db,
            target_date=target_datetime
        )
        
        if not extraction_result.get("success"):
            raise Exception(f"æ•°æ®æŠ½å–å¤±è´¥: {extraction_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        stats = extraction_result.get("statistics", {})
        extracted = stats.get("extracted", 0)
        inserted = stats.get("inserted", 0)
        skipped = stats.get("skipped", 0)
        
        # ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šæŠ½å–é˜¶æ®µåªæ›´æ–°é˜¶æ®µä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ï¼Œä¸è®¾ç½®è¿›åº¦ç›¸å…³å­—æ®µ
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–å®Œæˆï¼Œå¼€å§‹åˆ†æ",
            extracted_records=extracted,
            skipped_records=skipped,
            duplicate_records=skipped
        )
        
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡é‡å¤{skipped}æ¡")
        
        # é˜¶æ®µ2ï¼šæ‰¹é‡åˆ†æ
        from app.services.stage2_analysis_service import stage2_service
        
        # åœ¨åˆ†æé˜¶æ®µå¼€å§‹æ—¶ï¼ŒæŸ¥è¯¢å®é™…çš„å¾…åˆ†æå·¥å•æ•°é‡ï¼Œè®¾ç½®æ­£ç¡®çš„è¿›åº¦åŸºå‡†
        pending_count_sql = f"""
        SELECT COUNT(*) as count 
        FROM {stage1_service.pending_table_name} 
        WHERE ai_status = 'PENDING'
        """
        pending_result = db.execute(text(pending_count_sql))
        total_pending_orders = pending_result.fetchone()[0]
        
        # ç°åœ¨è®¾ç½®åˆ†æé˜¶æ®µçš„è¿›åº¦åŸºå‡†ï¼šåŸºäºå®é™…å¾…åˆ†æçš„å·¥å•æ•°é‡
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ‰¹é‡åˆ†æä¸­",
            total_records=total_pending_orders,
            processed_records=0
        )
        
        logger.info(f"ğŸ“Š åˆ†æé˜¶æ®µå¼€å§‹: å¾…åˆ†æå·¥å•æ•°={total_pending_orders}")
        
        analysis_result = await stage2_service.process_pending_analysis_queue(
            db=db,
            batch_size=analysis_limit,
            max_concurrent=5
        )
        
        if not analysis_result.get("success"):
            raise Exception(f"æ‰¹é‡åˆ†æå¤±è´¥: {analysis_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        analysis_stats = analysis_result.get("analysis_statistics", {})
        successful = analysis_stats.get("successful_analyses", 0)
        failed = analysis_stats.get("failed_analyses", 0)
        total_analyzed = analysis_stats.get("analyzed_orders", 0)
        skipped_orders = analysis_stats.get("skipped_orders", 0)
        denoised_orders = analysis_stats.get("denoised_orders", 0)
        
        # å®Œæˆä¸»ä»»åŠ¡
        final_details = {
            "extraction_summary": {
                "extracted": extracted,
                "inserted": inserted,
                "skipped": skipped
            },
            "analysis_summary": {
                "analyzed": total_analyzed,
                "successful": successful,
                "failed": failed,
                "skipped": skipped_orders,
                "denoised": denoised_orders
            },
            "completion_message": f"æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{successful}æ¡"
        }
        
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="completed",
            execution_details=final_details
        )
        
        logger.info(f"ğŸ‰ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å®Œæˆ: æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{successful}æ¡")
        return final_details
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä¸»ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}
