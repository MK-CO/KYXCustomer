"""
ä»»åŠ¡ç®¡ç†APIæ¥å£
"""
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List
from fastapi import APIRouter, Depends, HTTPException, Query
from sqlalchemy.orm import Session
from sqlalchemy import text

from app.db.database import get_db
from app.core.auth import get_current_user
from app.models.task import task_record
from app.services.apscheduler_service import apscheduler_service

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/tasks", tags=["ä»»åŠ¡ç®¡ç†"])


@router.get("/records", summary="è·å–ä»»åŠ¡è®°å½•åˆ—è¡¨")
async def get_task_records(
    limit: int = Query(50, description="æ¯é¡µè®°å½•æ•°", ge=1, le=200),
    offset: int = Query(0, description="åç§»é‡", ge=0),
    task_type: Optional[str] = Query(None, description="ä»»åŠ¡ç±»å‹ç­›é€‰"),
    status: Optional[str] = Query(None, description="çŠ¶æ€ç­›é€‰"),
    trigger_type: Optional[str] = Query(None, description="è§¦å‘ç±»å‹ç­›é€‰"),
    start_date: Optional[str] = Query(None, description="å¼€å§‹æ—¥æœŸç­›é€‰ (YYYY-MM-DD)"),
    end_date: Optional[str] = Query(None, description="ç»“æŸæ—¥æœŸç­›é€‰ (YYYY-MM-DD)"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    è·å–ä»»åŠ¡æ‰§è¡Œè®°å½•åˆ—è¡¨
    
    - **limit**: æ¯é¡µè®°å½•æ•° (1-200)
    - **offset**: åç§»é‡
    - **task_type**: ä»»åŠ¡ç±»å‹ç­›é€‰ (batch_analysis, manual_analysis, cleanupç­‰)
    - **status**: çŠ¶æ€ç­›é€‰ (running, completed, failed, cancelled)
    - **trigger_type**: è§¦å‘ç±»å‹ç­›é€‰ (scheduled, manual)
    - **start_date**: å¼€å§‹æ—¥æœŸç­›é€‰
    - **end_date**: ç»“æŸæ—¥æœŸç­›é€‰
    """
    try:
        # è§£ææ—¥æœŸå‚æ•°
        start_datetime = None
        end_datetime = None
        
        if start_date:
            try:
                start_datetime = datetime.strptime(start_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        
        if end_date:
            try:
                end_datetime = datetime.strptime(end_date, "%Y-%m-%d") + timedelta(days=1)
            except ValueError:
                raise HTTPException(status_code=400, detail="ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        
        # è·å–ä»»åŠ¡è®°å½•
        records = task_record.get_task_records(
            db=db,
            limit=limit,
            offset=offset,
            task_type=task_type,
            status=status,
            trigger_type=trigger_type,
            start_date=start_datetime,
            end_date=end_datetime
        )
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªä»»åŠ¡è®°å½•æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
        enhanced_records = []
        for record in records:
            enhanced_record = record.copy()
            
            # æ·»åŠ æ˜¯å¦å¯ä»¥ç»ˆæ­¢çš„æ ‡è¯†
            task_status = record.get("status", "")
            enhanced_record["can_terminate"] = task_status in ["running", "pending"]
            
            # æ·»åŠ çŠ¶æ€æ˜¾ç¤ºå‹å¥½æ–‡æœ¬
            status_display_map = {
                "running": "è¿è¡Œä¸­",
                "completed": "å·²å®Œæˆ", 
                "failed": "å¤±è´¥",
                "cancelled": "å·²å–æ¶ˆ",
                "pending": "ç­‰å¾…ä¸­"
            }
            enhanced_record["status_display"] = status_display_map.get(task_status, task_status)
            
            enhanced_records.append(enhanced_record)
        
        return {
            "success": True,
            "data": enhanced_records,
            "pagination": {
                "limit": limit,
                "offset": offset,
                "total": len(records)
            },
            "filters": {
                "task_type": task_type,
                "status": status,
                "trigger_type": trigger_type,
                "start_date": start_date,
                "end_date": end_date
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è®°å½•å¤±è´¥: {str(e)}")


@router.get("/records/{task_id}", summary="è·å–å•ä¸ªä»»åŠ¡è®°å½•è¯¦æƒ…")
async def get_task_record_detail(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """è·å–æŒ‡å®šä»»åŠ¡çš„è¯¦ç»†è®°å½•"""
    try:
        record = task_record.get_task_record(db, task_id)
        
        if not record:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡è®°å½•ä¸å­˜åœ¨: {task_id}")
        
        # ğŸ”¥ ä¸ºä»»åŠ¡è®°å½•æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
        enhanced_record = record.copy()
        task_status = record.get("status", "")
        enhanced_record["can_terminate"] = task_status in ["running", "pending"]
        
        # æ·»åŠ çŠ¶æ€æ˜¾ç¤ºå‹å¥½æ–‡æœ¬
        status_display_map = {
            "running": "è¿è¡Œä¸­",
            "completed": "å·²å®Œæˆ", 
            "failed": "å¤±è´¥",
            "cancelled": "å·²å–æ¶ˆ",
            "pending": "ç­‰å¾…ä¸­"
        }
        enhanced_record["status_display"] = status_display_map.get(task_status, task_status)
        
        return {
            "success": True,
            "data": enhanced_record
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è®°å½•è¯¦æƒ…å¤±è´¥: {task_id}, {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è®°å½•è¯¦æƒ…å¤±è´¥: {str(e)}")


@router.get("/statistics", summary="è·å–ä»»åŠ¡ç»Ÿè®¡ä¿¡æ¯")
async def get_task_statistics(
    days: int = Query(7, description="ç»Ÿè®¡å¤©æ•°", ge=1, le=90),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """è·å–ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = task_record.get_task_statistics(db, days)
        
        return {
            "success": True,
            "data": stats,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}")


@router.post("/manual-analysis", summary="æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡")
async def run_manual_analysis(
    limit: int = Query(50, description="åˆ†æè®°å½•æ•°é™åˆ¶", ge=1, le=500),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰‹åŠ¨è§¦å‘åˆ†æä»»åŠ¡
    
    - **limit**: åˆ†æè®°å½•æ•°é™åˆ¶ (1-500)
    """
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
        running_tasks = task_record.get_task_records(
            db=db,
            limit=10,
            status="running"
        )
        
        if running_tasks:
            return {
                "success": False,
                "message": "æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·ç­‰å¾…å®Œæˆåå†æ‰§è¡Œ",
                "running_tasks": [{"task_id": t["task_id"], "task_name": t["task_name"]} for t in running_tasks]
            }
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨è§¦å‘åˆ†æä»»åŠ¡ï¼Œé™åˆ¶: {limit} æ¡")
        
        # ğŸš€ ç›´æ¥ä½¿ç”¨stage2åˆ†ææœåŠ¡æ‰§è¡Œæ‰‹åŠ¨åˆ†æ
        from app.services.stage2_analysis_service import stage2_service
        
        result = await stage2_service.process_pending_analysis_queue(
            db=db,
            batch_size=limit,
            max_concurrent=5
        )
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœä»¥ä¿æŒAPIå…¼å®¹æ€§
        if result.get("success"):
            analysis_stats = result.get("analysis_statistics", {})
            result["task_id"] = f"manual_{int(datetime.now().timestamp())}"
        
        return result
        
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œåˆ†æä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/manual-extraction", summary="æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡")
async def run_manual_extraction(
    target_date: Optional[str] = Query(None, description="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©"),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰‹åŠ¨è§¦å‘æ•°æ®æŠ½å–ä»»åŠ¡
    
    - **target_date**: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
    """
    try:
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date:
            try:
                target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        else:
            target_datetime = datetime.now() - timedelta(days=1)
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # åˆ›å»ºä»»åŠ¡è®°å½•
        task_id = task_record.create_task_record(
            db=db,
            task_name="æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡",
            task_type="manual_extraction",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "æ‰‹åŠ¨è§¦å‘çš„æ•°æ®æŠ½å–ä»»åŠ¡",
                "target_date": target_date or target_datetime.strftime("%Y-%m-%d"),
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨è§¦å‘æ•°æ®æŠ½å–ä»»åŠ¡: {task_id}, ç›®æ ‡æ—¥æœŸ: {target_datetime.date()}")
        
        # æ›´æ–°è¿›åº¦
        task_record.update_task_progress(
            db=db,
            task_id=task_id,
            process_stage="æ•°æ®æŠ½å–ä¸­"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        # æ‰§è¡Œæ•°æ®æŠ½å–
        extraction_result = stage1_service.extract_work_data_by_time_range(
            db=db,
            target_date=target_datetime
        )
        
        if extraction_result.get("success"):
            stats = extraction_result.get("statistics", {})
            extracted = stats.get("extracted", 0)
            inserted = stats.get("inserted", 0)
            
            # å®Œæˆä»»åŠ¡
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="completed",
                execution_details={
                    "extraction_result": extraction_result,
                    "completed_at": datetime.now().isoformat()
                }
            )
            
            # æ›´æ–°ç»Ÿè®¡
            task_record.update_task_progress(
                db=db,
                task_id=task_id,
                total_records=extracted,
                extracted_records=extracted,
                success_records=inserted
            )
            
            logger.info(f"âœ… æ‰‹åŠ¨æ•°æ®æŠ½å–å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡")
            
            return {
                "success": True,
                "task_id": task_id,
                "message": f"æ•°æ®æŠ½å–å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡",
                "statistics": stats,
                "target_date": target_datetime.strftime("%Y-%m-%d")
            }
        else:
            error_msg = extraction_result.get("message", "æœªçŸ¥é”™è¯¯")
            
            # æ ‡è®°ä»»åŠ¡å¤±è´¥
            task_record.complete_task(
                db=db,
                task_id=task_id,
                status="failed",
                error_message=error_msg,
                execution_details=extraction_result
            )
            
            return {
                "success": False,
                "task_id": task_id,
                "message": f"æ•°æ®æŠ½å–å¤±è´¥: {error_msg}",
                "error": error_msg
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {e}")
        
        # å¦‚æœä»»åŠ¡è®°å½•å·²åˆ›å»ºï¼Œæ ‡è®°ä¸ºå¤±è´¥
        if 'task_id' in locals():
            try:
                task_record.complete_task(
                    db=db,
                    task_id=task_id,
                    status="failed",
                    error_message=str(e)
                )
            except:
                pass
        
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œæ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/scheduler/status", summary="è·å–è°ƒåº¦å™¨çŠ¶æ€")
async def get_scheduler_status(
    current_user: dict = Depends(get_current_user)
):
    """è·å–APSchedulerè°ƒåº¦å™¨çŠ¶æ€"""
    try:
        status = apscheduler_service.get_status()
        
        return {
            "success": True,
            "data": status
        }
        
    except Exception as e:
        logger.error(f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {str(e)}")


@router.post("/cleanup", summary="æ¸…ç†æ—§ä»»åŠ¡è®°å½•")
async def cleanup_old_records(
    days_to_keep: int = Query(30, description="ä¿ç•™å¤©æ•°", ge=7, le=180),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½•"""
    try:
        username = current_user.get("username", "unknown")
        logger.info(f"ğŸ§¹ ç”¨æˆ· {username} è§¦å‘æ¸…ç†æ—§ä»»åŠ¡è®°å½•ï¼Œä¿ç•™ {days_to_keep} å¤©")
        
        deleted_count = task_record.cleanup_old_records(db, days_to_keep)
        
        return {
            "success": True,
            "message": f"æˆåŠŸæ¸…ç† {deleted_count} æ¡ {days_to_keep} å¤©å‰çš„ä»»åŠ¡è®°å½•",
            "deleted_count": deleted_count,
            "days_to_keep": days_to_keep
        }
        
    except Exception as e:
        logger.error(f"æ¸…ç†æ—§ä»»åŠ¡è®°å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç†æ—§ä»»åŠ¡è®°å½•å¤±è´¥: {str(e)}")


@router.post("/full-task", summary="æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ - åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_full_task(
    target_date: Optional[str] = Query(None, description="ç›®æ ‡æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºæ˜¨å¤©"),
    analysis_limit: int = Query(50, description="åˆ†æè®°å½•æ•°é™åˆ¶", ge=1, le=500),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼šå…ˆæŠ½å–ååˆ†æ
    
    - **target_date**: ç›®æ ‡æ—¥æœŸï¼Œæ ¼å¼ä¸º YYYY-MM-DDï¼Œé»˜è®¤ä¸ºæ˜¨å¤©
    - **analysis_limit**: åˆ†æè®°å½•æ•°é™åˆ¶ (1-500)
    """
    try:
        # è§£æç›®æ ‡æ—¥æœŸ
        if target_date:
            try:
                target_datetime = datetime.strptime(target_date, "%Y-%m-%d")
            except ValueError:
                raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
        else:
            target_datetime = datetime.now() - timedelta(days=1)
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # åˆ›å»ºä¸»ä»»åŠ¡è®°å½•
        main_task_id = task_record.create_task_record(
            db=db,
            task_name="å®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆæŠ½å–+åˆ†æï¼‰",
            task_type="batch_analysis",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "å®Œæ•´ä»»åŠ¡æµç¨‹ï¼šå…ˆæŠ½å–ååˆ†æ",
                "target_date": target_date or target_datetime.strftime("%Y-%m-%d"),
                "analysis_limit": analysis_limit,
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸš€ ç”¨æˆ· {username} è§¦å‘å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}")
        
        # æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"full_task_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_full_task_async(db, target_datetime, analysis_limit, username, main_task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": main_task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"å®Œæ•´ä»»åŠ¡æµç¨‹å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "target_date": target_datetime.strftime("%Y-%m-%d"),
            "analysis_limit": analysis_limit,
            "check_status_url": f"/api/v1/tasks/status/{main_task_id}"
        }
        

        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {str(e)}")


@router.post("/full-task-range", summary="æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰æ—¶é—´èŒƒå›´+å¾ªç¯åˆ†æï¼‰- åå°å¼‚æ­¥æ‰§è¡Œ")
async def run_full_task_range(
    start_time: str = Query(..., description="å¼€å§‹æ—¶é—´ (YYYY-MM-DDTHH:MM:SS)"),
    end_time: str = Query(..., description="ç»“æŸæ—¶é—´ (YYYY-MM-DDTHH:MM:SS)"),
    loop_analysis: bool = Query(True, description="æ˜¯å¦å¾ªç¯åˆ†æç›´åˆ°å®Œæˆ"),
    batch_size: int = Query(50, description="åˆ†ææ‰¹æ¬¡å¤§å°", ge=1, le=100),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """
    æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼šè‡ªå®šä¹‰æ—¶é—´èŒƒå›´æŠ½å–+å¾ªç¯åˆ†æ - åå°å¼‚æ­¥æ‰§è¡Œ
    
    - **start_time**: å¼€å§‹æ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDTHH:MM:SS
    - **end_time**: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼ä¸º YYYY-MM-DDTHH:MM:SS
    - **loop_analysis**: æ˜¯å¦å¾ªç¯åˆ†æç›´åˆ°é˜Ÿåˆ—ä¸ºç©º
    - **batch_size**: æ¯æ‰¹æ¬¡åˆ†æçš„è®°å½•æ•° (1-100)
    
    **è¿”å›**: ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œå¯é€šè¿‡ `/api/v1/tasks/status/{task_id}` æŸ¥è¯¢è¿›åº¦
    """
    try:
        # è§£ææ—¶é—´
        try:
            start_datetime = datetime.fromisoformat(start_time.replace('Z', '+00:00'))
            end_datetime = datetime.fromisoformat(end_time.replace('Z', '+00:00'))
        except ValueError:
            raise HTTPException(status_code=400, detail="æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DDTHH:MM:SS æ ¼å¼")
        
        if start_datetime >= end_datetime:
            raise HTTPException(status_code=400, detail="å¼€å§‹æ—¶é—´å¿…é¡»æ—©äºç»“æŸæ—¶é—´")
        
        # è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        username = current_user.get("username", "unknown")
        
        # è®¡ç®—æ—¶é—´è·¨åº¦
        duration_hours = round((end_datetime - start_datetime).total_seconds() / 3600, 2)
        
        # åˆ›å»ºä¸»ä»»åŠ¡è®°å½•
        main_task_id = task_record.create_task_record(
            db=db,
            task_name="å®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰èŒƒå›´+å¾ªç¯åˆ†æï¼‰",
            task_type="batch_analysis",
            trigger_type="manual",
            trigger_user=username,
            task_config_key="customer_service_analysis",
            execution_details={
                "description": "å®Œæ•´ä»»åŠ¡æµç¨‹ï¼šè‡ªå®šä¹‰æ—¶é—´èŒƒå›´æŠ½å–+å¾ªç¯åˆ†æ",
                "start_time": start_time,
                "end_time": end_time,
                "duration_hours": duration_hours,
                "loop_analysis": loop_analysis,
                "batch_size": batch_size,
                "requested_by": username
            }
        )
        
        logger.info(f"ğŸš€ ç”¨æˆ· {username} è§¦å‘å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, æ—¶é—´èŒƒå›´: {start_time} ~ {end_time}")
        
        # ğŸ”¥ ä¿®å¤ï¼šæäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
        from app.core.concurrency import async_task_manager
        import uuid
        
        async_task_id = f"full_task_range_{uuid.uuid4().hex[:16]}"
        success = await async_task_manager.submit_task(
            async_task_id,
            run_full_task_range_async(db, start_datetime, end_datetime, loop_analysis, batch_size, username, main_task_id)
        )
        
        if not success:
            # ä»»åŠ¡æäº¤å¤±è´¥ï¼Œæ›´æ–°æ•°æ®åº“è®°å½•
            task_record.complete_task(
                db=db,
                task_id=main_task_id,
                status="failed", 
                error_message="ä»»åŠ¡æäº¤åˆ°åå°é˜Ÿåˆ—å¤±è´¥"
            )
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…å®Œæˆ
        return {
            "success": True,
            "task_id": main_task_id,
            "async_task_id": async_task_id,
            "status": "submitted",
            "message": f"å®Œæ•´ä»»åŠ¡æµç¨‹å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "time_range": f"{start_time} ~ {end_time}",
            "duration_hours": duration_hours,
            "loop_analysis": loop_analysis,
            "batch_size": batch_size,
            "check_status_url": f"/api/v1/tasks/status/{main_task_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å®Œæ•´ä»»åŠ¡æµç¨‹å¯åŠ¨å¤±è´¥: {str(e)}")


async def run_full_task_range_async(
    db: Session, 
    start_datetime: datetime, 
    end_datetime: datetime, 
    loop_analysis: bool, 
    batch_size: int, 
    username: str, 
    main_task_id: str
):
    """å¼‚æ­¥æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹ï¼ˆè‡ªå®šä¹‰æ—¶é—´èŒƒå›´+å¾ªç¯åˆ†æï¼‰"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, ç”¨æˆ·: {username}")
        
        # é˜¶æ®µ1ï¼šæ•°æ®æŠ½å–
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–ä¸­"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        extraction_result = stage1_service.extract_work_data_by_time_range(
            db=db,
            start_time=start_datetime,
            end_time=end_datetime
        )
        
        if not extraction_result.get("success"):
            raise Exception(f"æ•°æ®æŠ½å–å¤±è´¥: {extraction_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        stats = extraction_result.get("statistics", {})
        extracted = stats.get("extracted", 0)
        inserted = stats.get("inserted", 0)
        skipped = stats.get("skipped", 0)
        
        # ğŸ”¥ ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šæŠ½å–é˜¶æ®µåªæ›´æ–°é˜¶æ®µä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ï¼Œä¸è®¾ç½®è¿›åº¦ç›¸å…³å­—æ®µ
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–å®Œæˆï¼Œå¼€å§‹å¾ªç¯åˆ†æ",
            extracted_records=extracted,
            skipped_records=skipped,
            duplicate_records=skipped
        )
        
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡é‡å¤{skipped}æ¡")
        
        # é˜¶æ®µ2ï¼šå¾ªç¯åˆ†æï¼ˆç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºï¼‰
        from app.services.stage2_analysis_service import stage2_service
        
        # ğŸ”¥ åœ¨åˆ†æé˜¶æ®µå¼€å§‹æ—¶ï¼ŒæŸ¥è¯¢å®é™…çš„å¾…åˆ†æå·¥å•æ•°é‡ï¼ˆæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰ï¼Œè®¾ç½®æ­£ç¡®çš„è¿›åº¦åŸºå‡†
        pending_count_sql = f"""
        SELECT COUNT(*) as count 
        FROM {stage1_service.pending_table_name} 
        WHERE ai_status = 'PENDING' 
        AND create_time >= :start_time 
        AND create_time <= :end_time
        """
        pending_result = db.execute(text(pending_count_sql), {
            "start_time": start_datetime,
            "end_time": end_datetime
        })
        total_pending_orders = pending_result.fetchone()[0]
        
        # ğŸ”¥ ç°åœ¨è®¾ç½®åˆ†æé˜¶æ®µçš„è¿›åº¦åŸºå‡†ï¼šåŸºäºå®é™…å¾…åˆ†æçš„å·¥å•æ•°é‡
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="å¾ªç¯åˆ†æä¸­",
            total_records=total_pending_orders,
            processed_records=0
        )
        
        logger.info(f"ğŸ“Š åˆ†æé˜¶æ®µå¼€å§‹: å¾…åˆ†æå·¥å•æ•°={total_pending_orders}")
        
        total_successful = 0
        total_failed = 0
        total_analyzed = 0
        total_skipped = 0
        total_batches = 0
        
        # å¾ªç¯å¤„ç†ï¼Œç›´åˆ°é˜Ÿåˆ—ä¸ºç©º
        consecutive_empty_batches = 0
        max_empty_batches = 3
        
        while True:
            # åœ¨æ¯æ¬¡å¾ªç¯å‰ï¼Œæ£€æŸ¥å½“å‰PENDINGçŠ¶æ€çš„å·¥å•æ•°é‡ï¼ˆæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰
            pending_result = db.execute(text(pending_count_sql), {
                "start_time": start_datetime,
                "end_time": end_datetime
            })
            current_pending = pending_result.fetchone()[0]
            
            logger.info(f"ğŸ“‹ å¾ªç¯å‰æ£€æŸ¥: å½“å‰PENDINGå·¥å•æ•°={current_pending}")
            
            # å¦‚æœæ²¡æœ‰PENDINGå·¥å•äº†ï¼Œç›´æ¥åœæ­¢
            if current_pending == 0:
                logger.info("ğŸ“ æ²¡æœ‰å¾…å¤„ç†çš„PENDINGå·¥å•ï¼Œå¾ªç¯åˆ†æå®Œæˆ")
                break
            
            # ğŸ”¥ ä¿®å¤ï¼šæ·»åŠ æ—¶é—´èŒƒå›´å‚æ•°åˆ°åˆ†æé˜Ÿåˆ—å¤„ç†
            batch_result = await stage2_service.process_pending_analysis_queue(
                db=db,
                batch_size=batch_size,
                max_concurrent=5,
                start_date=start_datetime,
                end_date=end_datetime
            )
            
            if not batch_result.get("success"):
                logger.warning(f"âš ï¸ æ‰¹æ¬¡åˆ†æå¤±è´¥: {batch_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                break
            
            analysis_stats = batch_result.get("analysis_statistics", {})
            batch_successful = analysis_stats.get("successful_analyses", 0)
            batch_failed = analysis_stats.get("failed_analyses", 0)
            batch_analyzed = analysis_stats.get("analyzed_orders", 0)
            batch_skipped = analysis_stats.get("skipped_orders", 0)
            
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¦‚æœæœ¬æ‰¹æ¬¡æ²¡æœ‰å¤„ç†ä»»ä½•è®°å½•ï¼Œå¢åŠ ç©ºæ‰¹æ¬¡è®¡æ•°
            if batch_analyzed == 0:
                consecutive_empty_batches += 1
                logger.info(f"ğŸ“ æœ¬æ‰¹æ¬¡æ²¡æœ‰å¤„ç†è®°å½•ï¼Œè¿ç»­ç©ºæ‰¹æ¬¡: {consecutive_empty_batches}/{max_empty_batches}")
                
                if consecutive_empty_batches >= max_empty_batches:
                    logger.info("ğŸ“ è¿ç»­å¤šä¸ªç©ºæ‰¹æ¬¡ï¼Œé˜Ÿåˆ—å·²ç©ºï¼Œå¾ªç¯åˆ†æå®Œæˆ")
                    break
            else:
                consecutive_empty_batches = 0
            
            total_successful += batch_successful
            total_failed += batch_failed
            total_analyzed += batch_analyzed
            total_skipped += batch_skipped
            total_batches += 1
            
            # ğŸ”¥ ä¿®å¤è¿›åº¦è®¡ç®—ï¼šprocessed_recordsåº”è¯¥åŒ…å«æ‰€æœ‰å·²å¤„ç†çš„è®°å½•ï¼ˆæˆåŠŸ+å¤±è´¥+è·³è¿‡ï¼‰
            final_total_processed = total_successful + total_failed + total_skipped
            
            # æ›´æ–°è¿›åº¦
            task_record.update_task_progress(
                db=db,
                task_id=main_task_id,
                processed_records=final_total_processed,
                success_records=total_successful,
                failed_records=total_failed,
                analyzed_records=total_successful
            )
            
            logger.info(f"ğŸ“ˆ æ‰¹æ¬¡{total_batches}å®Œæˆ: æˆåŠŸ{batch_successful}, å¤±è´¥{batch_failed}, åˆ†æ{batch_analyzed}, è·³è¿‡{batch_skipped}")
            logger.info(f"ğŸ“Š ç´¯è®¡è¿›åº¦: æ€»æ‰¹æ¬¡{total_batches}, æˆåŠŸ{total_successful}, å¤±è´¥{total_failed}, ç´¯è®¡è·³è¿‡{total_skipped}")
            
            # å¯é€‰ï¼šæ·»åŠ å¤„ç†é—´éš”ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
            if not loop_analysis:
                logger.info("ğŸ“ ä¸å¾ªç¯åˆ†ææ¨¡å¼ï¼Œå®Œæˆä¸€æ‰¹æ¬¡ååœæ­¢")
                break
            
            # å°å»¶è¿Ÿï¼Œè®©å…¶ä»–æ“ä½œæœ‰æœºä¼šæ‰§è¡Œ
            import asyncio
            await asyncio.sleep(0.1)
        
        # æ„å»ºæœ€ç»ˆæ‰§è¡Œè¯¦æƒ…
        duration_hours = round((end_datetime - start_datetime).total_seconds() / 3600, 2)
        final_details = {
            "extraction_phase": {
                "extracted": extracted,
                "inserted": inserted,
                "skipped": skipped,
                "duration_hours": duration_hours,
                "time_range": f"{start_datetime} ~ {end_datetime}"
            },
            "analysis_phase": {
                "total_batches": total_batches,
                "analyzed": total_analyzed,
                "successful": total_successful,
                "failed": total_failed,
                "skipped": total_skipped,
                "batch_size": batch_size,
                "loop_analysis": loop_analysis
            },
            "completion_summary": f"æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{total_successful}æ¡ï¼Œå¤±è´¥{total_failed}æ¡ï¼Œè·³è¿‡{total_skipped}æ¡"
        }
        
        # å®Œæˆä¸»ä»»åŠ¡
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="completed",
            execution_details=final_details
        )
        
        logger.info(f"ğŸ‰ å®Œæ•´ä»»åŠ¡æµç¨‹å®Œæˆ: æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{total_successful}æ¡")
        
        return final_details
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä¸»ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}


@router.post("/stop/{task_id}", summary="åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
async def stop_task(
    task_id: str,
    reason: str = Query("æ‰‹åŠ¨åœæ­¢", description="åœæ­¢åŸå› "),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    try:
        username = current_user.get("username", "unknown")
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task = task_record.get_task_by_id(db, task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
        if task.get("status") not in ["running", "pending"]:
            return {
                "success": False,
                "message": f"ä»»åŠ¡å½“å‰çŠ¶æ€ä¸º {task.get('status')}ï¼Œæ— æ³•åœæ­¢"
            }
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²åœæ­¢
        success = task_record.complete_task(
            db=db,
            task_id=task_id,
            status="cancelled",
            error_message=f"ç”¨æˆ· {username} æ‰‹åŠ¨åœæ­¢: {reason}"
        )
        
        if success:
            logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²è¢«ç”¨æˆ· {username} åœæ­¢: {reason}")
            return {
                "success": True,
                "message": f"ä»»åŠ¡å·²åœæ­¢: {reason}",
                "task_id": task_id,
                "stopped_by": username
            }
        else:
            raise HTTPException(status_code=500, detail="åœæ­¢ä»»åŠ¡å¤±è´¥")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {task_id}, {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.post("/stop/batch", summary="æ‰¹é‡åœæ­¢ä»»åŠ¡")
async def stop_tasks_batch(
    task_ids: list[str],
    reason: str = Query("æ‰¹é‡æ‰‹åŠ¨åœæ­¢", description="åœæ­¢åŸå› "),
    db: Session = Depends(get_db),
    current_user: dict = Depends(get_current_user)
):
    """æ‰¹é‡åœæ­¢å¤šä¸ªæ­£åœ¨è¿è¡Œçš„ä»»åŠ¡"""
    try:
        username = current_user.get("username", "unknown")
        results = []
        
        for task_id in task_ids:
            try:
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
                task = task_record.get_task_by_id(db, task_id)
                if not task:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
                    })
                    continue
                
                # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
                if task.get("status") not in ["running", "pending"]:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": f"ä»»åŠ¡çŠ¶æ€ä¸º {task.get('status')}ï¼Œæ— æ³•åœæ­¢"
                    })
                    continue
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå·²åœæ­¢
                success = task_record.complete_task(
                    db=db,
                    task_id=task_id,
                    status="cancelled",
                    error_message=f"ç”¨æˆ· {username} æ‰¹é‡åœæ­¢: {reason}"
                )
                
                if success:
                    logger.info(f"âœ… ä»»åŠ¡ {task_id} å·²è¢«ç”¨æˆ· {username} æ‰¹é‡åœæ­¢: {reason}")
                    results.append({
                        "task_id": task_id,
                        "success": True,
                        "message": f"ä»»åŠ¡å·²åœæ­¢: {reason}"
                    })
                else:
                    results.append({
                        "task_id": task_id,
                        "success": False,
                        "message": "åœæ­¢ä»»åŠ¡å¤±è´¥"
                    })
                    
            except Exception as e:
                logger.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {task_id}, {e}")
                results.append({
                    "task_id": task_id,
                    "success": False,
                    "message": f"åœæ­¢å¤±è´¥: {str(e)}"
                })
        
        successful_count = sum(1 for r in results if r["success"])
        
        return {
            "success": successful_count > 0,
            "message": f"æ‰¹é‡åœæ­¢å®Œæˆï¼šæˆåŠŸ {successful_count}/{len(task_ids)} ä¸ªä»»åŠ¡",
            "results": results,
            "summary": {
                "total": len(task_ids),
                "successful": successful_count,
                "failed": len(task_ids) - successful_count,
                "stopped_by": username
            }
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åœæ­¢ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/types", summary="è·å–ä»»åŠ¡ç±»å‹åˆ—è¡¨")
async def get_task_types(
    current_user: dict = Depends(get_current_user)
):
    """è·å–æ”¯æŒçš„ä»»åŠ¡ç±»å‹åˆ—è¡¨"""
    try:
        task_types = [
            {
                "type": "data_extraction",
                "name": "æ•°æ®æŠ½å–",
                "description": "ä»æ•°æ®åº“æŠ½å–å·¥å•æ•°æ®"
            },
            {
                "type": "batch_analysis", 
                "name": "æ‰¹é‡åˆ†æ",
                "description": "æ‰¹é‡åˆ†æå·¥å•è¯„è®º"
            },
            {
                "type": "cleanup",
                "name": "æ•°æ®æ¸…ç†", 
                "description": "æ¸…ç†æ—§çš„æ•°æ®è®°å½•"
            },
            {
                "type": "scheduled",
                "name": "å®šæ—¶ä»»åŠ¡",
                "description": "ç³»ç»Ÿå®šæ—¶æ‰§è¡Œçš„ä»»åŠ¡"
            }
        ]
        
        return {
            "success": True,
            "task_types": task_types,
            "total": len(task_types)
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡ç±»å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡ç±»å‹å¤±è´¥: {str(e)}")


@router.post("/validate-cron", summary="éªŒè¯CRONè¡¨è¾¾å¼")
async def validate_cron_expression_api(
    cron_data: dict,
    current_user: dict = Depends(get_current_user)
):
    """éªŒè¯CRONè¡¨è¾¾å¼æ ¼å¼å’Œæœ‰æ•ˆæ€§"""
    try:
        cron_expr = cron_data.get("cron_expression", "").strip()
        
        if not cron_expr:
            return {
                "success": False,
                "message": "cronè¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º"
            }
        
        # ä½¿ç”¨éªŒè¯å‡½æ•°
        from app.services.apscheduler_service import validate_cron_expression
        validation_result = validate_cron_expression(cron_expr)
        
        return {
            "success": validation_result["valid"],
            "message": validation_result["message"],
            "description": validation_result.get("description", ""),
            "next_runs": validation_result.get("next_runs", []),
            "cron_expression": cron_expr
        }
        
    except Exception as e:
        logger.error(f"éªŒè¯cronè¡¨è¾¾å¼å¤±è´¥: {e}")
        return {
            "success": False,
            "message": f"éªŒè¯å¤±è´¥: {str(e)}"
        }


@router.get("/configs", summary="è·å–ä»»åŠ¡é…ç½®åˆ—è¡¨")
async def get_task_configs(
    current_user: dict = Depends(get_current_user)
):
    """è·å–æ‰€æœ‰ä»»åŠ¡é…ç½®"""
    try:
        from app.models.task_config import task_config
        from app.db.database import get_db
        
        db = next(get_db())
        try:
            configs = task_config.get_all_tasks(db, enabled_only=False)
            
            # è·å–å½“å‰è¿è¡Œä¸­çš„ä»»åŠ¡
            running_tasks = task_record.get_task_records(
                db=db,
                limit=10,
                status="running"
            )
            
            # åˆ›å»ºè¿è¡Œä»»åŠ¡çš„å¿«é€ŸæŸ¥æ‰¾æ˜ å°„
            running_task_keys = set()
            for running_task in running_tasks:
                task_config_key = running_task.get("task_config_key")
                if task_config_key:
                    running_task_keys.add(task_config_key)
            
            # ä¼˜åŒ–æ¯ä¸ªé…ç½®é¡¹
            optimized_configs = []
            for config in configs:
                task_key = config.get("task_key")
                optimized_config = config.copy()
                
                # æ·»åŠ è¿è¡ŒçŠ¶æ€ä¿¡æ¯
                optimized_config["is_running"] = task_key in running_task_keys
                
                # æ ¼å¼åŒ–æ—¶é—´å’Œè°ƒåº¦æ˜¾ç¤º
                optimized_config["last_execution_display"] = _format_datetime_display(config.get("last_execution_time"))
                optimized_config["next_execution_display"] = _format_datetime_display(config.get("next_execution_time"))
                optimized_config["schedule_display"] = _format_schedule_display(config)
                
                optimized_configs.append(optimized_config)
            
        finally:
            db.close()
        
        return {
            "success": True,
            "data": optimized_configs,
            "summary": {
                "total_configs": len(configs),
                "enabled_configs": sum(1 for cfg in configs if cfg.get("is_enabled", False)),
                "running_tasks": len(running_task_keys)
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡é…ç½®å¤±è´¥: {str(e)}")


@router.post("/configs/{task_key}/toggle", summary="åˆ‡æ¢ä»»åŠ¡å¯ç”¨çŠ¶æ€")
async def toggle_task_enabled(
    task_key: str,
    enabled: bool = Query(..., description="æ˜¯å¦å¯ç”¨"),
    current_user: dict = Depends(get_current_user)
):
    """åˆ‡æ¢ä»»åŠ¡çš„å¯ç”¨/ç¦ç”¨çŠ¶æ€"""
    try:
        username = current_user.get("username", "unknown")
        
        # ä½¿ç”¨APSchedulerå¤„ç†ä»»åŠ¡åˆ‡æ¢
        success = await apscheduler_service.toggle_task_enabled(task_key, enabled)
        
        if success:
            status_text = "å¯ç”¨" if enabled else "ç¦ç”¨"
            logger.info(f"ğŸ”§ ç”¨æˆ· {username} {status_text}äº†ä»»åŠ¡: {task_key}")
            
            return {
                "success": True,
                "message": f"ä»»åŠ¡ {task_key} å·²{status_text}",
                "task_key": task_key,
                "enabled": enabled,
                "operator": username
            }
        else:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_key}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ‡æ¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"åˆ‡æ¢ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")


@router.put("/configs/{task_key}", summary="æ›´æ–°ä»»åŠ¡é…ç½®")
async def update_task_config(
    task_key: str,
    updates: dict,
    current_user: dict = Depends(get_current_user)
):
    """æ›´æ–°ä»»åŠ¡é…ç½®"""
    try:
        username = current_user.get("username", "unknown")
        
        # éªŒè¯æ›´æ–°å­—æ®µ
        allowed_fields = {
            'task_name', 'task_description', 'schedule_interval', 'schedule_cron',
            'default_batch_size', 'priority', 'timeout_seconds', 'retry_times'
        }
        
        filtered_updates = {k: v for k, v in updates.items() if k in allowed_fields}
        
        if not filtered_updates:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æœ‰æ•ˆçš„æ›´æ–°å­—æ®µ")
        
        # éªŒè¯cronè¡¨è¾¾å¼ï¼ˆå¦‚æœæä¾›äº†ï¼‰
        if 'schedule_cron' in filtered_updates:
            cron_expr = filtered_updates['schedule_cron']
            if cron_expr and cron_expr.strip():
                from app.services.apscheduler_service import validate_cron_expression
                validation = validate_cron_expression(cron_expr)
                if not validation['valid']:
                    raise HTTPException(
                        status_code=400,
                        detail=f"cronè¡¨è¾¾å¼æ— æ•ˆ: {validation['message']}"
                    )
                logger.info(f"âœ… cronè¡¨è¾¾å¼éªŒè¯é€šè¿‡: {cron_expr} - {validation.get('description', '')}")
        
        # ä½¿ç”¨APScheduleræ›´æ–°ä»»åŠ¡é…ç½®
        success = await apscheduler_service.update_task_config(task_key, filtered_updates)
        
        if success:
            logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ›´æ–°äº†ä»»åŠ¡é…ç½®: {task_key}")
            
            return {
                "success": True,
                "message": f"ä»»åŠ¡é…ç½® {task_key} å·²æ›´æ–°",
                "task_key": task_key,
                "updates": filtered_updates,
                "operator": username
            }
        else:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_key}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡é…ç½®å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ä»»åŠ¡é…ç½®å¤±è´¥: {str(e)}")


@router.post("/manual-execution/{task_key}", summary="æ‰‹åŠ¨æ‰§è¡ŒæŒ‡å®šä»»åŠ¡")
async def manual_execute_task(
    task_key: str,
    current_user: dict = Depends(get_current_user)
):
    """æ‰‹åŠ¨æ‰§è¡ŒæŒ‡å®šçš„ä»»åŠ¡é…ç½®"""
    try:
        from app.models.task_config import task_config as tc
        from app.db.database import get_db
        
        db = next(get_db())
        try:
            task_config = tc.get_task_by_key(db, task_key)
        finally:
            db.close()
        
        if not task_config:
            raise HTTPException(status_code=404, detail=f"ä»»åŠ¡é…ç½®ä¸å­˜åœ¨: {task_key}")
        
        username = current_user.get("username", "unknown")
        task_handler = task_config.get("task_handler")
        
        logger.info(f"ğŸ”§ ç”¨æˆ· {username} æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡: {task_key} ({task_handler})")
        
        # æ ¹æ®ä»»åŠ¡å¤„ç†å™¨ç±»å‹è°ƒç”¨ç›¸åº”çš„æ‰§è¡Œæ–¹æ³•
        if task_handler == "batch_analysis":
            from app.services.stage2_analysis_service import stage2_service
            
            result = await stage2_service.process_pending_analysis_queue(
                db=next(get_db()),
                batch_size=task_config.get("default_batch_size", 50),
                max_concurrent=5
            )
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            if result.get("success"):
                analysis_stats = result.get("analysis_statistics", {})
                result["task_id"] = f"manual_{task_key}_{int(datetime.now().timestamp())}"
                
        elif task_handler == "cleanup":
            from app.db.connection_manager import get_db_session
            from sqlalchemy import text
            
            with get_db_session() as db:
                task_id = task_record.create_task_record(
                    db=db,
                    task_name=f"æ‰‹åŠ¨æ‰§è¡Œ - {task_config.get('task_name')}",
                    task_type="cleanup",
                    trigger_type="manual",
                    trigger_user=username,
                    task_config_key=task_key,
                    execution_details={
                        "task_key": task_key,
                        "description": "æ‰‹åŠ¨è§¦å‘çš„æ¸…ç†ä»»åŠ¡",
                        "requested_by": username
                    }
                )
                
                # æ‰§è¡Œæ¸…ç†é€»è¾‘
                cutoff_date = datetime.now() - timedelta(days=30)
                
                cleanup_sql = """
                DELETE FROM ai_work_pending_analysis 
                WHERE updated_at < :cutoff_date 
                AND ai_status IN ('COMPLETED', 'FAILED')
                """
                
                result = db.execute(text(cleanup_sql), {"cutoff_date": cutoff_date})
                deleted_count = result.rowcount
                
                if deleted_count > 0:
                    db.commit()
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†ä»»åŠ¡å®Œæˆ: åˆ é™¤äº†{deleted_count}æ¡è¿‡æœŸå¾…åˆ†æè®°å½•")
                else:
                    logger.info("âœ¨ æ¸…ç†ä»»åŠ¡å®Œæˆ: æ²¡æœ‰è¿‡æœŸè®°å½•éœ€è¦æ¸…ç†")
                
                # å®Œæˆä»»åŠ¡
                task_record.complete_task(
                    db=db,
                    task_id=task_id,
                    status="completed",
                    execution_details={
                        "completed_at": datetime.now().isoformat(),
                        "task_key": task_key,
                        "deleted_count": deleted_count
                    }
                )
                
                result = {
                    "success": True,
                    "task_id": task_id,
                    "message": f"æ¸…ç†ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œåˆ é™¤äº†{deleted_count}æ¡è®°å½•"
                }
        else:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„ä»»åŠ¡å¤„ç†å™¨: {task_handler}")
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡å¤±è´¥: {task_key}, {e}")
        raise HTTPException(status_code=500, detail=f"æ‰‹åŠ¨æ‰§è¡Œä»»åŠ¡å¤±è´¥: {str(e)}")


@router.get("/configs-and-records", summary="è·å–ä»»åŠ¡é…ç½®å’Œæ‰§è¡Œè®°å½•çš„è”åˆè§†å›¾")
async def get_tasks_combined_view(
    limit: int = Query(50, description="è®°å½•æ•°é™åˆ¶", ge=1, le=200),
    current_user: dict = Depends(get_current_user)
):
    """è·å–ä»»åŠ¡é…ç½®å’Œæœ€è¿‘æ‰§è¡Œè®°å½•çš„è”åˆè§†å›¾"""
    try:
        from app.models.task_config import task_config as tc
        from app.db.database import get_db
        
        db = next(get_db())
        
        configs = tc.get_all_tasks(db, enabled_only=False)
        
        try:
            # è·å–æœ€è¿‘æ‰§è¡Œè®°å½•
            recent_records = task_record.get_task_records(
                db=db,
                limit=limit,
                offset=0
            )
            
            # è·å–å½“å‰è¿è¡Œä¸­çš„ä»»åŠ¡
            running_tasks = task_record.get_task_records(
                db=db,
                limit=10,
                status="running"
            )
            
        finally:
            db.close()
        
        # åˆå¹¶æ•°æ®
        combined_data = []
        
        # åˆ›å»ºè¿è¡Œä»»åŠ¡çš„å¿«é€ŸæŸ¥æ‰¾æ˜ å°„
        running_task_keys = set()
        for running_task in running_tasks:
            task_config_key = running_task.get("task_config_key")
            if task_config_key:
                running_task_keys.add(task_config_key)
        
        # å¤„ç†æ‰€æœ‰é…ç½®çš„ä»»åŠ¡
        for config in configs:
            task_key = config.get("task_key")
            
            # æŸ¥æ‰¾è¯¥ä»»åŠ¡çš„æœ€è¿‘æ‰§è¡Œè®°å½•
            latest_record = None
            execution_stats = {
                "total_executions": 0,
                "successful_executions": 0,
                "failed_executions": 0
            }
            
            for record in recent_records:
                if record.get("task_config_key") == task_key:
                    if not latest_record:
                        latest_record = record
                    
                    # ç»Ÿè®¡æ‰§è¡Œæƒ…å†µ
                    execution_stats["total_executions"] += 1
                    if record.get("status") == "completed":
                        execution_stats["successful_executions"] += 1
                    elif record.get("status") == "failed":
                        execution_stats["failed_executions"] += 1
            
            # ä¼˜åŒ–åçš„ä»»åŠ¡é¡¹ç›®æ•°æ®ç»“æ„
            combined_item = {
                "type": "configured_task",
                "task_key": task_key,
                "task_name": config.get("task_name"),
                "task_description": config.get("task_description"),
                "task_type": config.get("task_handler"),
                "status": "ç”Ÿæ•ˆ" if config.get("is_enabled", False) else "ä¸ç”Ÿæ•ˆ",
                "is_enabled": config.get("is_enabled", False),
                "is_running": task_key in running_task_keys,
                "schedule_interval": config.get("schedule_interval"),
                "schedule_display": _format_schedule_display(config),
                "priority": config.get("priority", 5),
                
                # æ—¶é—´ä¿¡æ¯
                "last_execution_time": config.get("last_execution_time"),
                "last_execution_display": _format_datetime_display(config.get("last_execution_time")),
                "next_execution_time": config.get("next_execution_time"),
                "next_execution_display": _format_datetime_display(config.get("next_execution_time")),
                
                # æ‰§è¡Œç»Ÿè®¡
                "execution_stats": {
                    "config_count": config.get("execution_count", 0),
                    "config_success": config.get("success_count", 0),
                    "config_failure": config.get("failure_count", 0),
                    "actual_total": execution_stats["total_executions"],
                    "actual_success": execution_stats["successful_executions"],
                    "actual_failed": execution_stats["failed_executions"],
                    "display_total": max(config.get("execution_count", 0), execution_stats["total_executions"]),
                    "display_success": max(config.get("success_count", 0), execution_stats["successful_executions"]),
                    "display_failed": max(config.get("failure_count", 0), execution_stats["failed_executions"])
                },
                
                # æœ€è¿‘æ‰§è¡Œè®°å½• - ğŸ”¥ æ·»åŠ ç»ˆæ­¢çŠ¶æ€æ ‡è¯†
                "latest_record": latest_record,
                "has_recent_execution": latest_record is not None,
                
                # ğŸ”¥ ä¸ºæœ€è¿‘æ‰§è¡Œè®°å½•æ·»åŠ ç»ˆæ­¢èƒ½åŠ›
                "latest_record_can_terminate": (
                    latest_record.get("status") in ["running", "pending"] 
                    if latest_record else False
                )
            }
            
            combined_data.append(combined_item)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_running = len(running_task_keys)
        enabled_count = sum(1 for cfg in configs if cfg.get("is_enabled", False))
        
        return {
            "success": True,
            "data": {
                "combined_tasks": combined_data,
                "summary": {
                    "total_configured": len(configs),
                    "enabled_tasks": enabled_count,
                    "disabled_tasks": len(configs) - enabled_count,
                    "running_tasks": total_running,
                    "recent_executions": len(recent_records)
                }
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡è”åˆè§†å›¾å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ä»»åŠ¡è”åˆè§†å›¾å¤±è´¥: {str(e)}")


def _format_schedule_display(config: dict) -> str:
    """æ ¼å¼åŒ–è°ƒåº¦æ˜¾ç¤º"""
    try:
        schedule_cron = config.get("schedule_cron")
        schedule_interval = config.get("schedule_interval", 0)
        
        # ä¼˜å…ˆæ˜¾ç¤ºcronè¡¨è¾¾å¼
        if schedule_cron and schedule_cron.strip():
            try:
                from app.services.apscheduler_service import _describe_cron_expression
                description = _describe_cron_expression(schedule_cron.strip())
                return f"CRON: {description} ({schedule_cron})"
            except:
                return f"CRON: {schedule_cron}"
        
        # é™çº§æ˜¾ç¤ºinterval
        elif schedule_interval:
            return f"é—´éš”: {_format_interval(schedule_interval)}"
        
        else:
            return "æœªè®¾ç½®"
            
    except Exception as e:
        return f"æ ¼å¼é”™è¯¯: {str(e)[:20]}"


def _format_interval(seconds: int) -> str:
    """æ ¼å¼åŒ–æ—¶é—´é—´éš”æ˜¾ç¤º"""
    if not seconds:
        return "æœªè®¾ç½®"
    
    if seconds < 60:
        return f"{seconds}ç§’"
    elif seconds < 3600:
        minutes = seconds // 60
        return f"{minutes}åˆ†é’Ÿ"
    elif seconds < 86400:
        hours = seconds // 3600
        return f"{hours}å°æ—¶"
    else:
        days = seconds // 86400
        return f"{days}å¤©"


def _format_datetime_display(datetime_str: Optional[str]) -> str:
    """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´æ˜¾ç¤º"""
    if not datetime_str:
        return "æ— æ•°æ®"
    
    try:
        if isinstance(datetime_str, str):
            clean_str = datetime_str.replace('Z', '').replace('+00:00', '')
            dt = datetime.fromisoformat(clean_str)
        else:
            dt = datetime_str
            
        now = datetime.now()
        diff = dt - now
        
        # æœªæ¥æ—¶é—´ï¼ˆä¸‹æ¬¡æ‰§è¡Œï¼‰
        if diff.total_seconds() > 0:
            if diff.days > 0:
                return f"{diff.days}å¤©å"
            elif diff.seconds > 3600:
                hours = diff.seconds // 3600
                return f"{hours}å°æ—¶å"
            elif diff.seconds > 60:
                minutes = diff.seconds // 60
                return f"{minutes}åˆ†é’Ÿå"
            else:
                return "å³å°†æ‰§è¡Œ"
        
        # è¿‡å»æ—¶é—´ï¼ˆä¸Šæ¬¡æ‰§è¡Œï¼‰
        else:
            diff = now - dt
            if diff.days > 0:
                return f"{diff.days}å¤©å‰"
            elif diff.seconds > 3600:
                hours = diff.seconds // 3600
                return f"{hours}å°æ—¶å‰"
            elif diff.seconds > 60:
                minutes = diff.seconds // 60
                return f"{minutes}åˆ†é’Ÿå‰"
            else:
                return "åˆšåˆš"
                
    except Exception as e:
        return f"æ ¼å¼é”™è¯¯: {str(e)[:20]}"


# ==================== åå°å¼‚æ­¥æ‰§è¡Œå‡½æ•° ====================

async def run_full_task_async(db: Session, target_datetime: datetime, analysis_limit: int, username: str, main_task_id: str):
    """å¼‚æ­¥æ‰§è¡Œå®Œæ•´ä»»åŠ¡æµç¨‹"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹: {main_task_id}, ç”¨æˆ·: {username}")
        
        # é˜¶æ®µ1ï¼šæ•°æ®æŠ½å–
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–ä¸­"
        )
        
        from app.services.stage1_work_extraction import stage1_service
        
        extraction_result = stage1_service.extract_work_data_by_time_range(
            db=db,
            target_date=target_datetime
        )
        
        if not extraction_result.get("success"):
            raise Exception(f"æ•°æ®æŠ½å–å¤±è´¥: {extraction_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        stats = extraction_result.get("statistics", {})
        extracted = stats.get("extracted", 0)
        inserted = stats.get("inserted", 0)
        skipped = stats.get("skipped", 0)
        
        # ä¿®å¤è¿›åº¦æ˜¾ç¤ºï¼šæŠ½å–é˜¶æ®µåªæ›´æ–°é˜¶æ®µä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®ï¼Œä¸è®¾ç½®è¿›åº¦ç›¸å…³å­—æ®µ
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ•°æ®æŠ½å–å®Œæˆï¼Œå¼€å§‹åˆ†æ",
            extracted_records=extracted,
            skipped_records=skipped,
            duplicate_records=skipped
        )
        
        logger.info(f"âœ… é˜¶æ®µ1å®Œæˆ: æŠ½å–{extracted}æ¡ï¼Œæ’å…¥{inserted}æ¡ï¼Œè·³è¿‡é‡å¤{skipped}æ¡")
        
        # é˜¶æ®µ2ï¼šæ‰¹é‡åˆ†æ
        from app.services.stage2_analysis_service import stage2_service
        
        # åœ¨åˆ†æé˜¶æ®µå¼€å§‹æ—¶ï¼ŒæŸ¥è¯¢å®é™…çš„å¾…åˆ†æå·¥å•æ•°é‡ï¼Œè®¾ç½®æ­£ç¡®çš„è¿›åº¦åŸºå‡†
        pending_count_sql = f"""
        SELECT COUNT(*) as count 
        FROM {stage1_service.pending_table_name} 
        WHERE ai_status = 'PENDING'
        """
        pending_result = db.execute(text(pending_count_sql))
        total_pending_orders = pending_result.fetchone()[0]
        
        # ç°åœ¨è®¾ç½®åˆ†æé˜¶æ®µçš„è¿›åº¦åŸºå‡†ï¼šåŸºäºå®é™…å¾…åˆ†æçš„å·¥å•æ•°é‡
        task_record.update_task_progress(
            db=db,
            task_id=main_task_id,
            process_stage="æ‰¹é‡åˆ†æä¸­",
            total_records=total_pending_orders,
            processed_records=0
        )
        
        logger.info(f"ğŸ“Š åˆ†æé˜¶æ®µå¼€å§‹: å¾…åˆ†æå·¥å•æ•°={total_pending_orders}")
        
        analysis_result = await stage2_service.process_pending_analysis_queue(
            db=db,
            batch_size=analysis_limit,
            max_concurrent=5
        )
        
        if not analysis_result.get("success"):
            raise Exception(f"æ‰¹é‡åˆ†æå¤±è´¥: {analysis_result.get('message', 'æœªçŸ¥é”™è¯¯')}")
        
        analysis_stats = analysis_result.get("analysis_statistics", {})
        successful = analysis_stats.get("successful_analyses", 0)
        failed = analysis_stats.get("failed_analyses", 0)
        total_analyzed = analysis_stats.get("analyzed_orders", 0)
        skipped_orders = analysis_stats.get("skipped_orders", 0)
        denoised_orders = analysis_stats.get("denoised_orders", 0)
        
        # å®Œæˆä¸»ä»»åŠ¡
        final_details = {
            "extraction_summary": {
                "extracted": extracted,
                "inserted": inserted,
                "skipped": skipped
            },
            "analysis_summary": {
                "analyzed": total_analyzed,
                "successful": successful,
                "failed": failed,
                "skipped": skipped_orders,
                "denoised": denoised_orders
            },
            "completion_message": f"æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{successful}æ¡"
        }
        
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="completed",
            execution_details=final_details
        )
        
        logger.info(f"ğŸ‰ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å®Œæˆ: æŠ½å–{extracted}æ¡ï¼ŒæˆåŠŸåˆ†æ{successful}æ¡")
        return final_details
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ å¼‚æ­¥å®Œæ•´ä»»åŠ¡æµç¨‹å¤±è´¥: {error_msg}")
        
        # æ ‡è®°ä¸»ä»»åŠ¡å¤±è´¥
        task_record.complete_task(
            db=db,
            task_id=main_task_id,
            status="failed",
            error_message=error_msg
        )
        
        return {"error": error_msg}
