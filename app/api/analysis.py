"""
åˆ†æç›¸å…³APIç«¯ç‚¹ - ç®€åŒ–ç‰ˆ
"""
from datetime import datetime, timedelta
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException, BackgroundTasks, Query
from sqlalchemy.orm import Session
from pydantic import BaseModel, Field

from app.db.database import get_db

from app.core.auth import get_current_user

from app.services.stage2_analysis_service import stage2_service
from app.services.stage1_work_extraction import stage1_service
from app.services.content_denoiser import content_denoiser
from app.models.denoise import denoise_record_manager
from config.settings import settings

router = APIRouter(prefix="/analysis", tags=["analysis"])


class AnalysisRequest(BaseModel):
    """åˆ†æè¯·æ±‚æ¨¡å‹"""
    mode: str = Field("batch", description="åˆ†ææ¨¡å¼: batch(æ‰¹é‡), time_range(æ—¶é—´æ®µ)")
    start_date: Optional[datetime] = Field(None, description="å¼€å§‹æ—¥æœŸ")
    end_date: Optional[datetime] = Field(None, description="ç»“æŸæ—¥æœŸ")
    days_back: int = Field(1, description="å‘å‰æŸ¥æ‰¾å¤©æ•°", ge=1, le=30)
    limit: int = Field(100, description="å¤„ç†é™åˆ¶æ•°é‡", ge=1, le=1000)
    ai_status: str = Field("PENDING", description="å¾…åˆ†ææ•°æ®çŠ¶æ€")


class ExtractionRequest(BaseModel):
    """æ•°æ®æŠ½å–è¯·æ±‚æ¨¡å‹"""
    mode: str = Field("daily", description="æŠ½å–æ¨¡å¼: daily(æŒ‰å¤©), historical(å†å²å…¨é‡), date_range(æ—¥æœŸèŒƒå›´), time_range(ç²¾ç¡®æ—¶é—´èŒƒå›´)")
    
    # ç²¾ç¡®æ—¶é—´èŒƒå›´å‚æ•°ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    start_time: Optional[datetime] = Field(None, description="ç²¾ç¡®å¼€å§‹æ—¶é—´(time_rangeæ¨¡å¼ç”¨)")
    end_time: Optional[datetime] = Field(None, description="ç²¾ç¡®ç»“æŸæ—¶é—´(time_rangeæ¨¡å¼ç”¨)")
    
    # æ—¥æœŸèŒƒå›´å‚æ•°ï¼ˆå…¼å®¹å†å²åŠŸèƒ½ï¼‰
    start_date: Optional[datetime] = Field(None, description="å¼€å§‹æ—¥æœŸ(å†å²æŠ½å–å’Œdate_rangeæ¨¡å¼ç”¨)")
    end_date: Optional[datetime] = Field(None, description="ç»“æŸæ—¥æœŸ(å†å²æŠ½å–å’Œdate_rangeæ¨¡å¼ç”¨)")
    
    # ç®€å•å‚æ•°ï¼ˆå…¼å®¹æ—¥å¸¸æŠ½å–ï¼‰
    days_back: int = Field(1, description="å‘å‰æŸ¥æ‰¾å¤©æ•°(dailyæ¨¡å¼ç”¨)", ge=1, le=365)
    target_date: Optional[datetime] = Field(None, description="æŒ‡å®šæŠ½å–æ—¥æœŸ(dailyæ¨¡å¼ç”¨)")


class DenoiseQueryRequest(BaseModel):
    """å»å™ªæŸ¥è¯¢è¯·æ±‚æ¨¡å‹"""
    batch_id: Optional[str] = Field(None, description="æ‰¹æ¬¡ID")
    work_id: Optional[int] = Field(None, description="å·¥å•ID")
    days: int = Field(7, description="æŸ¥è¯¢å¤©æ•°", ge=1, le=30)
    limit: int = Field(50, description="æŸ¥è¯¢é™åˆ¶", ge=1, le=500)


@router.post("/analyze")
async def run_analysis(
    request: AnalysisRequest,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ç»Ÿä¸€åˆ†ææ¥å£ - åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. batch: æ‰¹é‡åˆ†æå¾…å¤„ç†æ•°æ®ï¼ˆé»˜è®¤ï¼‰
    2. time_range: æŒ‰æ—¶é—´æ®µåˆ†ææ•°æ®
    """
    import asyncio
    import uuid
    from app.core.concurrency import async_task_manager
    
    try:
        # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = f"analysis_{uuid.uuid4().hex[:16]}"
        username = current_user.get("username", "unknown")
        
        # æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸ç­‰å¾…å®Œæˆ
        if request.mode == "time_range":
            # æ—¶é—´æ®µåˆ†æ
            success = await async_task_manager.submit_task(
                task_id,
                analyze_by_time_range_async(db, request, username, task_id)
            )
        else:
            # æ‰¹é‡åˆ†æï¼ˆé»˜è®¤ï¼‰
            success = await async_task_manager.submit_task(
                task_id,
                analyze_batch_data_async(db, request, username, task_id)
            )
        
        if not success:
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…ä»»åŠ¡å®Œæˆ
        return {
            "success": True,
            "task_id": task_id,
            "mode": request.mode,
            "status": "submitted",
            "message": f"åˆ†æä»»åŠ¡å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "check_status_url": f"/api/v1/tasks/status/{task_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æä»»åŠ¡æäº¤å¤±è´¥: {str(e)}")


async def analyze_batch_data(db: Session, request: AnalysisRequest):
    """æ‰¹é‡åˆ†æå¾…å¤„ç†æ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"å¼€å§‹æ‰¹é‡åˆ†æï¼ŒçŠ¶æ€: {request.ai_status}, é™åˆ¶: {request.limit}")
        
        # å¦‚æœæ˜¯FAILEDçŠ¶æ€ï¼Œå…ˆé‡ç½®ä¸ºPENDINGä»¥ä¾¿é‡æ–°åˆ†æ
        reset_count = 0
        if request.ai_status == "FAILED":
            logger.info("ğŸ”„ æ£€æµ‹åˆ°FAILEDçŠ¶æ€åˆ†æè¯·æ±‚ï¼Œå…ˆé‡ç½®å·¥å•çŠ¶æ€...")
            reset_count = stage1_service.reset_failed_work_orders_for_retry(
                db=db,
                limit=request.limit
            )
            if reset_count > 0:
                logger.info(f"âœ… æˆåŠŸé‡ç½® {reset_count} ä¸ªFAILEDå·¥å•ä¸ºPENDINGçŠ¶æ€")
                # é‡ç½®åï¼Œæ”¹ä¸ºæŸ¥è¯¢PENDINGçŠ¶æ€çš„å·¥å•
                actual_status = "PENDING"
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡ç½®çš„FAILEDå·¥å•")
                actual_status = "FAILED"  # ä¿æŒåŸçŠ¶æ€æŸ¥è¯¢
        else:
            actual_status = request.ai_status
        
        # è·å–å¾…å¤„ç†çš„å·¥å•
        pending_orders = stage1_service.get_pending_work_orders(
            db=db,
            ai_status=actual_status,
            limit=request.limit
        )
        
        if not pending_orders:
            message = f"æ²¡æœ‰æ‰¾åˆ°çŠ¶æ€ä¸º {request.ai_status} çš„å¾…å¤„ç†å·¥å•"
            if request.ai_status == "FAILED" and reset_count > 0:
                message = f"é‡ç½®äº† {reset_count} ä¸ªFAILEDå·¥å•ï¼Œä½†æ²¡æœ‰æ‰¾åˆ°å¯åˆ†æçš„å·¥å•"
            return {
                "success": True,
                "mode": "batch",
                "message": message,
                "reset_count": reset_count if request.ai_status == "FAILED" else 0,
                "statistics": {
                    "total_found": 0,
                    "processed": 0,
                    "successful": 0,
                    "failed": 0
                }
            }
        
        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        result = await stage2_service.process_pending_analysis_queue(
            db, 
            batch_size=request.limit
        )
        
        message = f"æ‰¹é‡åˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(pending_orders)} ä¸ªå·¥å•"
        if request.ai_status == "FAILED" and reset_count > 0:
            message = f"é‡ç½®äº† {reset_count} ä¸ªFAILEDå·¥å•å¹¶å®Œæˆåˆ†æï¼Œå¤„ç†äº† {len(pending_orders)} ä¸ªå·¥å•"
        
        # å°è¯•è·å–å»å™ªç»Ÿè®¡ä¿¡æ¯
        denoise_info = None
        if result.get("success") and "batch_id" in result.get("statistics", {}):
            try:
                batch_id = result["statistics"].get("batch_id")
                if batch_id:
                    denoise_records = denoise_record_manager.get_batch_statistics(db, batch_id, 1)
                    if denoise_records:
                        denoise_info = denoise_records[0]
                        logger.info(f"ğŸ“Š è·å–åˆ°å»å™ªç»Ÿè®¡: æ‰¹æ¬¡ {batch_id}")
            except Exception as e:
                logger.warning(f"è·å–å»å™ªç»Ÿè®¡å¤±è´¥: {e}")
        
        return {
            "success": True,
            "mode": "batch",
            "ai_status": request.ai_status,
            "limit": request.limit,
            "message": message,
            "reset_count": reset_count if request.ai_status == "FAILED" else 0,
            "pending_count": len(pending_orders),
            "result": result,
            "denoise_statistics": denoise_info
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        return {
            "success": False,
            "mode": "batch",
            "error": str(e),
            "message": "æ‰¹é‡åˆ†æå¤±è´¥"
        }


async def analyze_by_time_range(db: Session, request: AnalysisRequest):
    """æŒ‰æ—¶é—´æ®µåˆ†ææ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # è®¡ç®—æ—¶é—´èŒƒå›´
        if not request.start_date and not request.end_date:
            end_date = datetime.now()
            start_date = end_date - timedelta(days=request.days_back)
        else:
            start_date = request.start_date
            end_date = request.end_date or datetime.now()
        
        logger.info(f"å¼€å§‹æ—¶é—´æ®µåˆ†æ: {start_date.date()} åˆ° {end_date.date()}")
        
        # å¦‚æœæ˜¯FAILEDçŠ¶æ€ï¼Œå…ˆé‡ç½®ä¸ºPENDINGä»¥ä¾¿é‡æ–°åˆ†æ
        reset_count = 0
        if request.ai_status == "FAILED":
            logger.info("ğŸ”„ æ£€æµ‹åˆ°FAILEDçŠ¶æ€åˆ†æè¯·æ±‚ï¼Œå…ˆé‡ç½®å·¥å•çŠ¶æ€...")
            reset_count = stage1_service.reset_failed_work_orders_for_retry(
                db=db,
                limit=request.limit
            )
            if reset_count > 0:
                logger.info(f"âœ… æˆåŠŸé‡ç½® {reset_count} ä¸ªFAILEDå·¥å•ä¸ºPENDINGçŠ¶æ€")
                actual_status = "PENDING"
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦é‡ç½®çš„FAILEDå·¥å•")
                actual_status = "FAILED"
        else:
            actual_status = request.ai_status
        
        # ğŸ”¥ ä¿®å¤ï¼šç›´æ¥åœ¨æ•°æ®åº“å±‚é¢æŒ‰æ—¶é—´èŒƒå›´è·å–å¾…å¤„ç†å·¥å•ï¼Œæé«˜æ•ˆç‡
        pending_orders = stage1_service.get_pending_work_orders(
            db=db,
            ai_status=actual_status,
            limit=request.limit,
            start_date=start_date,
            end_date=end_date
        )
        
        if not pending_orders:
            return {
                "success": True,
                "mode": "time_range",
                "time_range": {
                    "start_date": start_date.strftime("%Y-%m-%d"),
                    "end_date": end_date.strftime("%Y-%m-%d")
                },
                "message": f"åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„å·¥å•",
                "statistics": {
                    "total_found": len(pending_orders),
                    "time_range_filtered": len(pending_orders),
                    "processed": 0,
                    "successful": 0,
                    "failed": 0
                }
            }
        
        # æ‰§è¡Œåˆ†æï¼ˆå¤„ç†æ—¶é—´èŒƒå›´å†…çš„å·¥å•æ•°é‡ï¼‰
        result = await stage2_service.process_pending_analysis_queue(
            db, 
            batch_size=len(pending_orders),
            start_date=start_date,
            end_date=end_date
        )
        
        return {
            "success": True,
            "mode": "time_range",
            "time_range": {
                "start_date": start_date.strftime("%Y-%m-%d"),
                "end_date": end_date.strftime("%Y-%m-%d")
            },
            "ai_status": request.ai_status,
            "message": f"æ—¶é—´æ®µåˆ†æå®Œæˆï¼Œå¤„ç†äº† {len(pending_orders)} ä¸ªå·¥å•",
            "statistics": {
                "total_found": len(pending_orders),
                "time_range_filtered": len(pending_orders),
            },
            "result": result
        }
        
    except Exception as e:
        logger.error(f"æ—¶é—´æ®µåˆ†æå¤±è´¥: {e}")
        return {
            "success": False,
            "mode": "time_range",
            "error": str(e),
            "message": "æ—¶é—´æ®µåˆ†æå¤±è´¥"
        }


# ===== æ•°æ®æŠ½å–ç›¸å…³æ¥å£ =====

@router.post("/extraction/extract")
async def extract_work_data(
    request: ExtractionRequest,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    ç»Ÿä¸€æ•°æ®æŠ½å–æ¥å£ - åå°å¼‚æ­¥æ‰§è¡Œï¼Œç«‹å³è¿”å›ä»»åŠ¡ID
    æ”¯æŒå››ç§æ¨¡å¼ï¼š
    1. time_range: ç²¾ç¡®æ—¶é—´èŒƒå›´æŠ½å–ï¼ˆæ¨èï¼‰
    2. daily: æŒ‰å¤©æŠ½å–ï¼ˆé»˜è®¤ï¼‰
    3. historical: å†å²å…¨é‡æŠ½å–
    4. date_range: æŒ‡å®šæ—¥æœŸèŒƒå›´æŠ½å–
    """
    import uuid
    from app.core.concurrency import async_task_manager
    
    try:
        # å‚æ•°éªŒè¯
        if request.mode == "time_range":
            if not request.start_time or not request.end_time:
                raise HTTPException(status_code=400, detail="æ—¶é—´èŒƒå›´æ¨¡å¼éœ€è¦æŒ‡å®šstart_timeå’Œend_time")
            if request.start_time >= request.end_time:
                raise HTTPException(status_code=400, detail="å¼€å§‹æ—¶é—´å¿…é¡»æ—©äºç»“æŸæ—¶é—´")
        elif request.mode == "date_range":
            if not request.start_date or not request.end_date:
                raise HTTPException(status_code=400, detail="æ—¥æœŸèŒƒå›´æ¨¡å¼éœ€è¦æŒ‡å®šstart_dateå’Œend_date")
        
        # ç”Ÿæˆå”¯ä¸€ä»»åŠ¡ID
        task_id = f"extraction_{uuid.uuid4().hex[:16]}"
        username = current_user.get("username", "unknown")
        
        # æäº¤åˆ°åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸ç­‰å¾…å®Œæˆ
        success = await async_task_manager.submit_task(
            task_id,
            extract_work_data_async(db, request, username, task_id)
        )
        
        if not success:
            raise HTTPException(status_code=500, detail="ä»»åŠ¡æäº¤å¤±è´¥")
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä¸ç­‰å¾…ä»»åŠ¡å®Œæˆ
        return {
            "success": True,
            "task_id": task_id,
            "mode": request.mode,
            "status": "submitted",
            "message": f"æ•°æ®æŠ½å–ä»»åŠ¡å·²æäº¤åˆ°åå°æ‰§è¡Œ",
            "check_status_url": f"/api/v1/tasks/status/{task_id}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ•°æ®æŠ½å–ä»»åŠ¡æäº¤å¤±è´¥: {str(e)}")


async def extract_historical_data(db: Session, start_date: Optional[datetime] = None, end_date: Optional[datetime] = None):
    """æŠ½å–å†å²å…¨é‡æ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œé»˜è®¤æŠ½å–è¿‘30å¤©çš„æ•°æ®
        if not end_date:
            end_date = datetime.now()
        if not start_date:
            start_date = end_date - timedelta(days=30)
        
        logger.info(f"å¼€å§‹å†å²æ•°æ®æŠ½å–: {start_date.date()} åˆ° {end_date.date()}")
        
        total_extracted = 0
        total_inserted = 0
        current_date = start_date.date()
        end_date_only = end_date.date()
        
        # æŒ‰å¤©å¾ªç¯æŠ½å–
        while current_date <= end_date_only:
            try:
                day_result = stage1_service.extract_work_data_by_time_range(
                    db=db,
                    target_date=datetime.combine(current_date, datetime.min.time()),
                    days_back=1
                )
                
                if day_result.get("success"):
                    stats = day_result.get("statistics", {})
                    total_extracted += stats.get("extracted", 0)
                    total_inserted += stats.get("inserted", 0)
                    logger.info(f"æ—¥æœŸ {current_date}: æŠ½å– {stats.get('extracted', 0)} ä¸ªå·¥å•ï¼Œæ’å…¥ {stats.get('inserted', 0)} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"æ—¥æœŸ {current_date} æŠ½å–å¤±è´¥: {e}")
            
            current_date += timedelta(days=1)
        
        return {
            "success": True,
            "mode": "historical",
            "date_range": {
                "start_date": start_date.strftime("%Y-%m-%d"),
                "end_date": end_date.strftime("%Y-%m-%d")
            },
            "statistics": {
                "total_extracted": total_extracted,
                "total_inserted": total_inserted,
                "days_processed": (end_date.date() - start_date.date()).days + 1
            },
            "message": f"å†å²æ•°æ®æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {(end_date.date() - start_date.date()).days + 1} å¤©çš„æ•°æ®"
        }
        
    except Exception as e:
        logger.error(f"å†å²æ•°æ®æŠ½å–å¤±è´¥: {e}")
        return {
            "success": False,
            "mode": "historical",
            "error": str(e),
            "message": "å†å²æ•°æ®æŠ½å–å¤±è´¥"
        }


async def extract_date_range_data(db: Session, start_date: datetime, end_date: datetime):
    """æŠ½å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"å¼€å§‹æ—¥æœŸèŒƒå›´æ•°æ®æŠ½å–: {start_date.date()} åˆ° {end_date.date()}")
        
        total_extracted = 0
        total_inserted = 0
        current_date = start_date.date()
        end_date_only = end_date.date()
        
        # æŒ‰å¤©å¾ªç¯æŠ½å–
        while current_date <= end_date_only:
            try:
                day_result = stage1_service.extract_work_data_by_time_range(
                    db=db,
                    target_date=datetime.combine(current_date, datetime.min.time()),
                    days_back=1
                )
                
                if day_result.get("success"):
                    stats = day_result.get("statistics", {})
                    total_extracted += stats.get("extracted", 0)
                    total_inserted += stats.get("inserted", 0)
                    logger.info(f"æ—¥æœŸ {current_date}: æŠ½å– {stats.get('extracted', 0)} ä¸ªå·¥å•ï¼Œæ’å…¥ {stats.get('inserted', 0)} æ¡è®°å½•")
                
            except Exception as e:
                logger.error(f"æ—¥æœŸ {current_date} æŠ½å–å¤±è´¥: {e}")
            
            current_date += timedelta(days=1)
        
        return {
            "success": True,
            "mode": "date_range",
            "date_range": {
                "start_date": start_date.strftime("%Y-%m-%d"),
                "end_date": end_date.strftime("%Y-%m-%d")
            },
            "statistics": {
                "total_extracted": total_extracted,
                "total_inserted": total_inserted,
                "days_processed": (end_date.date() - start_date.date()).days + 1
            },
            "message": f"æ—¥æœŸèŒƒå›´æ•°æ®æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {(end_date.date() - start_date.date()).days + 1} å¤©çš„æ•°æ®"
        }
        
    except Exception as e:
        logger.error(f"æ—¥æœŸèŒƒå›´æ•°æ®æŠ½å–å¤±è´¥: {e}")
        return {
            "success": False,
            "mode": "date_range",
            "error": str(e),
            "message": "æ—¥æœŸèŒƒå›´æ•°æ®æŠ½å–å¤±è´¥"
        }


@router.get("/extraction/statistics")
async def get_extraction_statistics(
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–æ•°æ®æŠ½å–ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        stats = stage1_service.get_extraction_statistics(db)
        return stats
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")


# ===== å»å™ªç›¸å…³æ¥å£ =====

@router.get("/denoise/summary")
async def get_denoise_summary(
    days: int = Query(7, description="ç»Ÿè®¡å¤©æ•°", ge=1, le=30),
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–å»å™ªç»Ÿè®¡æ‘˜è¦
    è¿”å›æŒ‡å®šå¤©æ•°å†…çš„å»å™ªå¤„ç†ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        summary = denoise_record_manager.get_denoise_summary(db, days)
        return {
            "success": True,
            "data": summary,
            "message": f"æˆåŠŸè·å– {days} å¤©å†…çš„å»å™ªç»Ÿè®¡æ‘˜è¦"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å»å™ªç»Ÿè®¡æ‘˜è¦å¤±è´¥: {str(e)}")


@router.get("/denoise/batches")
async def get_denoise_batches(
    batch_id: Optional[str] = Query(None, description="æ‰¹æ¬¡ID"),
    limit: int = Query(50, description="æŸ¥è¯¢é™åˆ¶", ge=1, le=500),
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–å»å™ªæ‰¹æ¬¡åˆ—è¡¨
    æ”¯æŒæŒ‰æ‰¹æ¬¡IDæŸ¥è¯¢æˆ–è·å–æœ€è¿‘çš„æ‰¹æ¬¡åˆ—è¡¨
    """
    try:
        batches = denoise_record_manager.get_batch_statistics(db, batch_id, limit)
        return {
            "success": True,
            "data": batches,
            "total": len(batches),
            "message": f"æˆåŠŸè·å– {len(batches)} ä¸ªæ‰¹æ¬¡è®°å½•"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ‰¹æ¬¡åˆ—è¡¨å¤±è´¥: {str(e)}")


@router.get("/denoise/records")
async def get_denoise_records(
    work_id: Optional[int] = Query(None, description="å·¥å•ID"),
    batch_id: Optional[str] = Query(None, description="æ‰¹æ¬¡ID"),
    limit: int = Query(100, description="æŸ¥è¯¢é™åˆ¶", ge=1, le=500),
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    è·å–å·¥å•å»å™ªè®°å½•
    æ”¯æŒæŒ‰å·¥å•IDæˆ–æ‰¹æ¬¡IDæŸ¥è¯¢
    """
    try:
        records = denoise_record_manager.get_work_order_denoise_records(
            db, work_id, batch_id, limit
        )
        return {
            "success": True,
            "data": records,
            "total": len(records),
            "message": f"æˆåŠŸè·å– {len(records)} æ¡å»å™ªè®°å½•"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å»å™ªè®°å½•å¤±è´¥: {str(e)}")


@router.post("/denoise/test")
async def test_denoise_single_work_order(
    work_id: int,
    current_user: dict = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """
    æµ‹è¯•å•ä¸ªå·¥å•çš„å»å™ªæ•ˆæœ
    ç”¨äºè°ƒè¯•å’ŒéªŒè¯å»å™ªè§„åˆ™
    """
    try:
        # è·å–å·¥å•è¯„è®ºæ•°æ®
        pending_orders = stage1_service.get_pending_work_orders(db, limit=1000)
        target_order = None
        
        for order in pending_orders:
            if order["work_id"] == work_id:
                target_order = order
                break
        
        if not target_order:
            raise HTTPException(status_code=404, detail=f"å·¥å• {work_id} ä¸åœ¨å¾…å¤„ç†åˆ—è¡¨ä¸­")
        
        # è·å–è¯„è®º
        comments = stage1_service.get_work_comments(
            db, work_id, target_order["comment_table_name"]
        )
        
        if not comments:
            return {
                "success": True,
                "work_id": work_id,
                "message": "å·¥å•æ— è¯„è®ºæ•°æ®",
                "original_comments": [],
                "denoise_result": {
                    "filtered_comments": [],
                    "original_count": 0,
                    "filtered_count": 0,
                    "removed_count": 0,
                    "filter_statistics": {}
                }
            }
        
        # è¿‡æ»¤ç©ºè¯„è®º
        valid_comments = [c for c in comments if c.get("content") and str(c.get("content", "")).strip()]
        
        # åº”ç”¨å»å™ª
        denoise_result = content_denoiser.filter_comments(valid_comments)
        
        # è¿”å›è¯¦ç»†çš„å¯¹æ¯”ç»“æœ
        return {
            "success": True,
            "work_id": work_id,
            "message": f"å»å™ªæµ‹è¯•å®Œæˆï¼š{denoise_result['original_count']} -> {denoise_result['filtered_count']} æ¡è¯„è®º",
            "original_comments": valid_comments,
            "denoise_result": denoise_result,
            "comparison": {
                "removed_comments": [
                    {
                        "index": detail["index"],
                        "content": detail["comment"].get("content", ""),
                        "user_type": detail["comment"].get("user_type", ""),
                        "name": detail["comment"].get("name", ""),
                        "reason": detail["reason"]
                    }
                    for detail in denoise_result["filter_statistics"].get("removed_details", [])
                ]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å»å™ªæµ‹è¯•å¤±è´¥: {str(e)}")


# ==================== åå°å¼‚æ­¥æ‰§è¡Œå‡½æ•° ====================

async def analyze_batch_data_async(db: Session, request: AnalysisRequest, username: str, task_id: str):
    """å¼‚æ­¥æ‰¹é‡åˆ†æå¾…å¤„ç†æ•°æ®"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ‰¹é‡åˆ†æä»»åŠ¡: {task_id}, ç”¨æˆ·: {username}")
        
        from app.services.stage2_analysis_service import stage2_service
        
        result = await stage2_service.process_pending_analysis_queue(
            db=db,
            batch_size=request.limit or 50,
            max_concurrent=8
        )
        
        logger.info(f"âœ… å¼‚æ­¥æ‰¹é‡åˆ†æä»»åŠ¡å®Œæˆ: {task_id}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥æ‰¹é‡åˆ†æä»»åŠ¡å¤±è´¥: {task_id}, {e}")
        raise


async def analyze_by_time_range_async(db: Session, request: AnalysisRequest, username: str, task_id: str):
    """å¼‚æ­¥æ—¶é—´æ®µåˆ†æ"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ—¶é—´æ®µåˆ†æä»»åŠ¡: {task_id}, ç”¨æˆ·: {username}")
        
        # è°ƒç”¨åŸæ¥çš„æ—¶é—´æ®µåˆ†æé€»è¾‘
        result = await analyze_by_time_range(db, request)
        
        logger.info(f"âœ… å¼‚æ­¥æ—¶é—´æ®µåˆ†æä»»åŠ¡å®Œæˆ: {task_id}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥æ—¶é—´æ®µåˆ†æä»»åŠ¡å¤±è´¥: {task_id}, {e}")
        raise


async def extract_work_data_async(db: Session, request: ExtractionRequest, username: str, task_id: str):
    """å¼‚æ­¥æ•°æ®æŠ½å–"""
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¼‚æ­¥æ•°æ®æŠ½å–ä»»åŠ¡: {task_id}, ç”¨æˆ·: {username}, æ¨¡å¼: {request.mode}")
        
        from app.services.stage1_work_extraction import stage1_service
        
        if request.mode == "time_range":
            # ç²¾ç¡®æ—¶é—´èŒƒå›´æŠ½å–
            result = stage1_service.extract_work_data_by_time_range(
                db=db,
                start_time=request.start_time,
                end_time=request.end_time
            )
        elif request.mode == "historical":
            # å†å²å…¨é‡æŠ½å–
            result = await extract_historical_data(db, request.start_date, request.end_date)
        elif request.mode == "date_range":
            # æ—¥æœŸèŒƒå›´æŠ½å–
            result = await extract_date_range_data(db, request.start_date, request.end_date)
        else:
            # æ—¥å¸¸æŒ‰å¤©æŠ½å–ï¼ˆé»˜è®¤ï¼‰
            result = stage1_service.extract_work_data_by_time_range(
                db=db,
                target_date=request.target_date,
                days_back=request.days_back
            )
        
        logger.info(f"âœ… å¼‚æ­¥æ•°æ®æŠ½å–ä»»åŠ¡å®Œæˆ: {task_id}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥æ•°æ®æŠ½å–ä»»åŠ¡å¤±è´¥: {task_id}, {e}")
        raise


